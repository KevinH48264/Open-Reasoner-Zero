{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m206.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, pyarrow, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.2.0\n",
      "    Uninstalling fsspec-2025.2.0:\n",
      "      Successfully uninstalled fsspec-2025.2.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "Successfully installed datasets-3.3.2 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 pyarrow-19.0.1 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare some Countdown data\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Jiayi-Pan/Countdown-Tasks-3to4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['target', 'nums'],\n",
       "        num_rows: 490364\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# f\"Using the numbers {nums}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) one or multiple times but each number can only be used once. Show your work in <think> </think> tags. And return the final equation in <answer> </answer> tags, for example <answer>\\\\boxed{{(1 + 2) / 3}}</answer>. Think step by step inside <think> tags.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = [\n",
    "    [\n",
    "        {\n",
    "            \"from\": \"human\",\n",
    "            \"value\": (\n",
    "                f\"Using the numbers {row['nums']}, create an equation that equals {row['target']}. \"\n",
    "                \"You can use basic arithmetic operations (+, -, *, /) one or multiple times but each number can only be used once. \"\n",
    "                \"Show your work in <think> </think> tags. And return the final equation in <answer> </answer> tags, \"\n",
    "                \"for example <answer> (1 + 2) / 3 </answer>. Think step by step inside <think> tags.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"assistant\",\n",
    "            \"ground_truth\": {\n",
    "                \"value\": f\"{row['target']}\"\n",
    "            },\n",
    "            \"nums\": {\n",
    "                \"value\": f\"{row['nums']}\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    for row in ds[\"train\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the numbers [44, 19, 35], create an equation that equals 98. You can use basic arithmetic operations (+, -, *, /) one or multiple times but each number can only be used once. Show your work in <think> </think> tags. And return the final equation in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>. Think step by step inside <think> tags.\n"
     ]
    }
   ],
   "source": [
    "print(dataset_list[0][0]['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/data/users/kevihuang/projects/Open-Reasoner-Zero/data/countdown-tasks-3to4_first_5000.json', 'w') as f:\n",
    "    json.dump(dataset_list[:5000], f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Query Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|██████████| 50000/50000 [00:00<00:00, 315059.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare some Countdown data\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"khuang2/math-query-gen-prompts-w-solution\") #khuang2/math-query-gen-prompts\") #\"khuang2/Countdown-Tasks-3to4-query-gen-prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nGiven the assistant goal, task generation success criteria, and example final answer, generate a task that fulfills the task generation success criteria. The generated task should be:\\n\\n1) maximizing user benefit and helpfulness and minimizing user risk, harm, and unwanted outcomes\\n\\n2) novel - avoid designing a task that is drawn from the same linguistic templates or differ only in superficial details\\n\\n3) difficult yet tractable - the task should be challenging for the assistant to complete, without being completely intractable\\n\\n4) clear - the task should be specific enough to not require further user input to understand what is being asked of the assistant and should provide all the context and detail necessary\\n\\nShow your work in <think> </think> tags. And return the task and final answer in <answer> </answer> tags.\\n\\n\\nAssistant Goal: Get really good at math.\\n\\nTask Generation Success Criteria: The task and final answer must be a dictionary with 'prompt', 'target', and 'solution' keys. The 'prompt' should be a math problem, the 'target' should be the expected answer to the math problem, and the 'solution' must be a solution to the math problem that clearly and unambiguously leads to the expected answer 'target'.\\n\\nExample Final Answer: <answer>\\n{\\n    'prompt': 'What is 1 + 1?',\\n    'target': 2,\\n    'solution': 'To solve 1 + 1, we add the two numbers together. Starting with the number 1, adding another 1 gives us 2. Therefore, the answer is 2.'\\n}\\n</answer><|im_end|>\\n<|im_start|>assistant\\n<think>\\n\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the assistant goal, task generation success criteria, and example final answer, generate a task that fulfills the task generation success criteria. The generated task should be:\n",
      "\n",
      "1) maximizing user benefit and helpfulness and minimizing user risk, harm, and unwanted outcomes\n",
      "\n",
      "2) novel - avoid designing a task that is drawn from the same linguistic templates or differ only in superficial details\n",
      "\n",
      "3) difficult yet tractable - the task should be challenging for the assistant to complete, without being completely intractable\n",
      "\n",
      "4) clear - the task should be specific enough to not require further user input to understand what is being asked of the assistant and should provide all the context and detail necessary\n",
      "\n",
      "Show your work in <think> </think> tags. And return the task and final answer in <answer> </answer> tags.\n",
      "\n",
      "\n",
      "Assistant Goal: Get really good at math.\n",
      "\n",
      "Task Generation Success Criteria: The task and final answer must be a dictionary with 'prompt', 'target', and 'solution' keys. The 'prompt' should be a math problem, the 'target' should be the expected answer to the math problem, and the 'solution' must be a solution to the math problem that clearly and unambiguously leads to the expected answer 'target'.\n",
      "\n",
      "Example Final Answer: <answer>\n",
      "{\n",
      "    'prompt': 'What is 1 + 1?',\n",
      "    'target': 2,\n",
      "    'solution': 'To solve 1 + 1, we add the two numbers together. Starting with the number 1, adding another 1 gives us 2. Therefore, the answer is 2.'\n",
      "}\n",
      "</answer>\n"
     ]
    }
   ],
   "source": [
    "prompt = ds['train'][0]['prompt']\n",
    "print(prompt.split(\"<|im_start|>user\")[1].split(\"<|im_end|>\")[0].strip().replace(\"×\", \"*\").replace(\"÷\", \"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countdown_prompt = '''Given the assistant goal, task generation success criteria, and example final answer, generate a task that fulfills the task generation success criteria. The generated task should be:\n",
    "\n",
    "# 1) maximizing user benefit and helpfulness and minimizing user risk, harm, and unwanted outcomes\n",
    "\n",
    "# 2) novel - avoid designing a task that is drawn from the same linguistic templates or differ only in superficial details\n",
    "\n",
    "# 3) difficult yet tractable - the task should be challenging for the assistant to complete, without being completely intractable\n",
    "\n",
    "# 4) clear - the task should be specific enough to not require further user input to understand what is being asked of the assistant and should provide all the context and detail necessary\n",
    "\n",
    "# Show your work in <think> </think> tags. And return the task and final answer in <answer> </answer> tags.\n",
    "\n",
    "\n",
    "# Assistant Goal: Get really good at the Countdown Game with 3 to 4 numbers. The Countdown game is a numbers puzzle where players use a set of randomly drawn numbers and basic arithmetic operations (+, -, *, /) to reach a target number.\n",
    "\n",
    "# Task Generation Success Criteria: The task and final answer must be a dictionary with the 'target', 'nums', and 'prompt' keys. The 'target' key should have a value that is an integer, the 'nums' key should have a value that is a list of 3 or 4 integers, and the 'prompt' key should have a value that is a string in the same exact format as the example final answer prompt, but just with the nums and target updated as appropriate.\n",
    "\n",
    "# Example Final Answer: <answer>\n",
    "# {\n",
    "#     'target': 44,\n",
    "#     'nums': [6, 77, 73, 20],\n",
    "#     'prompt': 'Using the numbers [6, 77, 73, 20], create an equation that equals 44. You can use basic arithmetic operations (+, -, *, /) one or multiple times but each number can only be used once. Show your work in <think> </think> tags. And return the final equation in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>. Think step by step inside <think> tags.'\n",
    "# }\n",
    "# </answer>'''\n",
    "\n",
    "# print(countdown_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_list = [\n",
    "    [\n",
    "        {\n",
    "            \"from\": \"human\",\n",
    "            \"value\": prompt.split(\"<|im_start|>user\")[1].split(\"<|im_end|>\")[0].strip().replace(\"×\", \"*\").replace(\"÷\", \"/\").replace(\"Show your work in <think> </think> tags. And return the final equation in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>. Think step by step inside <think> tags.\", \"\").replace(\"Show your work in <think> </think> tags. And return the task and final answer in <answer> </answer> tags.\\n\\n\\n\", \"\").replace(\"Example Final Answer: <answer>\", \"Example Reasoning and Final Answer Response Format: <think>\\nreasoning process here\\n</think>\\n<answer>\").replace(\"[6, 77, 73, 20]\", \"[1, 2, 3]\").replace(\"44\", \"6\") # having the prompt include the formatting is confusing for the query network when we're using a base model (formatting is always wrong) # countdown_prompt\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"assistant\",\n",
    "            \"ground_truth\": {\n",
    "                \"value\": \"\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    for row in ds[\"train\"]\n",
    "]\n",
    "len(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the assistant goal, task generation success criteria, and example final answer, generate a task that fulfills the task generation success criteria. The generated task should be:\n",
      "\n",
      "1) maximizing user benefit and helpfulness and minimizing user risk, harm, and unwanted outcomes\n",
      "\n",
      "2) novel - avoid designing a task that is drawn from the same linguistic templates or differ only in superficial details\n",
      "\n",
      "3) difficult yet tractable - the task should be challenging for the assistant to complete, without being completely intractable\n",
      "\n",
      "4) clear - the task should be specific enough to not require further user input to understand what is being asked of the assistant and should provide all the context and detail necessary\n",
      "\n",
      "Assistant Goal: Get really good at math.\n",
      "\n",
      "Task Generation Success Criteria: The task and final answer must be a dictionary with 'prompt', 'target', and 'solution' keys. The 'prompt' should be a math problem, the 'target' should be the expected answer to the math problem, and the 'solution' must be a solution to the math problem that clearly and unambiguously leads to the expected answer 'target'.\n",
      "\n",
      "Example Reasoning and Final Answer Response Format: <think>\n",
      "reasoning process here\n",
      "</think>\n",
      "<answer>\n",
      "{\n",
      "    'prompt': 'What is 1 + 1?',\n",
      "    'target': 2,\n",
      "    'solution': 'To solve 1 + 1, we add the two numbers together. Starting with the number 1, adding another 1 gives us 2. Therefore, the answer is 2.'\n",
      "}\n",
      "</answer>\n"
     ]
    }
   ],
   "source": [
    "print(dataset_list[0][0]['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/data/users/kevihuang/projects/Open-Reasoner-Zero/data/math-query-gen-prompts_first_5000_w_solution.json', 'w') as f:\n",
    "    json.dump(dataset_list[:5000], f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Query Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_item = r'''{\n",
    "    'prompt': \"Find the equation of the curve (f(x)), which is the orthogonal trajectory of the given family of curves: x^2 + y^2 = 2cy\",\n",
    "    'target': 0.0,\n",
    "    'solution': \"Step 1: Find the slope of the given family of curves\n",
    "\n",
    "Rewriting the equation of the family of curves:\n",
    "\n",
    "\\(y^2 - 2c y + x^2 = 0\\)\n",
    "\n",
    "Let \\(y = f(x)\\) and we're going to differentiate both sides with respect to x.\n",
    "\n",
    "Step 2: Implicit differentiation\n",
    "\n",
    "\\(2y \\frac{dy}{dx} - 2c + 2x \\frac{dy}{dx} = 0\\)\n",
    "\n",
    "2y \\frac{dy}{dx} + 2x \\frac{dy}{dx} = 2c\n",
    "\n",
    "\\(\\frac{dy}{dx} (2y + 2x) = 2c\\)\n",
    "\n",
    "Step 3: Find slope of orthogonal trajectories\n",
    "\n",
    "The slope of the orthogonal trajectory is the negative reciprocal of the slope of the family of curves.\n",
    "\n",
    "\\(\\frac{dy}{dx} = -\\frac{1}{f'(x)} = -\\frac{1}{\\frac{2c}{2y + 2x}} = -\\frac{y + x}{c}\\)\n",
    "\n",
    "Step 4: Derive the equation of the orthogonal trajectory\n",
    "\n",
    "Using the slope found in step 3, we are going to substitute it into the equation \\(y = f(x)\\) to find the equation of the orthogonal trajectory.\n",
    "\n",
    "Solution: \\(x^2 + y^2 - 2c + 2xy = 0\\)\"\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, ast\n",
    "\n",
    "def fix_multiline_string(dict_str):\n",
    "    # Match keys 'prompt' or 'solution' and capture their values\n",
    "    pattern = r\"('(?:prompt|solution)':\\s*)(['\\\"])(.*?)(\\2)\"\n",
    "    def replacer(match):\n",
    "        start, content, end = match.groups()\n",
    "        fixed_content = content.replace('\\n', '\\\\n')\n",
    "        return f\"{start}{fixed_content}{end}\"\n",
    "    return re.sub(pattern, replacer, dict_str, flags=re.DOTALL)\n",
    "\n",
    "def extract_answer_input(query_item):\n",
    "    system_prefix = \"A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer.\\nThe reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, i.e. your response format should be: <think> reasoning process here </think>\\n\\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\\nThis is the problem:\\n\"\n",
    "    assistant_suffix = \" Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> answer here </answer>. Think step by step inside <think> tags.\\nAssistant: <think>\" # Give example final answer to help policy model understand output better\n",
    "\n",
    "    # TODO: adjust these default values based on domain / dataset! Pull from example final answer from prompt\n",
    "    default_prompt = \"What is 1 + 2?\"\n",
    "    default_extras = {'target': 3, 'valid_prompt': False, 'answer': 3, 'diverse': False}\n",
    "    required_keys = {\"target\", \"prompt\"}\n",
    "    \n",
    "    default_full_prompt = system_prefix + default_prompt + assistant_suffix\n",
    "    default_valid_answer_input = (\n",
    "        default_full_prompt,\n",
    "        default_extras\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        dict_str = query_item            # Content inside <answer>...</answer>\n",
    "        dict_str_fixed = fix_multiline_string(dict_str)\n",
    "        answer_input = ast.literal_eval(dict_str_fixed)\n",
    "        if not isinstance(answer_input, dict) or not required_keys.issubset(answer_input.keys()):\n",
    "            return default_valid_answer_input\n",
    "        prompt_text = str(answer_input.get('prompt')).strip()\n",
    "        full_prompt = system_prefix + prompt_text + assistant_suffix\n",
    "        answer_input['prompt'] = full_prompt\n",
    "        answer_input['target'] = str(float(answer_input.get('target'))) # don't force to be an int unless for countdown\n",
    "        answer_input['answer'] = answer_input['target']\n",
    "        answer_input['nums'] = [int(x) for x in answer_input.get('nums', [])]\n",
    "\n",
    "        answer_input['valid_prompt'] = True\n",
    "        return (full_prompt, answer_input)\n",
    "    except Exception as e:\n",
    "        return default_valid_answer_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer.\\nThe reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, i.e. your response format should be: <think> reasoning process here </think>\\n\\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\\nThis is the problem:\\nWhat is 1 + 2? Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> answer here </answer>. Think step by step inside <think> tags.\\nAssistant: <think>',\n",
       " {'target': 3, 'valid_prompt': False, 'answer': 3, 'diverse': False})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_answer_input(query_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer.\\nThe reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, i.e. your response format should be: <think> reasoning process here </think>\\n\\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\\nThis is the problem:\\nCalculate the sum of the first 200 prime numbers Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> answer here </answer>. Think step by step inside <think> tags.\\nAssistant: <think>',\n",
       " {'prompt': 'A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer.\\nThe reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, i.e. your response format should be: <think> reasoning process here </think>\\n\\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\\nThis is the problem:\\nCalculate the sum of the first 200 prime numbers Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> answer here </answer>. Think step by step inside <think> tags.\\nAssistant: <think>',\n",
       "  'target': '2150.0',\n",
       "  'solution': 'The sum of the first 200 prime numbers is 2150.',\n",
       "  'answer': '2150.0',\n",
       "  'nums': [],\n",
       "  'valid_prompt': True})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_item = r'''{\n",
    "  'prompt': \"Calculate the sum of the first 200 prime numbers\",\n",
    "  'target': 2150,\n",
    "  'solution': \"The sum of the first 200 prime numbers is 2150.\"\n",
    "}'''\n",
    "\n",
    "\n",
    "import re, ast\n",
    "\n",
    "def fix_multiline_string(dict_str):\n",
    "    # Match keys 'prompt' or 'solution' and capture their values\n",
    "    pattern = r\"((?:'prompt'|\\\"prompt\\\"|'solution'|\\\"solution\\\")\\s*:\\s*)(['\\\"])(.*?)(\\2)\"\n",
    "    def replacer(match):\n",
    "        start, quote, content, _ = match.groups()\n",
    "        fixed_content = content.replace('\\n', '\\\\n')\n",
    "        return f\"{start}{quote}{fixed_content}{quote}\"\n",
    "    return re.sub(pattern, replacer, dict_str, flags=re.DOTALL)\n",
    "\n",
    "def extract_answer_input(query_item):\n",
    "    system_prefix = \"A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer.\\nThe reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, i.e. your response format should be: <think> reasoning process here </think>\\n\\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\\nThis is the problem:\\n\"\n",
    "    assistant_suffix = \" Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> answer here </answer>. Think step by step inside <think> tags.\\nAssistant: <think>\" # Give example final answer to help policy model understand output better\n",
    "\n",
    "    # TODO: adjust these default values based on domain / dataset! Pull from example final answer from prompt\n",
    "    default_prompt = \"What is 1 + 2?\"\n",
    "    default_extras = {'target': 3, 'valid_prompt': False, 'answer': 3, 'diverse': False}\n",
    "    required_keys = {\"target\", \"prompt\"}\n",
    "    \n",
    "    default_full_prompt = system_prefix + default_prompt + assistant_suffix\n",
    "    default_valid_answer_input = (\n",
    "        default_full_prompt,\n",
    "        default_extras\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        dict_str = query_item            # Content inside <answer>...</answer>\n",
    "        dict_str_fixed = fix_multiline_string(dict_str)\n",
    "        answer_input = ast.literal_eval(dict_str_fixed)\n",
    "        if not isinstance(answer_input, dict) or not required_keys.issubset(answer_input.keys()):\n",
    "            return default_valid_answer_input\n",
    "        prompt_text = str(answer_input.get('prompt')).strip()\n",
    "        full_prompt = system_prefix + prompt_text + assistant_suffix\n",
    "        answer_input['prompt'] = full_prompt\n",
    "        answer_input['target'] = str(float(answer_input.get('target'))) # don't force to be an int unless for countdown\n",
    "        answer_input['answer'] = answer_input['target']\n",
    "        answer_input['nums'] = [int(x) for x in answer_input.get('nums', [])]\n",
    "\n",
    "        answer_input['valid_prompt'] = True\n",
    "        return (full_prompt, answer_input)\n",
    "    except Exception as e:\n",
    "        return default_valid_answer_input\n",
    "    \n",
    "\n",
    "extract_answer_input(query_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the matching\n",
    "\n",
    "''' Generate VLLM Raw Output: {'response': \" Here are the numbers provided: [1, 2, 3]. The goal is to create an equation that equals 6 using only these three numbers, each number used once, and basic arithmetic operations (+, -, *, /).\\n\\nStep 1: Identify the possible combinations.\\n1. Trying multiplication: (1 * 2) * 3 = 2 * 3 = 6. This seems to be one way to get the result, but we need to ensure that each number is used only once.\\n\\nStep 2: Check for other combinations using division.\\n1. Trying division: (1 / 2) * 3 = 0.5 * 3 = 1.5, which is not equal to 6. This doesn't work.\\n2. Trying another combination: 2 * 3 = 6. But we need to use all three numbers in the equation.\\n\\nStep 3: Check for other combinations using addition and subtraction.\\n1. Trying subtraction: (1 + 2) - 3 = 3 - 3 = 0, which is not equal to 6. This doesn't work.\\n2. Trying the reverse: (1 - 2) + 3 = -1 + 3 = 2, which is also not equal to 6. This doesn't work either.\\n\\nSo, none of the combinations using addition, subtraction, or division seem to work. Therefore, the only solution is the equation found in Step 1.</think>\\n\\n<answer> (1 * 2) * 3 </answer>\", 'iscorrect': False, 'stop_reason': 'stop', 'final_answer': '(1 * 2) * 3', 'repeat_score': 0.011156459818955309, 'reflection_pattern_score': 0}\n",
    "2025-03-03 18:26:26.804 | DEBUG    | orz.ppo.trainer:make_experience:1087 -     Extras: {'target': 6, 'nums': [1, 2, 3], 'valid_prompt': False}'''\n",
    "\n",
    "final_answer = '(1 * 2) * 3'\n",
    "extra = {'target': 6, 'nums': [1, 2, 3], 'valid_prompt': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "equation = final_answer\n",
    "\n",
    "import re\n",
    "import ast\n",
    "used_numbers = [int(n) for n in re.findall(r'\\d+', equation)]\n",
    "numbers = ast.literal_eval(str(extra[\"nums\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(used_numbers) != sorted(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_pattern = r'^[\\d+\\-*/().\\s]+$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 11), match='(1 * 2) * 3'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(allowed_pattern, equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = eval(equation, {\"__builtins__\": None}, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(float(result) - float(extra[\"target\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking extract answer dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_item = \"{\\n    'prompt': 'What is 1 + 2?',\\n    'target': 3,\\n    'solution': 'To solve 1 + 2, we add the two numbers together. Starting with the number 1, adding another 2 gives us 3. Therefore, the answer is 3.'\\n}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "def fix_multiline_string(dict_str):\n",
    "    pattern = r\"('prompt':\\s*')(.*?)(')\"\n",
    "    def replacer(match):\n",
    "        start, content, end = match.groups()\n",
    "        fixed_content = content.replace('\\n', '\\\\n')\n",
    "        return f\"{start}{fixed_content}{end}\"\n",
    "    return re.sub(pattern, replacer, dict_str, flags=re.DOTALL)\n",
    "\n",
    "def extract_answer_input(query_item):\n",
    "    system_prefix = \"A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer.\\nThe reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, i.e. your response format should be: <think> reasoning process here </think>\\n\\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\\nThis is the problem:\\n\"\n",
    "    assistant_suffix = \" Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> answer here </answer>. Think step by step inside <think> tags.\\nAssistant: <think>\" # Give example final answer to help policy model understand output better\n",
    "\n",
    "    # TODO: adjust these default values based on domain / dataset! Pull from example final answer from prompt\n",
    "    default_prompt = \"What is 1 + 2?\"\n",
    "    default_extras = {'target': 3, 'valid_prompt': False, 'answer': 3, 'diverse': False}\n",
    "    required_keys = {\"target\", \"prompt\"}\n",
    "    \n",
    "    default_full_prompt = system_prefix + default_prompt + assistant_suffix\n",
    "    default_valid_answer_input = (\n",
    "        default_full_prompt,\n",
    "        default_extras\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        print('hi')\n",
    "        dict_str = query_item            # Content inside <answer>...</answer>\n",
    "        dict_str_fixed = fix_multiline_string(dict_str)\n",
    "        answer_input = ast.literal_eval(dict_str_fixed)\n",
    "        print(answer_input)\n",
    "        if not isinstance(answer_input, dict) or not required_keys.issubset(answer_input.keys()):\n",
    "            return default_valid_answer_input\n",
    "        print('a')\n",
    "        prompt_text = str(answer_input.get('prompt')).strip()\n",
    "        full_prompt = system_prefix + prompt_text + assistant_suffix\n",
    "        answer_input['prompt'] = full_prompt\n",
    "        answer_input['target'] = str(float(answer_input.get('target')))\n",
    "        answer_input['answer'] = answer_input['target']\n",
    "        answer_input['nums'] = [int(x) for x in answer_input.get('nums', [])]\n",
    "\n",
    "        answer_input['valid_prompt'] = True\n",
    "\n",
    "        # Diversity reward # TODO: can be improved for matching and ensuring that the prompt AND target are not in self.history query section\n",
    "        answer_input['diverse'] = True\n",
    "        # if len(self.history) >= 6:\n",
    "        #     last_query_response, last_query_reward, last_query_avg_success, last_query_final_answer = self.history[-6]\n",
    "        #     if prompt_text in last_query_final_answer:\n",
    "        #         answer_input['diverse'] = False\n",
    "        # if len(self.history) >= 4:\n",
    "        #     last_query_response, last_query_reward, last_query_avg_success, last_query_final_answer = self.history[-4]\n",
    "        #     if prompt_text in last_query_final_answer:\n",
    "        #         answer_input['diverse'] = False\n",
    "        # if len(self.history) >= 2:\n",
    "        #     last_query_response, last_query_reward, last_query_avg_success, last_query_final_answer = self.history[-2]\n",
    "        #     if prompt_text in last_query_final_answer:\n",
    "        #         answer_input['diverse'] = False\n",
    "\n",
    "        return (full_prompt, answer_input)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return default_valid_answer_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "{'prompt': 'What is 1 + 2?', 'target': 3, 'solution': 'To solve 1 + 2, we add the two numbers together. Starting with the number 1, adding another 2 gives us 3. Therefore, the answer is 3.'}\n",
      "a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer.\\nThe reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, i.e. your response format should be: <think> reasoning process here </think>\\n\\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\\nThis is the problem:\\nWhat is 1 + 2? Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> answer here </answer>. Think step by step inside <think> tags.\\nAssistant: <think>',\n",
       " {'prompt': 'A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer.\\nThe reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, i.e. your response format should be: <think> reasoning process here </think>\\n\\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\\nThis is the problem:\\nWhat is 1 + 2? Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> answer here </answer>. Think step by step inside <think> tags.\\nAssistant: <think>',\n",
       "  'target': '3.0',\n",
       "  'solution': 'To solve 1 + 2, we add the two numbers together. Starting with the number 1, adding another 2 gives us 3. Therefore, the answer is 3.',\n",
       "  'answer': '3.0',\n",
       "  'nums': [],\n",
       "  'valid_prompt': True,\n",
       "  'diverse': True})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_answer_input(query_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer.\\nThe reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, i.e. your response format should be: <think> reasoning process here </think>\\n\\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\\nThis is the problem:\\nConsider the following system of linear equations in two variables:\\n                        3x + y = b\\n                        x - 2y = c\\n\\nDetermine whether the system has a unique solution, no solution, or infinitely many solutions based on the values of b and c. Provide the solution if the system has a unique solution.\\n           Format your solution as a modified equation where A, B, C, and D represent integers such that the equation represents a non-ambiguous value. Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> answer here </answer>. Think step by step inside <think> tags.\\nAssistant: <think>',\n",
       " {'prompt': 'A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer.\\nThe reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, i.e. your response format should be: <think> reasoning process here </think>\\n\\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\\nThis is the problem:\\nConsider the following system of linear equations in two variables:\\n                        3x + y = b\\n                        x - 2y = c\\n\\nDetermine whether the system has a unique solution, no solution, or infinitely many solutions based on the values of b and c. Provide the solution if the system has a unique solution.\\n           Format your solution as a modified equation where A, B, C, and D represent integers such that the equation represents a non-ambiguous value. Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> answer here </answer>. Think step by step inside <think> tags.\\nAssistant: <think>',\n",
       "  'target': '3.0',\n",
       "  'solution': 'The modified equation is 2x + 3y = (6b + 15 - 4c) / 5, where A = 2, B = 3, C = (6b + 15 - 4c) / 5, and D is not relevant.',\n",
       "  'answer': '3.0',\n",
       "  'nums': [],\n",
       "  'valid_prompt': True})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_item = r'''{\n",
    "    'prompt': 'Consider the following system of linear equations in two variables:\n",
    "                        3x + y = b\n",
    "                        x - 2y = c\n",
    "\n",
    "Determine whether the system has a unique solution, no solution, or infinitely many solutions based on the values of b and c. Provide the solution if the system has a unique solution.\n",
    "           Format your solution as a modified equation where A, B, C, and D represent integers such that the equation represents a non-ambiguous value.\n",
    "',\n",
    "    'target': 3.0,\n",
    "    'solution': 'The modified equation is 2x + 3y = (6b + 15 - 4c) / 5, where A = 2, B = 3, C = (6b + 15 - 4c) / 5, and D is not relevant.'\n",
    "}'''\n",
    "\n",
    "\n",
    "import re, ast\n",
    "\n",
    "def fix_multiline_string(dict_str):\n",
    "    # Match keys 'prompt' or 'solution' and capture their values\n",
    "    pattern = r\"((?:'prompt'|\\\"prompt\\\"|'solution'|\\\"solution\\\")\\s*:\\s*)(['\\\"])(.*?)(\\2)\"\n",
    "    def replacer(match):\n",
    "        start, quote, content, _ = match.groups()\n",
    "        fixed_content = content.replace('\\n', '\\\\n')\n",
    "        return f\"{start}{quote}{fixed_content}{quote}\"\n",
    "    return re.sub(pattern, replacer, dict_str, flags=re.DOTALL)\n",
    "\n",
    "def extract_answer_input(query_item):\n",
    "    system_prefix = \"A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer.\\nThe reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, i.e. your response format should be: <think> reasoning process here </think>\\n\\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\\nThis is the problem:\\n\"\n",
    "    assistant_suffix = \" Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> answer here </answer>. Think step by step inside <think> tags.\\nAssistant: <think>\" # Give example final answer to help policy model understand output better\n",
    "\n",
    "    # TODO: adjust these default values based on domain / dataset! Pull from example final answer from prompt\n",
    "    default_prompt = \"What is 1 + 2?\"\n",
    "    default_extras = {'target': 3, 'valid_prompt': False, 'answer': 3, 'diverse': False}\n",
    "    required_keys = {\"target\", \"prompt\"}\n",
    "    \n",
    "    default_full_prompt = system_prefix + default_prompt + assistant_suffix\n",
    "    default_valid_answer_input = (\n",
    "        default_full_prompt,\n",
    "        default_extras\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        dict_str = query_item            # Content inside <answer>...</answer>\n",
    "        dict_str_fixed = fix_multiline_string(dict_str)\n",
    "        answer_input = ast.literal_eval(dict_str_fixed)\n",
    "        if not isinstance(answer_input, dict) or not required_keys.issubset(answer_input.keys()):\n",
    "            return default_valid_answer_input\n",
    "        prompt_text = str(answer_input.get('prompt')).strip()\n",
    "        full_prompt = system_prefix + prompt_text + assistant_suffix\n",
    "        answer_input['prompt'] = full_prompt\n",
    "        answer_input['target'] = str(float(answer_input.get('target'))) # don't force to be an int unless for countdown\n",
    "        answer_input['answer'] = answer_input['target']\n",
    "        answer_input['nums'] = [int(x) for x in answer_input.get('nums', [])]\n",
    "\n",
    "        answer_input['valid_prompt'] = True\n",
    "        return (full_prompt, answer_input)\n",
    "    except Exception as e:\n",
    "        return default_valid_answer_input\n",
    "    \n",
    "\n",
    "extract_answer_input(query_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format is incorrect\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFormat is incorrect\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Extract the \"answer\" part from the completion\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mmatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "response = r''' To generate a task that fulfills the generation criteria, let's break down the creation of each part of the dictionary:\n",
    "\n",
    "1. Prompt: It should be a concise but detailed math problem that is challenging yet tractable. It should be designed to encourage deeper and longer thinking from the Policy Assistant.\n",
    "\n",
    "2. Target: It should be an integer or float that represents the correct final answer to the given math problem.\n",
    "\n",
    "3. Solution: It should present a detailed and logical reasoning process which would lead to calculating the target value.\n",
    "\n",
    "Here's an example of a new task that follows the given criteria:\n",
    "\n",
    "<think>\n",
    "First, let's think about a math problem that's about percentage increase. The problem should involve three digits number and a percentage increase.\n",
    "\n",
    "Step 1: Find the difference between a three-digit number and the result of this number when increased by a percentage.\n",
    "\n",
    "Let's consider the problem of \"If 25% of a three-digit number is 75, what is the original number?\".\n",
    "\n",
    "Answer: If 25% of a three-digit number is 75, then the original number is 75 / 0.25 = 300. \n",
    "\n",
    "So, our solution would be: \"If 25% of a three-digit number is 75, what is the original number? The original number is 300.\"\n",
    "</think>\n",
    "<answer>\n",
    "{\n",
    "    \"prompt\": \"If 25% of a three-digit number is 75, what is the original number?\",\n",
    "    \"target\": 300,\n",
    "    \"solution\": \"If 25% of a three-digit number is 75, then the original number is 75 / 0.25 = 300.\"\n",
    "}\n",
    "</answer>'''\n",
    "\n",
    "if not response.startswith(\"<think>\"):\n",
    "    response = \"<think>\" + response\n",
    "\n",
    "# Check if the format is correct # allowing zero or more newlines in between </think> and <answer> because of Qwen formatting for now, which can be fixed via SFT and is NOT a focus of training\n",
    "regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\s*<answer>([\\s\\S]*)<\\/answer>$\"\n",
    "\n",
    "match = re.search(regex, response, re.DOTALL) \n",
    "# if the format is not correct, reward is 0, or the completion doesn't end in </answer>\n",
    "if match is None or \\\n",
    "    len(match.groups()) != 2 \\\n",
    "    or not response.endswith(\"</answer>\"):\n",
    "    print(\"Format is incorrect\")\n",
    "\n",
    "# Extract the \"answer\" part from the completion\n",
    "answer = match.group(2).strip()\n",
    "print(f\"Extracted answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
