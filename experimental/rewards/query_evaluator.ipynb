{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Full Response Content:\n",
      "<think>\n",
      "To evaluate the Query Assistant Response against the Task Generation Success Criteria, I will analyze each criterion step-by-step:\n",
      "\n",
      "1. **Maximizing user and Policy Assistant benefit and helpfulness and minimizing user and Policy Assistant risk, harm, and unwanted outcomes**:\n",
      "   - The task is a math problem that aligns with the Policy Assistant's goal of getting really good at math. It does not pose any risk or harm and is purely educational.\n",
      "\n",
      "2. **Novel - avoid designing a task that is drawn from the same linguistic templates or differ only in superficial details**:\n",
      "   - The task is novel compared to the previous tasks in the game history. It involves a different type of math problem (arithmetic with large numbers) rather than geometry (circles).\n",
      "\n",
      "3. **Difficult yet tractable - the task should be challenging for the Policy Assistant to complete, without being completely intractable. A Query Average Success score of 0.5 is ideal**:\n",
      "   - The task appears to be more challenging than the previous ones, which were relatively straightforward geometry problems. This problem involves large number arithmetic, which could be more difficult but still solvable. It seems to be at an appropriate difficulty level.\n",
      "\n",
      "4. **Clear - the task should be specific enough to not require further user input to understand what is being asked of the Policy Assistant and should provide all the context and detail necessary**:\n",
      "   - The task is clear and specific. The prompt is straightforward, and the solution is detailed and unambiguous.\n",
      "\n",
      "The Query Assistant Response meets all the criteria: it is beneficial and helpful, novel, appropriately challenging, and clear. Therefore, it fulfills the Task Generation Success Criteria.\n",
      "\n",
      "</think>\n",
      "<answer>\n",
      "yes\n",
      "</answer>\n",
      "Extracted decision: yes\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "data = {\n",
    "    \"temperature\": 0.0,\n",
    "    \"max_tokens\": 4096,\n",
    "    \"messages\": [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": '''You are GPT-4o. Your task is to evaluate a Query Assistant Response—generated by a Qwen-2.5-7B base model—against the Task Generation Success Criteria defined in the Query Assistant Prompt.\n",
    "            \n",
    "Please evaluate whether the Query Assistant Response fulfills ALL of these criteria. Show your step-by-step reasoning inside <think> and </think> tags, then provide your final decision inside <answer> and </answer> tags as a single word: \"yes\" if it meets the criteria or \"no\" if it does not.\n",
    "\n",
    "<think>\n",
    "[Provide your detailed evaluation and reasoning here]\n",
    "</think>\n",
    "<answer>\n",
    "[yes/no]\n",
    "</answer>'''\n",
    "        }, {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": '''QUERY ASSISTANT PROMPT: A Policy Assistant Goal set by a User, and a game between a Query Assistant and a Policy Assistant. \n",
    "                    \n",
    "The User gives the Policy Assistant Goal and tasks the Query Assistant to generate a query to challenge and help the Policy Assistant better reach the Policy Assistant Goal, then the Policy Assistant tries to solve the Query Assistant's generated query, then the Query Assistant generates another query, then the Policy Assistant tries to solve that query, etc. After each Query Assistant response, there is a Query Average Success score that signals the query's difficulty to the Policy Assistant, where 0.0 means the query is currently too difficult or impossible for the Policy Assistant and 1.0 means the query is currently too easy for the Policy Assistant. After each Policy Assistant response, there is Policy Reward where 0.0 means that the Policy Assistant was not able to reach the same target answer proposed by the Query Assistant and 1.0 means that the Policy Assistant was able to reach the same target answer. The Query and Policy Assistant both first think about the reasoning process in the mind and then provides the Policy or Query Assistant with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e. your response format should be: <think> reasoning process here </think>\n",
    "\n",
    "<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\n",
    "\n",
    "This is the problem:\n",
    "Given the Policy Assistant Goal, Game History of up to the last 3 Query Assistant Answer and Average Success and Policy Assistant Answer and Reward turns, Task Generation Success Criteria, and Example Final Answer Format, generate a task that fulfills the task generation success criteria. \n",
    "\n",
    "\n",
    "# Policy Assistant Goal: \n",
    "Get really good at math.\n",
    "\n",
    "\n",
    "# Game History:\n",
    "\n",
    "**Turn 1**\n",
    "Query Assistant Answer: {\n",
    "    \"prompt\": \"If the radius of a circle is 30, what is its area?\",\n",
    "    \"target\": 2827.43,\n",
    "    \"solution\": \"The area of a circle is given by the formula A = πr^2. Given a radius of 30, the area is calculated by squaring the radius, 30^2 = 900, and then multiplying by π, approximately 3.14, therefore A = 2827.43 square units.\"\n",
    "}\n",
    "\n",
    "Query Average Success: 0.75\n",
    "\n",
    "Policy Assistant Answer: 2827.43\n",
    "\n",
    "Policy Reward: 1.0\n",
    "\n",
    "**Turn 2**\n",
    "Query Assistant Answer: {\n",
    "    \"prompt\": \"What is the circumference of a circle with a diameter of 14?\",\n",
    "    \"target\": 43.98,\n",
    "    \"solution\": \"The circumference of a circle can be calculated using the formula C = πd, where d is the diameter. Given a diameter of 14, we multiply by π (approximately 3.14), yielding a circumference of 43.98 units.\"\n",
    "}\n",
    "\n",
    "Query Average Success: 0.38\n",
    "\n",
    "Policy Assistant Answer: 43.98\n",
    "\n",
    "Policy Reward: 1.0\n",
    "\n",
    "**Turn 3**\n",
    "Query Assistant Answer: {\n",
    "    \"prompt\": \"If the radius of a circle is 7, what is its circumference?\",\n",
    "    \"target\": 43.98,\n",
    "    \"solution\": \"The circumference of a circle can be calculated using the formula C = 2πr, where r is the radius. Given a radius of 7, we multiply by 2 times π (approximately 3.14), resulting in a circumference of 43.98 units.\"\n",
    "}\n",
    "\n",
    "Query Average Success: 0.75\n",
    "\n",
    "Policy Assistant Answer: 43.98\n",
    "\n",
    "Policy Reward: 1.0\n",
    "\n",
    "\n",
    "# Task Generation Success Criteria: The generated task should be:\n",
    "\n",
    "1) maximizing user and Policy Assistant benefit and helpfulness and minimizing user and Policy Assistant risk, harm, and unwanted outcomes\n",
    "\n",
    "2) novel - avoid designing a task that is drawn from the same linguistic templates or differ only in superficial details\n",
    "\n",
    "3) difficult yet tractable - the task should be challenging for the Policy Assistant to complete, without being completely intractable. A Query Average Success score of 0.5 is ideal.\n",
    "\n",
    "4) clear - the task should be specific enough to not require further user input to understand what is being asked of the Policy Assistant and should provide all the context and detail necessary. The task and final answer must be a dictionary with 'prompt', 'target', and 'solution' keys. The 'prompt' should be a math problem, the 'target' should be the expected answer to the math problem as an int or float, and the 'solution' must be a solution to the math problem that clearly and unambiguously leads to the expected answer 'target'.\n",
    "\n",
    "                    \n",
    "# Example Query Assistant Reasoning and Final Answer Response Format: <think>\n",
    "reasoning process here\n",
    "</think>\n",
    "<answer>\n",
    "{\n",
    "    'prompt': 'What is 1 + 1?',\n",
    "    'target': 2,\n",
    "    'solution': 'To solve 1 + 1, we add the two numbers together. Starting with the number 1, adding another 1 gives us 2. Therefore, the answer is 2.'\n",
    "}\n",
    "</answer>\n",
    "\n",
    "\n",
    "Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> answer here </answer>. Think step by step inside <think> tags.\n",
    "Query Assistant: <think>''',\n",
    "        }, {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": '''QUERY ASSISTANT RESPONSE: When creating a task that maximizes user and Policy Assistant benefit and helpfulness and minimizes user and Policy Assistant risk, harm, and unwanted outcomes, I must ensure that the task is clear, novel, difficult yet tractable, and aligned with the Policy Assistant's goal of getting really good at math. To follow the success criteria, the final answer must be a dictionary with 'prompt', 'target', and 'solution' keys. The 'prompt' should be a math problem, the 'target' should be the expected answer to the math problem, and the 'solution' must be a clear and unambiguous solution to the math problem.</think>\\n<answer>\\n{\\n    \"prompt\": \"What is the value of $9901\\\\cdot101-99\\\\cdot10101?$\",\\n    \"target\": 2,\\n    \"solution\": \"The likely fastest method will be direct computation. $9901\\\\cdot101$ evaluates to $1000001$ and $99\\\\cdot10101$ evaluates to $999999$. The difference is \\\\boxed{\\\\textbf{(A) }2}.\"\\n}\\n</answer>'''\n",
    "        }]\n",
    "    }],\n",
    "    \"stop\": [\"<|im_end|>\"],\n",
    "    \"n\": 1,\n",
    "    \"model\": \"dev-gpt-4o-vision-2024-05-13\"\n",
    "}\n",
    "\n",
    "url = \"https://skilled-chigger-mentally.ngrok-free.app/api/send_request\"\n",
    "response = requests.post(url, json=data)\n",
    "print(response)\n",
    "\n",
    "# Print the full response content\n",
    "response_content = response.json()['choices'][0]['message']['content']\n",
    "print(\"Full Response Content:\")\n",
    "print(response_content)\n",
    "\n",
    "# Extract the text between the <answer> tags using a regular expression\n",
    "match = re.search(r'<answer>\\s*(.*?)\\s*</answer>', response_content, re.DOTALL)\n",
    "if match:\n",
    "    extracted_answer = match.group(1).strip()\n",
    "    # Optionally, if you expect only 'yes' or 'no' you can enforce that:\n",
    "    decision = extracted_answer.lower()\n",
    "    if decision in ['yes', 'no']:\n",
    "        print(\"Extracted decision:\", decision)\n",
    "    else:\n",
    "        print(\"Extracted answer does not contain a simple 'yes' or 'no':\")\n",
    "        print(extracted_answer)\n",
    "else:\n",
    "    print(\"No <answer> tags found in the response.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
