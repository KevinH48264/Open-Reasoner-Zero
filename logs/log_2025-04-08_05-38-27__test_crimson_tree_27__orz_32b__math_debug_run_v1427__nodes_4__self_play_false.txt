[2025-04-08 05:38:42,820] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
df: /home/aiscuser/.triton/autotune: No such file or directory
2025-04-08 05:38:51.881 | INFO     | __main__:PPOExpConfig:160 - prompt_data: ['data/orz_math_57k_collected.json'], eval_prompt_data: ['data/eval_data/math500.json', 'data/eval_data/aime2024.json', 'data/eval_data/gpqa_diamond.json'], prompt_data_probs: [1.0]
2025-04-08 05:38:51.892 | INFO     | __main__:<module>:2753 - --------- config key ---------                  ------ value ------
seed                                            42
ref_num_nodes                                   12
ref_num_gpus_per_node                           1
reward_num_nodes                                12
reward_num_gpus_per_node                        1
actor_num_nodes                                 12
actor_num_gpus_per_node                         1
critic_num_nodes                                12
critic_num_gpus_per_node                        1
colocate_critic_reward                          True
colocate_actor_ref                              True
colocate_all                                    False
vllm_num_engines                                2
vllm_tensor_parallel_size                       4
vllm_sync_backend                               nccl
local_rank                                      -1
pretrain                                        Qwen/Qwen2.5-32B
critic_pretrain                                 Qwen/Qwen2.5-32B
reward_pretrain                                 <class 'NoneType'>
ckpt_path                                       /vc_data_blob/users/kevihuang/models/orz/orz_ckpt/debug_orz_7b_ppo_self_play__math__v1427__self_play_False
save_path                                       /vc_data_blob/users/kevihuang/models/orz/orz_ckpt/debug_orz_7b_ppo_self_play__math__v1427__self_play_False
tensorboard_log_dir                             orz_logs/debug_orz_7b_ppo_self_play__math__v1427__self_play_False
prompt_data                                     <class 'omegaconf.listconfig.ListConfig'>
load_checkpoint                                 True
zero_stage                                      3
bf16                                            True
zpg                                             1
adam_offload                                    False
flash_attn                                      True
grad_accum_dtype                                <class 'NoneType'>
disable_trace_cache                             False
gradient_checkpointing                          True
gradient_checkpointing_use_reentrant            False
disable_fast_tokenizer                          False
target_modules                                  all-linear
enable_prefix_caching                           True
enable_chunked_prefill                          False
max_num_batched_tokens                          2048
enforce_eager                                   False
gpu_memory_utilization                          0.5
eval_steps                                      -1
save_steps                                      -1
save_interval                                   50
actor_learning_rate                             1e-06
critic_learning_rate                            5e-06
num_episodes                                    20
max_epochs                                      1
prompt_max_len                                  8000
generate_max_len                                8000
train_batch_size                                256
micro_train_batch_size                          1
rollout_batch_size                              64
micro_rollout_batch_size                        240
micro_forward_batch_size                        1
policy_update_steps                             1
critic_update_steps                             1
max_len                                         8192
max_norm                                        1.0
num_warmup_steps                                50
l2                                              0.0
eps_clip                                        0.2
value_clip                                      0.2
lambd                                           1.0
gamma                                           1.0
normalize_reward                                True
top_p                                           1.0
temperature                                     1.0
freezing_actor_steps                            -1
n_samples_per_prompt                            64
kl_target                                       <class 'NoneType'>
init_kl_coef                                    0
use_kl_estimator_k3                             True
use_abs_kl                                      False
use_kl_loss                                     True
kl_loss_coef                                    0.0
adam_betas                                      (0.9, 0.95)
reward_clip_range                               (-10, 10)
use_compute_reward_fn                           True
advantage_normalize                             True
value_head_prefix                               value_head
ref_reward_offload                              False
enable_eval                                     True
eval_interval                                   10
update_ref_every_epoch                          True
use_orm_score                                   False
train_num_nodes_per_group                       12
n_policy_evaluator_samples_per_policy_response  1
eval_prompt_data                                <class 'omegaconf.listconfig.ListConfig'>
prompt_data_probs                               <class 'omegaconf.listconfig.ListConfig'>
packing_max_len                                 16384
top_k                                           -1
stop                                            <class 'omegaconf.listconfig.ListConfig'>
use_grpo                                        False
2025-04-08 05:38:51,981	INFO worker.py:1636 -- Connecting to existing Ray cluster at address: 10.4.68.80:6379...
2025-04-08 05:38:51,998	INFO worker.py:1821 -- Connected to Ray cluster.
2025-04-08 05:39:00.178 | INFO     | __main__:train_dataset:2697 - Start processing 56878 dialogues
[36m(LLMActor pid=12145)[0m Connecting to existing Ray cluster at address: 10.4.68.80:6379...
[36m(LLMActor pid=12145)[0m Calling ray.init() again after it has already been called.
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Connecting to existing Ray cluster at address: 10.4.68.80:6379...
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Calling ray.init() again after it has already been called.
2025-04-08 05:39:39.835 | INFO     | __main__:train_dataset:2708 - Finished processing 56878 dialogues
2025-04-08 05:39:39.882 | INFO     | __main__:eval_dataset:2726 - Start processing 728 dialogues
2025-04-08 05:39:40.391 | INFO     | __main__:eval_dataset:2747 - Finished processing 728 dialogues
[36m(pid=5104, ip=10.4.38.122)[0m df: /home/aiscuser/.triton/autotune: No such file or directory
[36m(pid=5270, ip=10.4.40.184)[0m df: /home/aiscuser/.triton/autotune: No such file or directory
[36m(pid=5271, ip=10.4.40.184)[0m df: /home/aiscuser/.triton/autotune: No such file or directory
[36m(PolicyRayActorBase pid=5265, ip=10.4.38.122)[0m [W408 05:40:31.015661624 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[36m(pid=5267, ip=10.4.40.184)[0m df: /home/aiscuser/.triton/autotune: No such file or directory[32m [repeated 2x across cluster][0m
[36m(PolicyRayActorBase pid=5263, ip=10.4.38.122)[0m Downloading shards:   0%|          | 0/17 [00:00<?, ?it/s]
[36m(RefRayActorBase pid=5579, ip=10.4.40.184)[0m [W408 05:40:35.498641226 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())[32m [repeated 23x across cluster][0m
[36m(pid=5837, ip=10.4.34.192)[0m df: /home/aiscuser/.triton/autotune: No such file or directory
[36m(PolicyRayActorBase pid=5271, ip=10.4.40.184)[0m Downloading shards:   0%|          | 0/17 [00:00<?, ?it/s][32m [repeated 23x across cluster][0m
[36m(CriticRayActorBase pid=13501)[0m [W408 05:40:43.386027802 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[36m(pid=5836, ip=10.4.34.192)[0m df: /home/aiscuser/.triton/autotune: No such file or directory
[36m(CriticRayActorBase pid=13502)[0m [W408 05:40:43.390507508 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[36m(CriticRayActorBase pid=13501)[0m Downloading shards:   0%|          | 0/17 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m Downloading shards:   6%|â–Œ         | 1/17 [00:10<02:41, 10.08s/it]
[36m(CriticRayActorBase pid=6043, ip=10.4.40.184)[0m [W408 05:40:49.572177636 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())[32m [repeated 6x across cluster][0m
[36m(CriticRayActorBase pid=6043, ip=10.4.40.184)[0m Downloading shards:   0%|          | 0/17 [00:00<?, ?it/s][32m [repeated 4x across cluster][0m
[36m(PolicyRayActorBase pid=5267, ip=10.4.40.184)[0m Downloading shards:   6%|â–Œ         | 1/17 [00:18<05:02, 18.90s/it][32m [repeated 16x across cluster][0m
[36m(CriticRayActorBase pid=13205)[0m [W408 05:40:49.934438220 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())[32m [repeated 4x across cluster][0m
[36m(CriticRayActorBase pid=5839, ip=10.4.34.192)[0m Downloading shards:   0%|          | 0/17 [00:00<?, ?it/s][32m [repeated 7x across cluster][0m
[36m(CriticRayActorBase pid=5838, ip=10.4.34.192)[0m Downloading shards:   6%|â–Œ         | 1/17 [00:15<04:12, 15.79s/it][32m [repeated 28x across cluster][0m
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m Downloading shards:  18%|â–ˆâ–Š        | 3/17 [00:37<02:58, 12.75s/it][32m [repeated 20x across cluster][0m
[36m(PolicyRayActorBase pid=5270, ip=10.4.40.184)[0m Downloading shards:  18%|â–ˆâ–Š        | 3/17 [00:48<03:43, 15.98s/it][32m [repeated 16x across cluster][0m
[36m(CriticRayActorBase pid=5839, ip=10.4.34.192)[0m Downloading shards:  12%|â–ˆâ–        | 2/17 [00:42<05:35, 22.35s/it][32m [repeated 28x across cluster][0m
[36m(PolicyRayActorBase pid=5270, ip=10.4.40.184)[0m Downloading shards:  24%|â–ˆâ–ˆâ–Ž       | 4/17 [01:02<03:16, 15.09s/it][32m [repeated 8x across cluster][0m
[36m(CriticRayActorBase pid=13205)[0m Downloading shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [00:57<00:48,  6.01s/it][32m [repeated 28x across cluster][0m
[36m(CriticRayActorBase pid=13205)[0m Downloading shards:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:58<00:02,  2.24s/it]
[36m(PolicyRayActorBase pid=5270, ip=10.4.40.184)[0m Downloading shards:  29%|â–ˆâ–ˆâ–‰       | 5/17 [01:17<03:00, 15.06s/it][32m [repeated 16x across cluster][0m
[36m(CriticRayActorBase pid=13504)[0m Downloading shards:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [01:03<00:02,  2.37s/it][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=12145)[0m Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]
[36m(CriticRayActorBase pid=13205)[0m Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:09<00:00,  3.49s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:09<00:00,  4.07s/it]
[36m(CriticRayActorBase pid=13501)[0m Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]
[36m(LLMActor pid=12145)[0m Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:00<00:03,  4.45it/s]
[36m(CriticRayActorBase pid=13502)[0m Loading checkpoint shards:   6%|â–Œ         | 1/17 [00:00<00:04,  3.22it/s]
[36m(LLMActor pid=12145)[0m Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:00<00:03,  3.87it/s]
[36m(LLMActor pid=12145)[0m Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:00<00:03,  3.72it/s]
[36m(RefRayActorBase pid=6207, ip=10.4.38.122)[0m Downloading shards:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [01:20<02:30, 13.67s/it][32m [repeated 27x across cluster][0m
[36m(LLMActor pid=12145)[0m Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:01<00:03,  3.69it/s]
[36m(LLMActor pid=12145)[0m Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:01<00:03,  3.76it/s]
[36m(LLMActor pid=12145)[0m Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:01<00:03,  3.58it/s]
[36m(LLMActor pid=12145)[0m Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:01<00:02,  3.55it/s]
[36m(LLMActor pid=12145)[0m Loading safetensors checkpoint shards:  47% Completed | 8/17 [00:02<00:02,  3.69it/s]
[36m(LLMActor pid=12145)[0m Loading safetensors checkpoint shards:  53% Completed | 9/17 [00:02<00:02,  3.78it/s]
[36m(LLMActor pid=12145)[0m Loading safetensors checkpoint shards:  59% Completed | 10/17 [00:02<00:01,  3.78it/s]
[36m(LLMActor pid=12145)[0m Loading safetensors checkpoint shards:  65% Completed | 11/17 [00:02<00:01,  3.78it/s]
[36m(LLMActor pid=12145)[0m Loading safetensors checkpoint shards:  71% Completed | 12/17 [00:03<00:01,  3.71it/s]
[36m(LLMActor pid=12145)[0m Loading safetensors checkpoint shards:  76% Completed | 13/17 [00:03<00:01,  3.63it/s]
[36m(LLMActor pid=12145)[0m Loading safetensors checkpoint shards:  82% Completed | 14/17 [00:03<00:00,  3.62it/s]
[36m(CriticRayActorBase pid=13205)[0m Loading checkpoint shards:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:03<00:00,  4.73it/s]
[36m(CriticRayActorBase pid=13205)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:03<00:00,  4.86it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:03<00:00,  4.48it/s]
[36m(LLMActor pid=12145)[0m Loading safetensors checkpoint shards:  88% Completed | 15/17 [00:04<00:00,  3.65it/s]
[36m(CriticRayActorBase pid=13205)[0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[36m(CriticRayActorBase pid=13205)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(LLMActor pid=12145)[0m Loading safetensors checkpoint shards:  94% Completed | 16/17 [00:04<00:00,  3.70it/s]
[36m(LLMActor pid=12145)[0m Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:04<00:00,  3.68it/s]
[36m(LLMActor pid=12145)[0m Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:04<00:00,  3.70it/s]
[36m(LLMActor pid=12145)[0m 
[36m(CriticRayActorBase pid=13504)[0m Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:13<00:00,  3.60s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:13<00:00,  4.35s/it][32m [repeated 3x across cluster][0m
[36m(CriticRayActorBase pid=13205)[0m Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s][32m [repeated 3x across cluster][0m
[36m(CriticRayActorBase pid=13504)[0m Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:03<00:00,  4.60it/s][32m [repeated 59x across cluster][0m
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m Downloading shards:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [01:32<02:15, 13.56s/it]
[36m(CriticRayActorBase pid=13504)[0m Loading checkpoint shards:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:04<00:00,  4.62it/s][32m [repeated 3x across cluster][0m
[36m(CriticRayActorBase pid=13504)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:04<00:00,  4.68it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:04<00:00,  4.00it/s][32m [repeated 3x across cluster][0m
[36m(CriticRayActorBase pid=13504)[0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.[32m [repeated 3x across cluster][0m
[36m(CriticRayActorBase pid=13504)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 3x across cluster][0m
[36m(PolicyRayActorBase pid=5265, ip=10.4.38.122)[0m Downloading shards:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [01:36<02:17, 13.71s/it]
[36m(CriticRayActorBase pid=5838, ip=10.4.34.192)[0m Downloading shards:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [01:23<00:55,  7.87s/it][32m [repeated 27x across cluster][0m
[36m(CriticRayActorBase pid=5836, ip=10.4.34.192)[0m Downloading shards:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [01:25<00:02,  2.23s/it]
[36m(CriticRayActorBase pid=5838, ip=10.4.34.192)[0m Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:26<00:00,  2.06s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:26<00:00,  5.10s/it]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]
[36m(CriticRayActorBase pid=5838, ip=10.4.34.192)[0m Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:00<00:03,  4.73it/s]
[36m(CriticRayActorBase pid=5838, ip=10.4.34.192)[0m Loading checkpoint shards:   6%|â–Œ         | 1/17 [00:00<00:05,  3.10it/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:00<00:03,  4.16it/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:00<00:03,  4.03it/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:00<00:03,  4.02it/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:01<00:02,  4.01it/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:01<00:02,  3.81it/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:01<00:02,  3.69it/s]
[36m(CriticRayActorBase pid=5837, ip=10.4.34.192)[0m Downloading shards:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [01:25<00:09,  3.14s/it][32m [repeated 16x across cluster][0m
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Loading safetensors checkpoint shards:  47% Completed | 8/17 [00:02<00:02,  3.62it/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Loading safetensors checkpoint shards:  53% Completed | 9/17 [00:02<00:02,  3.44it/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Loading safetensors checkpoint shards:  59% Completed | 10/17 [00:02<00:02,  3.39it/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Loading safetensors checkpoint shards:  65% Completed | 11/17 [00:03<00:01,  3.37it/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Loading safetensors checkpoint shards:  71% Completed | 12/17 [00:03<00:01,  3.29it/s]
[36m(CriticRayActorBase pid=5837, ip=10.4.34.192)[0m Loading checkpoint shards:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:03<00:00,  4.88it/s]
[36m(CriticRayActorBase pid=5839, ip=10.4.34.192)[0m Downloading shards:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [01:24<00:02,  2.23s/it][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Loading safetensors checkpoint shards:  76% Completed | 13/17 [00:03<00:01,  3.21it/s]
[36m(CriticRayActorBase pid=5837, ip=10.4.34.192)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:03<00:00,  4.88it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:03<00:00,  4.76it/s]
[36m(CriticRayActorBase pid=5837, ip=10.4.34.192)[0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[36m(CriticRayActorBase pid=5837, ip=10.4.34.192)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Loading safetensors checkpoint shards:  82% Completed | 14/17 [00:03<00:00,  3.17it/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Loading safetensors checkpoint shards:  88% Completed | 15/17 [00:04<00:00,  3.28it/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Loading safetensors checkpoint shards:  94% Completed | 16/17 [00:04<00:00,  3.31it/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:04<00:00,  3.69it/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:04<00:00,  3.56it/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m 
[36m(CriticRayActorBase pid=5839, ip=10.4.34.192)[0m Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:26<00:00,  2.06s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:26<00:00,  5.07s/it][32m [repeated 3x across cluster][0m
[36m(CriticRayActorBase pid=5839, ip=10.4.34.192)[0m Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s][32m [repeated 3x across cluster][0m
[36m(CriticRayActorBase pid=5839, ip=10.4.34.192)[0m Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:04<00:00,  3.41it/s][32m [repeated 59x across cluster][0m
[36m(PolicyRayActorBase pid=5270, ip=10.4.40.184)[0m Downloading shards:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [01:49<02:36, 15.61s/it][32m [repeated 17x across cluster][0m
[36m(CriticRayActorBase pid=5836, ip=10.4.34.192)[0m Loading checkpoint shards:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:04<00:00,  3.16it/s][32m [repeated 3x across cluster][0m
[36m(CriticRayActorBase pid=5836, ip=10.4.34.192)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:05<00:00,  3.19it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:05<00:00,  3.26it/s][32m [repeated 3x across cluster][0m
[36m(CriticRayActorBase pid=5836, ip=10.4.34.192)[0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.[32m [repeated 3x across cluster][0m
[36m(CriticRayActorBase pid=5836, ip=10.4.34.192)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 3x across cluster][0m
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m Downloading shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [01:59<01:49, 13.63s/it][32m [repeated 12x across cluster][0m
[36m(PolicyRayActorBase pid=5270, ip=10.4.40.184)[0m Downloading shards:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [02:05<02:20, 15.62s/it][32m [repeated 16x across cluster][0m
[36m(PolicyRayActorBase pid=5264, ip=10.4.38.122)[0m Downloading shards:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [02:17<01:35, 13.70s/it][32m [repeated 12x across cluster][0m
[36m(PolicyRayActorBase pid=5270, ip=10.4.40.184)[0m Downloading shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [02:21<02:05, 15.63s/it][32m [repeated 16x across cluster][0m
[36m(PolicyRayActorBase pid=5270, ip=10.4.40.184)[0m Downloading shards:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [02:37<01:50, 15.78s/it][32m [repeated 28x across cluster][0m
[36m(PolicyRayActorBase pid=5270, ip=10.4.40.184)[0m Downloading shards:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [02:52<01:34, 15.75s/it][32m [repeated 28x across cluster][0m
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m Downloading shards:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [03:08<00:41, 13.86s/it][32m [repeated 28x across cluster][0m
[36m(PolicyRayActorBase pid=5271, ip=10.4.40.184)[0m Downloading shards:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [03:14<01:28, 17.67s/it][32m [repeated 16x across cluster][0m
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m Downloading shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [03:21<00:27, 13.70s/it][32m [repeated 12x across cluster][0m
[36m(PolicyRayActorBase pid=5271, ip=10.4.40.184)[0m Downloading shards:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [03:30<01:08, 17.18s/it][32m [repeated 16x across cluster][0m
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m Downloading shards:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [03:35<00:13, 13.64s/it]
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [03:45<00:00, 12.75s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [03:45<00:00, 13.28s/it]
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(CriticRayActorBase pid=6045, ip=10.4.40.184)[0m Downloading shards:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [03:17<01:08, 17.13s/it][32m [repeated 11x across cluster][0m
[36m(RefRayActorBase pid=6211, ip=10.4.38.122)[0m Downloading shards:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [03:37<00:13, 13.66s/it][32m [repeated 15x across cluster][0m
[36m(RefRayActorBase pid=6213, ip=10.4.38.122)[0m Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [03:48<00:00, 12.78s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [03:48<00:00, 13.42s/it][32m [repeated 15x across cluster][0m
[36m(RefRayActorBase pid=6213, ip=10.4.38.122)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 15x across cluster][0m
[36m(PolicyRayActorBase pid=5270, ip=10.4.40.184)[0m Downloading shards:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [03:51<00:54, 18.25s/it]
[36m(PolicyRayActorBase pid=5271, ip=10.4.40.184)[0m Downloading shards:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [03:51<00:54, 18.26s/it]
[36m(PolicyRayActorBase pid=5270, ip=10.4.40.184)[0m Downloading shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [04:06<00:34, 17.22s/it][32m [repeated 11x across cluster][0m
[36m(PolicyRayActorBase pid=5271, ip=10.4.40.184)[0m Downloading shards:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [04:21<00:16, 16.53s/it]
[36m(CriticRayActorBase pid=6046, ip=10.4.40.184)[0m Downloading shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [03:53<00:34, 17.19s/it][32m [repeated 11x across cluster][0m
[36m(RefRayActorBase pid=5582, ip=10.4.40.184)[0m Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [04:33<00:00, 15.11s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [04:33<00:00, 16.08s/it]
[36m(RefRayActorBase pid=5563, ip=10.4.40.184)[0m Downloading shards:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [04:21<00:16, 16.54s/it][32m [repeated 11x across cluster][0m
[36m(PolicyRayActorBase pid=5270, ip=10.4.40.184)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(CriticRayActorBase pid=6043, ip=10.4.40.184)[0m Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]
[36m(CriticRayActorBase pid=6043, ip=10.4.40.184)[0m Loading checkpoint shards:   6%|â–Œ         | 1/17 [00:00<00:04,  3.45it/s]
[36m(CriticRayActorBase pid=6043, ip=10.4.40.184)[0m Loading checkpoint shards:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:03<00:00,  5.06it/s]
[36m(CriticRayActorBase pid=6043, ip=10.4.40.184)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:03<00:00,  5.10it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:03<00:00,  4.93it/s]
[36m(CriticRayActorBase pid=6043, ip=10.4.40.184)[0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[36m(CriticRayActorBase pid=6045, ip=10.4.40.184)[0m Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [04:20<00:00, 15.11s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [04:20<00:00, 15.31s/it][32m [repeated 11x across cluster][0m
[36m(CriticRayActorBase pid=6044, ip=10.4.40.184)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 11x across cluster][0m
[36m(CriticRayActorBase pid=13205)[0m Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s][32m [repeated 4x across cluster][0m
[36m(CriticRayActorBase pid=6044, ip=10.4.40.184)[0m Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:03<00:00,  4.46it/s][32m [repeated 59x across cluster][0m
[36m(CriticRayActorBase pid=6044, ip=10.4.40.184)[0m Loading checkpoint shards:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:03<00:00,  4.47it/s][32m [repeated 3x across cluster][0m
[36m(CriticRayActorBase pid=6044, ip=10.4.40.184)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:04<00:00,  4.50it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:04<00:00,  4.25it/s][32m [repeated 3x across cluster][0m
[36m(CriticRayActorBase pid=6044, ip=10.4.40.184)[0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.[32m [repeated 3x across cluster][0m
[36m(CriticRayActorBase pid=5839, ip=10.4.34.192)[0m Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s][32m [repeated 11x across cluster][0m
[36m(CriticRayActorBase pid=13504)[0m Loading checkpoint shards:  24%|â–ˆâ–ˆâ–Ž       | 4/17 [00:04<00:16,  1.25s/it][32m [repeated 48x across cluster][0m
[36m(CriticRayActorBase pid=5838, ip=10.4.34.192)[0m Loading checkpoint shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [00:10<00:09,  1.20s/it][32m [repeated 49x across cluster][0m
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m Loading checkpoint shards:  12%|â–ˆâ–        | 2/17 [00:04<00:35,  2.39s/it][32m [repeated 84x across cluster][0m
[36m(RefRayActorBase pid=5579, ip=10.4.40.184)[0m Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s][32m [repeated 23x across cluster][0m
[36m(CriticRayActorBase pid=13205)[0m Loading checkpoint shards:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:18<00:01,  1.14s/it]
[36m(CriticRayActorBase pid=5838, ip=10.4.34.192)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:19<00:00,  1.00s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:19<00:00,  1.14s/it]
[36m(CriticRayActorBase pid=5838, ip=10.4.34.192)[0m Some weights of CriticModel were not initialized from the model checkpoint at Qwen/Qwen2.5-32B and are newly initialized: ['value_head.weight']
[36m(CriticRayActorBase pid=5838, ip=10.4.34.192)[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[36m(CriticRayActorBase pid=6043, ip=10.4.40.184)[0m 2025-04-08 05:45:40.375 | INFO     | orz.ppo.models:get_llm_for_sequence_regression:572 - initialize value_head for ZeRO-3 reward model training.
[36m(CriticRayActorBase pid=13205)[0m Using /home/aiscuser/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
[36m(CriticRayActorBase pid=13205)[0m Creating extension directory /home/aiscuser/.cache/torch_extensions/py310_cu124/fused_adam...
[36m(CriticRayActorBase pid=6045, ip=10.4.40.184)[0m Detected CUDA files, patching ldflags
[36m(CriticRayActorBase pid=6045, ip=10.4.40.184)[0m Emitting ninja build file /home/aiscuser/.cache/torch_extensions/py310_cu124/fused_adam/build.ninja...
[36m(CriticRayActorBase pid=6045, ip=10.4.40.184)[0m /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36m(CriticRayActorBase pid=6045, ip=10.4.40.184)[0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36m(CriticRayActorBase pid=6045, ip=10.4.40.184)[0m   warnings.warn(
[36m(CriticRayActorBase pid=6045, ip=10.4.40.184)[0m Building extension module fused_adam...
[36m(CriticRayActorBase pid=6045, ip=10.4.40.184)[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[36m(PolicyRayActorBase pid=5267, ip=10.4.38.122)[0m Loading checkpoint shards:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:11<00:26,  2.25s/it][32m [repeated 96x across cluster][0m
[36m(CriticRayActorBase pid=13504)[0m Loading checkpoint shards:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:18<00:01,  1.15s/it][32m [repeated 11x across cluster][0m
[36m(CriticRayActorBase pid=13504)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:19<00:00,  1.02s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:19<00:00,  1.16s/it][32m [repeated 11x across cluster][0m
[36m(CriticRayActorBase pid=13504)[0m Some weights of CriticModel were not initialized from the model checkpoint at Qwen/Qwen2.5-32B and are newly initialized: ['value_head.weight'][32m [repeated 11x across cluster][0m
[36m(CriticRayActorBase pid=13504)[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[32m [repeated 11x across cluster][0m
[36m(CriticRayActorBase pid=13504)[0m 2025-04-08 05:45:40.625 | INFO     | orz.ppo.models:get_llm_for_sequence_regression:572 - initialize value_head for ZeRO-3 reward model training.[32m [repeated 11x across cluster][0m
[36m(CriticRayActorBase pid=6046, ip=10.4.40.184)[0m Using /home/aiscuser/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...[32m [repeated 11x across cluster][0m
[36m(CriticRayActorBase pid=6046, ip=10.4.40.184)[0m Creating extension directory /home/aiscuser/.cache/torch_extensions/py310_cu124/fused_adam...[32m [repeated 9x across cluster][0m
[36m(CriticRayActorBase pid=13501)[0m Detected CUDA files, patching ldflags[32m [repeated 2x across cluster][0m
[36m(CriticRayActorBase pid=13501)[0m Emitting ninja build file /home/aiscuser/.cache/torch_extensions/py310_cu124/fused_adam/build.ninja...[32m [repeated 2x across cluster][0m
[36m(CriticRayActorBase pid=13501)[0m /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. [32m [repeated 2x across cluster][0m
[36m(CriticRayActorBase pid=13501)[0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].[32m [repeated 2x across cluster][0m
[36m(CriticRayActorBase pid=13501)[0m   warnings.warn([32m [repeated 2x across cluster][0m
[36m(CriticRayActorBase pid=13501)[0m Building extension module fused_adam...[32m [repeated 2x across cluster][0m
[36m(CriticRayActorBase pid=13501)[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)[32m [repeated 2x across cluster][0m
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m Loading checkpoint shards:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:16<00:24,  2.41s/it][32m [repeated 48x across cluster][0m
[36m(PolicyRayActorBase pid=5269, ip=10.4.40.184)[0m Loading checkpoint shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 9/17 [00:21<00:19,  2.50s/it][32m [repeated 48x across cluster][0m
[36m(PolicyRayActorBase pid=5264, ip=10.4.38.122)[0m Loading checkpoint shards:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:26<00:14,  2.48s/it][32m [repeated 64x across cluster][0m
[36m(PolicyRayActorBase pid=5271, ip=10.4.40.184)[0m Loading checkpoint shards:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:31<00:09,  2.46s/it][32m [repeated 52x across cluster][0m
[36m(PolicyRayActorBase pid=5265, ip=10.4.38.122)[0m Loading checkpoint shards:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:38<00:02,  2.29s/it]
[36m(RefRayActorBase pid=5579, ip=10.4.40.184)[0m Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:36<00:04,  2.43s/it][32m [repeated 51x across cluster][0m
[36m(PolicyRayActorBase pid=5270, ip=10.4.40.184)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:39<00:00,  1.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:39<00:00,  2.33s/it]
[36m(PolicyRayActorBase pid=5270, ip=10.4.40.184)[0m Using /home/aiscuser/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
[36m(PolicyRayActorBase pid=5261, ip=10.4.38.122)[0m Creating extension directory /home/aiscuser/.cache/torch_extensions/py310_cu124/fused_adam...
[36m(PolicyRayActorBase pid=5261, ip=10.4.38.122)[0m Detected CUDA files, patching ldflags
[36m(PolicyRayActorBase pid=5261, ip=10.4.38.122)[0m Emitting ninja build file /home/aiscuser/.cache/torch_extensions/py310_cu124/fused_adam/build.ninja...
[36m(PolicyRayActorBase pid=5261, ip=10.4.38.122)[0m /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36m(PolicyRayActorBase pid=5261, ip=10.4.38.122)[0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36m(PolicyRayActorBase pid=5261, ip=10.4.38.122)[0m   warnings.warn(
[36m(PolicyRayActorBase pid=5261, ip=10.4.38.122)[0m Building extension module fused_adam...
[36m(PolicyRayActorBase pid=5261, ip=10.4.38.122)[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[36m(CriticRayActorBase pid=13205)[0m Loading extension module fused_adam...
[36m(PolicyRayActorBase pid=5266, ip=10.4.38.122)[0m Loading checkpoint shards:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:38<00:02,  2.33s/it][32m [repeated 23x across cluster][0m
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:39<00:00,  2.02s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:39<00:00,  2.34s/it][32m [repeated 23x across cluster][0m
[36m(PolicyRayActorBase pid=5266, ip=10.4.38.122)[0m Using /home/aiscuser/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...[32m [repeated 11x across cluster][0m
[36m(CriticRayActorBase pid=6045, ip=10.4.40.184)[0m Loading extension module fused_adam...[32m [repeated 8x across cluster][0m
[36m(PolicyRayActorBase pid=5265, ip=10.4.38.122)[0m Loading extension module fused_adam...[32m [repeated 8x across cluster][0m
2025-04-08 05:47:54.131 | INFO     | orz.ppo.trainer:build_models:1949 - init policy/ref/critic/reward models done
2025-04-08 05:47:55.215 | INFO     | orz.ppo.trainer:train:79 - Create vllm engine groups done.
2025-04-08 05:50:41.590 | INFO     | orz.ppo.trainer:train:81 - Sync actor weights to vllm engines, time cost: 166.37s
2025-04-08 05:50:41.590 | DEBUG    | orz.ppo.trainer:train:89 - train_dataset size: 56878
2025-04-08 05:50:41.590 | DEBUG    | orz.ppo.trainer:train:90 - train_dataset sample: ('A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e. your response format should be: <think> reasoning process here </think>\n\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\nThis is the problem:\n$P(x)$ is a polynomial of degree $3n$ such that\n\\begin{eqnarray*} P(0) = P(3) = \\cdots &=& P(3n) = 2, \\\\ P(1) = P(4) = \\cdots &=& P(3n-2) = 1, \\\\ P(2) = P(5) = \\cdots &=& P(3n-1) = 0, \\quad\\text{ and }\\\\ && P(3n+1) = 730.\\end{eqnarray*}\nDetermine $n$.\nAssistant: <think>', {'prompt': 'A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e. your response format should be: <think> reasoning process here </think>\n\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\nThis is the problem:\n$P(x)$ is a polynomial of degree $3n$ such that\n\\begin{eqnarray*} P(0) = P(3) = \\cdots &=& P(3n) = 2, \\\\ P(1) = P(4) = \\cdots &=& P(3n-2) = 1, \\\\ P(2) = P(5) = \\cdots &=& P(3n-1) = 0, \\quad\\text{ and }\\\\ && P(3n+1) = 730.\\end{eqnarray*}\nDetermine $n$.\nAssistant: <think>', 'answer': 'n = 4', 'target': 'n = 4'})
[36m(LLMActor pid=12145)[0m INFO 04-08 05:39:17 config.py:478] This model supports multiple tasks: {'reward', 'embed', 'generate', 'score', 'classify'}. Defaulting to 'generate'.
[36m(LLMActor pid=12145)[0m WARNING 04-08 05:39:17 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(LLMActor pid=12145)[0m WARNING 04-08 05:39:17 config.py:604] Async output processing is not supported on the current platform type cuda.
[36m(LLMActor pid=12145)[0m INFO 04-08 05:39:17 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='Qwen/Qwen2.5-32B', speculative_config=None, tokenizer='Qwen/Qwen2.5-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=Qwen/Qwen2.5-32B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[],"max_capture_size":0}, use_cached_outputs=False, 
[36m(LLMActor pid=12145)[0m INFO 04-08 05:39:18 ray_gpu_executor.py:122] use_ray_spmd_worker: False
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m INFO 04-08 05:39:32 config.py:478] This model supports multiple tasks: {'score', 'generate', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m WARNING 04-08 05:39:32 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m WARNING 04-08 05:39:32 config.py:604] Async output processing is not supported on the current platform type cuda.
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m INFO 04-08 05:39:32 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='Qwen/Qwen2.5-32B', speculative_config=None, tokenizer='Qwen/Qwen2.5-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=43, served_model_name=Qwen/Qwen2.5-32B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[],"max_capture_size":0}, use_cached_outputs=False, 
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m INFO 04-08 05:39:35 ray_gpu_executor.py:122] use_ray_spmd_worker: False
[36m(pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:39:48,964] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(LLMActor pid=12145)[0m INFO 04-08 05:39:55 selector.py:120] Using Flash Attention backend.
[36m(LLMActor pid=12145)[0m INFO 04-08 05:39:57 utils.py:922] Found nccl from library libnccl.so.2
[36m(LLMActor pid=12145)[0m INFO 04-08 05:39:57 pynccl.py:69] vLLM is using nccl==2.21.5
[36m(LLMActor pid=12145)[0m NCCL version 2.21.5+cuda12.4
[36m(LLMActor pid=12145)[0m INFO 04-08 05:39:58 custom_all_reduce_utils.py:204] generating GPU P2P access cache in /home/aiscuser/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[36m(pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:40:08,769] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(RayWorkerWrapper pid=12571)[0m INFO 04-08 05:39:55 selector.py:120] Using Flash Attention backend.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(RayWorkerWrapper pid=12807)[0m INFO 04-08 05:39:57 utils.py:922] Found nccl from library libnccl.so.2[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=12807)[0m INFO 04-08 05:39:57 pynccl.py:69] vLLM is using nccl==2.21.5[32m [repeated 3x across cluster][0m
[36m(pid=5267, ip=10.4.40.184)[0m [2025-04-08 05:40:13,705] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)[32m [repeated 11x across cluster][0m
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m INFO 04-08 05:40:17 selector.py:120] Using Flash Attention backend.
[36m(RayWorkerWrapper pid=5378, ip=10.4.34.192)[0m INFO 04-08 05:40:17 selector.py:120] Using Flash Attention backend.
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m INFO 04-08 05:40:20 utils.py:922] Found nccl from library libnccl.so.2
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m INFO 04-08 05:40:20 pynccl.py:69] vLLM is using nccl==2.21.5
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m NCCL version 2.21.5+cuda12.4
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m INFO 04-08 05:40:21 custom_all_reduce_utils.py:204] generating GPU P2P access cache in /home/aiscuser/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[36m(LLMActor pid=12145)[0m INFO 04-08 05:40:22 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/aiscuser/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[36m(LLMActor pid=12145)[0m INFO 04-08 05:40:22 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_5f5aaec6'), local_subscribe_port=55897, remote_subscribe_port=None)
[36m(LLMActor pid=12145)[0m INFO 04-08 05:40:22 model_runner.py:1092] Starting to load model Qwen/Qwen2.5-32B...
[36m(LLMActor pid=12145)[0m INFO 04-08 05:40:23 weight_utils.py:243] Using model weights format ['*.safetensors']
[36m(pid=13205)[0m [2025-04-08 05:40:21,986] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(RayWorkerWrapper pid=5592, ip=10.4.34.192)[0m INFO 04-08 05:40:17 selector.py:120] Using Flash Attention backend.[32m [repeated 2x across cluster][0m
[36m(pid=6209, ip=10.4.38.122)[0m [2025-04-08 05:40:23,321] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:40:30,138] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:40:30,138] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[36m(RayWorkerWrapper pid=5592, ip=10.4.34.192)[0m INFO 04-08 05:40:20 utils.py:922] Found nccl from library libnccl.so.2[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=5592, ip=10.4.34.192)[0m INFO 04-08 05:40:20 pynccl.py:69] vLLM is using nccl==2.21.5[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=12807)[0m INFO 04-08 05:40:22 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/aiscuser/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=12807)[0m INFO 04-08 05:40:22 model_runner.py:1092] Starting to load model Qwen/Qwen2.5-32B...[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=12807)[0m INFO 04-08 05:40:23 weight_utils.py:243] Using model weights format ['*.safetensors'][32m [repeated 3x across cluster][0m
[36m(pid=5582, ip=10.4.40.184)[0m [2025-04-08 05:40:24,538] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)[32m [repeated 10x across cluster][0m
[36m(RefRayActorBase pid=6212, ip=10.4.38.122)[0m [2025-04-08 05:40:32,654] [INFO] [comm.py:652:init_distributed] cdb=None[32m [repeated 16x across cluster][0m
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:40:30,172] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl[32m [repeated 2x across cluster][0m
[36m(pid=5836, ip=10.4.34.192)[0m [2025-04-08 05:40:35,679] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)[32m [repeated 4x across cluster][0m
[36m(CriticRayActorBase pid=13502)[0m [2025-04-08 05:40:42,431] [INFO] [comm.py:652:init_distributed] cdb=None[32m [repeated 9x across cluster][0m
[36m(pid=6044, ip=10.4.40.184)[0m [2025-04-08 05:40:37,533] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)[32m [repeated 7x across cluster][0m
[36m(CriticRayActorBase pid=6044, ip=10.4.40.184)[0m [2025-04-08 05:40:48,888] [INFO] [comm.py:652:init_distributed] cdb=None[32m [repeated 7x across cluster][0m
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m INFO 04-08 05:40:49 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/aiscuser/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m INFO 04-08 05:40:49 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_0c89c6df'), local_subscribe_port=50905, remote_subscribe_port=None)
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m INFO 04-08 05:40:49 model_runner.py:1092] Starting to load model Qwen/Qwen2.5-32B...
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m INFO 04-08 05:40:49 weight_utils.py:243] Using model weights format ['*.safetensors']
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:42:02,844] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 12
[36m(CriticRayActorBase pid=6046, ip=10.4.40.184)[0m [2025-04-08 05:40:48,889] [INFO] [comm.py:652:init_distributed] cdb=None[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=5592, ip=10.4.34.192)[0m INFO 04-08 05:40:49 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/aiscuser/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=5592, ip=10.4.34.192)[0m INFO 04-08 05:40:49 model_runner.py:1092] Starting to load model Qwen/Qwen2.5-32B...[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=5592, ip=10.4.34.192)[0m INFO 04-08 05:40:50 weight_utils.py:243] Using model weights format ['*.safetensors'][32m [repeated 3x across cluster][0m
[36m(CriticRayActorBase pid=13205)[0m NCCL version 2.21.5+cuda12.4
[36m(LLMActor pid=12145)[0m INFO 04-08 05:42:03 model_runner.py:1097] Loading model weights took 15.4136 GB
[36m(RayWorkerWrapper pid=12688)[0m INFO 04-08 05:42:14 worker.py:241] Memory profiling takes 9.48 seconds
[36m(RayWorkerWrapper pid=12688)[0m INFO 04-08 05:42:14 worker.py:241] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.50) = 39.58GiB
[36m(RayWorkerWrapper pid=12688)[0m INFO 04-08 05:42:14 worker.py:241] model weights take 15.41GiB; non_torch_memory takes 2.21GiB; PyTorch activation peak memory takes 0.72GiB; the rest of the memory reserved for KV Cache is 21.24GiB.
[36m(CriticRayActorBase pid=13504)[0m [2025-04-08 05:42:03,276] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 12[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=12571)[0m INFO 04-08 05:42:05 model_runner.py:1097] Loading model weights took 15.4136 GB[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=12145)[0m INFO 04-08 05:42:15 distributed_gpu_executor.py:57] # GPU blocks: 1310, # CPU blocks: 256
[36m(LLMActor pid=12145)[0m INFO 04-08 05:42:15 distributed_gpu_executor.py:61] Maximum concurrency for 8192 tokens per request: 40.94x
[36m(LLMActor pid=12145)[0m INFO 04-08 05:42:17 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 12.00 seconds
[36m(LLMActor pid=12145)[0m INFO 04-08 05:42:14 worker.py:241] Memory profiling takes 9.61 seconds[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=12145)[0m INFO 04-08 05:42:14 worker.py:241] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.50) = 39.58GiB[32m [repeated 3x across cluster][0m
[36m(LLMActor pid=12145)[0m INFO 04-08 05:42:14 worker.py:241] model weights take 15.41GiB; non_torch_memory takes 2.30GiB; PyTorch activation peak memory takes 1.38GiB; the rest of the memory reserved for KV Cache is 20.48GiB.[32m [repeated 3x across cluster][0m
[36m(CriticRayActorBase pid=5837, ip=10.4.34.192)[0m [2025-04-08 05:42:20,844] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 12
[36m(CriticRayActorBase pid=5838, ip=10.4.34.192)[0m [2025-04-08 05:42:21,078] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 12
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m INFO 04-08 05:42:22 model_runner.py:1097] Loading model weights took 15.4136 GB
[36m(RayWorkerWrapper pid=5378, ip=10.4.34.192)[0m INFO 04-08 05:42:33 worker.py:241] Memory profiling takes 10.66 seconds
[36m(RayWorkerWrapper pid=5378, ip=10.4.34.192)[0m INFO 04-08 05:42:33 worker.py:241] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.50) = 39.58GiB
[36m(RayWorkerWrapper pid=5378, ip=10.4.34.192)[0m INFO 04-08 05:42:33 worker.py:241] model weights take 15.41GiB; non_torch_memory takes 2.21GiB; PyTorch activation peak memory takes 0.72GiB; the rest of the memory reserved for KV Cache is 21.24GiB.
[36m(CriticRayActorBase pid=5836, ip=10.4.34.192)[0m [2025-04-08 05:42:22,523] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 12[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=5592, ip=10.4.34.192)[0m INFO 04-08 05:42:22 model_runner.py:1097] Loading model weights took 15.4136 GB[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=5592, ip=10.4.34.192)[0m INFO 04-08 05:42:33 worker.py:241] Memory profiling takes 10.66 seconds
[36m(RayWorkerWrapper pid=5592, ip=10.4.34.192)[0m INFO 04-08 05:42:33 worker.py:241] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.50) = 39.58GiB
[36m(RayWorkerWrapper pid=5592, ip=10.4.34.192)[0m INFO 04-08 05:42:33 worker.py:241] model weights take 15.41GiB; non_torch_memory takes 1.93GiB; PyTorch activation peak memory takes 0.72GiB; the rest of the memory reserved for KV Cache is 21.52GiB.
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m INFO 04-08 05:42:34 distributed_gpu_executor.py:57] # GPU blocks: 1310, # CPU blocks: 256
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m INFO 04-08 05:42:34 distributed_gpu_executor.py:61] Maximum concurrency for 8192 tokens per request: 40.94x
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m INFO 04-08 05:42:36 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 13.55 seconds
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:44:21,968] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 12
[36m(RayWorkerWrapper pid=5483, ip=10.4.34.192)[0m INFO 04-08 05:42:33 worker.py:241] Memory profiling takes 10.92 seconds[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=5483, ip=10.4.34.192)[0m INFO 04-08 05:42:33 worker.py:241] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.50) = 39.58GiB[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=5483, ip=10.4.34.192)[0m INFO 04-08 05:42:33 worker.py:241] model weights take 15.41GiB; non_torch_memory takes 2.21GiB; PyTorch activation peak memory takes 0.72GiB; the rest of the memory reserved for KV Cache is 21.24GiB.[32m [repeated 2x across cluster][0m
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:44:22,014] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 12
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m NCCL version 2.21.5+cuda12.4
[36m(RefRayActorBase pid=5568, ip=10.4.40.184)[0m [2025-04-08 05:45:10,347] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 12[32m [repeated 15x across cluster][0m
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m NCCL version 2.21.5+cuda12.4
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:45:20,698] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 771, num_elems = 31.99B
[36m(CriticRayActorBase pid=6044, ip=10.4.40.184)[0m [2025-04-08 05:45:14,798] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 12[32m [repeated 11x across cluster][0m
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:45:32,442] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 771, num_elems = 32.76B
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:45:32,446] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 771, num_elems = 32.76B
[36m(RefRayActorBase pid=5568, ip=10.4.40.184)[0m [2025-04-08 05:46:12,452] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 12
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:12,612] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:12,612] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:12,629] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:12,632] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(CriticRayActorBase pid=13501)[0m [1/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/adam -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/TH -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/envs/ptca/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:12,944] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:12,945] [INFO] [utils.py:782:see_memory_usage] MA 5.13 GB         Max_MA 8.03 GB         CA 8.38 GB         Max_CA 8 GB 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:12,945] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 59.78 GB, percent = 3.4%
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m Parameter Offload: Total persistent parameters: 1119232 in 321 params
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,295] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,296] [INFO] [utils.py:782:see_memory_usage] MA 5.13 GB         Max_MA 5.13 GB         CA 8.38 GB         Max_CA 8 GB 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,296] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 59.81 GB, percent = 3.4%
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,299] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,299] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "profile": false
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m }
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,299] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,299] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,299] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,299] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "enabled": false, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "start_step": null, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "end_step": null, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "model_info": null, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "fast": true, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m }
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,299] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbbfb2756c0>
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "enabled": false, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "detailed": true, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "output_file": null
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m }
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,300] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "enabled": false, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "load_path": null
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m }
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   train_batch_size ............. 12
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   world_size ................... 12
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,301] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,302] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,302] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,302] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,302] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m [2025-04-08 05:46:13,302] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m         "stage": 3, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m         "offload_param": {
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m             "device": "none", 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m             "pin_memory": true
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m         }
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     }, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "bf16": {
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m         "enabled": true
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     }, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=5260, ip=10.4.38.122)[0m }
[36m(CriticRayActorBase pid=13501)[0m [2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output multi_tensor_adam.cuda.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/adam -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/TH -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/envs/ptca/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -std=c++17 -c /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o 
[36m(CriticRayActorBase pid=13205)[0m Time to load fused_adam op: 36.486486196517944 seconds
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:17,151] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:17,151] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:17,211] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:17,211] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:17,211] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:17,211] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[36m(CriticRayActorBase pid=13501)[0m [3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:17,403] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:17,407] [INFO] [stage3.py:167:__init__] Reduce bucket size 500000000
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:17,407] [INFO] [stage3.py:168:__init__] Prefetch bucket size 50000000
[36m(CriticRayActorBase pid=13504)[0m [2025-04-08 05:46:17,147] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 12[32m [repeated 15x across cluster][0m
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:17,134] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:17,134] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:17,149] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:18,027] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[36m(CriticRayActorBase pid=5837, ip=10.4.34.192)[0m [1/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/adam -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/TH -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/envs/ptca/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o 
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:17,599] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:18,028] [INFO] [utils.py:782:see_memory_usage] MA 5.01 GB         Max_MA 5.01 GB         CA 8.38 GB         Max_CA 8 GB [32m [repeated 4x across cluster][0m
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:18,028] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 92.95 GB, percent = 5.2%[32m [repeated 4x across cluster][0m
[36m(CriticRayActorBase pid=13205)[0m Parameter Offload: Total persistent parameters: 1124352 in 322 params
[36m(CriticRayActorBase pid=6045, ip=10.4.40.184)[0m [1/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/adam -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/TH -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/envs/ptca/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o 
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:17,830] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[36m(CriticRayActorBase pid=6045, ip=10.4.40.184)[0m [2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output multi_tensor_adam.cuda.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/adam -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/TH -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/envs/ptca/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -std=c++17 -c /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o [32m [repeated 2x across cluster][0m
[36m(CriticRayActorBase pid=5838, ip=10.4.34.192)[0m Time to load fused_adam op: 40.7235586643219 seconds[32m [repeated 7x across cluster][0m
[36m(CriticRayActorBase pid=5837, ip=10.4.34.192)[0m [3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so
[36m(CriticRayActorBase pid=5838, ip=10.4.34.192)[0m [2025-04-08 05:46:21,376] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 12[32m [repeated 4x across cluster][0m
[36m(CriticRayActorBase pid=6045, ip=10.4.40.184)[0m [3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:30,351] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 4
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:30,352] [INFO] [utils.py:782:see_memory_usage] MA 4.96 GB         Max_MA 5.01 GB         CA 4.97 GB         Max_CA 8 GB 
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:30,352] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 92.87 GB, percent = 5.2%
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:30,547] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:30,547] [INFO] [utils.py:782:see_memory_usage] MA 4.96 GB         Max_MA 4.96 GB         CA 4.97 GB         Max_CA 5 GB 
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:30,548] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 92.87 GB, percent = 5.2%
[36m(CriticRayActorBase pid=6046, ip=10.4.40.184)[0m Time to load fused_adam op: 45.35390567779541 seconds[32m [repeated 8x across cluster][0m
[36m(CriticRayActorBase pid=6046, ip=10.4.40.184)[0m [2025-04-08 05:46:26,007] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 12[32m [repeated 8x across cluster][0m
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:30,777] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:30,778] [INFO] [utils.py:782:see_memory_usage] MA 14.9 GB         Max_MA 16.13 GB         CA 16.77 GB         Max_CA 17 GB 
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:30,778] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 92.87 GB, percent = 5.2%
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:30,972] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:30,973] [INFO] [utils.py:782:see_memory_usage] MA 14.9 GB         Max_MA 14.9 GB         CA 16.77 GB         Max_CA 17 GB 
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:30,973] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 92.87 GB, percent = 5.2%
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:31,173] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:31,173] [INFO] [utils.py:782:see_memory_usage] MA 14.9 GB         Max_MA 18.63 GB         CA 20.5 GB         Max_CA 20 GB 
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:31,174] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 92.86 GB, percent = 5.2%
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:46:31,695] [INFO] [stage3.py:525:_setup_for_real_optimizer] optimizer state initialized
[36m(PolicyRayActorBase pid=5261, ip=10.4.38.122)[0m [1/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/adam -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/TH -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/envs/ptca/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o 
[36m(PolicyRayActorBase pid=5261, ip=10.4.38.122)[0m [2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output multi_tensor_adam.cuda.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/adam -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/TH -isystem /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/envs/ptca/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -std=c++17 -c /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o 
[36m(PolicyRayActorBase pid=5261, ip=10.4.38.122)[0m [3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so
[36m(PolicyRayActorBase pid=5261, ip=10.4.38.122)[0m Time to load fused_adam op: 33.38496279716492 seconds
[36m(PolicyRayActorBase pid=5261, ip=10.4.38.122)[0m [2025-04-08 05:46:45,793] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 12
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:45,824] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:45,824] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:45,839] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:45,841] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:45,841] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:45,902] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:45,902] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:45,902] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:45,902] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:46,092] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:46,093] [INFO] [utils.py:782:see_memory_usage] MA 5.13 GB         Max_MA 8.03 GB         CA 8.38 GB         Max_CA 8 GB 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:46,093] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 59.64 GB, percent = 3.4%
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:46,096] [INFO] [stage3.py:167:__init__] Reduce bucket size 500000000
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:46,096] [INFO] [stage3.py:168:__init__] Prefetch bucket size 50000000
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:46,286] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:46,287] [INFO] [utils.py:782:see_memory_usage] MA 5.13 GB         Max_MA 5.13 GB         CA 8.38 GB         Max_CA 8 GB 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:46,287] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 59.64 GB, percent = 3.4%
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m Parameter Offload: Total persistent parameters: 1119232 in 321 params
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:46,515] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:46,516] [INFO] [utils.py:782:see_memory_usage] MA 5.13 GB         Max_MA 5.13 GB         CA 8.38 GB         Max_CA 8 GB 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:46,516] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 59.64 GB, percent = 3.4%
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:46,710] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:46,711] [INFO] [utils.py:782:see_memory_usage] MA 5.13 GB         Max_MA 5.13 GB         CA 8.38 GB         Max_CA 8 GB 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:46,711] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 59.64 GB, percent = 3.4%
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:50,124] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 4
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:50,125] [INFO] [utils.py:782:see_memory_usage] MA 5.09 GB         Max_MA 5.13 GB         CA 5.09 GB         Max_CA 8 GB 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:50,126] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 59.6 GB, percent = 3.4%
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:50,319] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:50,320] [INFO] [utils.py:782:see_memory_usage] MA 5.09 GB         Max_MA 5.09 GB         CA 5.09 GB         Max_CA 5 GB 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:50,320] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 59.6 GB, percent = 3.4%
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:50,550] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:50,551] [INFO] [utils.py:782:see_memory_usage] MA 15.26 GB         Max_MA 16.61 GB         CA 17.13 GB         Max_CA 17 GB 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:50,551] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 59.59 GB, percent = 3.4%
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:50,745] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:50,745] [INFO] [utils.py:782:see_memory_usage] MA 15.26 GB         Max_MA 15.26 GB         CA 17.13 GB         Max_CA 17 GB 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:50,745] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 59.59 GB, percent = 3.4%
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:50,945] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:50,946] [INFO] [utils.py:782:see_memory_usage] MA 15.26 GB         Max_MA 18.99 GB         CA 20.86 GB         Max_CA 21 GB 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:50,946] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 59.6 GB, percent = 3.4%
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:46:50,947] [INFO] [stage3.py:525:_setup_for_real_optimizer] optimizer state initialized
[36m(PolicyRayActorBase pid=5263, ip=10.4.38.122)[0m Time to load fused_adam op: 33.37654781341553 seconds[32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=5263, ip=10.4.38.122)[0m [2025-04-08 05:46:45,825] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 12[32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,127] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,128] [INFO] [utils.py:782:see_memory_usage] MA 26.36 GB         Max_MA 29.26 GB         CA 31.03 GB         Max_CA 31 GB 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,128] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 59.6 GB, percent = 3.4%
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,128] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,128] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,128] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f6ed82a2440>
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,128] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,130] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "partition_activations": false, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "contiguous_memory_optimization": false, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "cpu_checkpointing": false, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "number_checkpoints": null, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "synchronize_checkpoint_boundary": false, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "profile": false
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m }
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "enabled": false, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "start_step": null, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "end_step": null, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "metric_path": null, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "arg_mappings": null, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "metric": "throughput", 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "model_info": null, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "results_dir": "autotuning_results", 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "exps_dir": "autotuning_exps", 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "overwrite": true, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "fast": true, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "start_profile_step": 3, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "end_profile_step": 5, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "tuner_type": "gridsearch", 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "tuner_early_stopping": 5, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "tuner_num_trials": 50, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "model_info_path": null, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "mp_size": 1, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "max_train_batch_size": null, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "min_train_batch_size": 1, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "num_tuning_micro_batch_sizes": 3
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m }
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6ed80f1990>
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,131] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "enabled": false, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "recompute_fwd_factor": 0.0, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "profile_step": 1, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "module_depth": -1, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "top_modules": 1, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "detailed": true, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "output_file": null
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m }
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   grad_accum_dtype ............. fp32
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "enabled": false, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "persistent_storage_path": null, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "persistent_time_interval": 100, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "num_of_version_in_retention": 2, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "enable_nebula_load": true, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "load_path": null
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m }
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,132] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:1003:print]   train_batch_size ............. 12
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:1003:print]   world_size ................... 12
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m [2025-04-08 05:47:15,133] [INFO] [config.py:989:print_user_config]   json = {
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "steps_per_print": 100, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "zero_optimization": {
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m         "stage": 3, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m         "offload_param": {
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m             "device": "none"
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m         }, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m         "offload_optimizer": {
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m             "device": "none", 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m             "pin_memory": true
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m         }, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m         "sub_group_size": "auto", 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m         "stage3_max_live_parameters": "auto", 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m         "stage3_max_reuse_distance": "auto", 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m         "stage3_prefetch_bucket_size": "auto", 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m         "reduce_bucket_size": "auto", 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m         "zero_hpz_partition_size": 1, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m         "zero_quantized_weights": false, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m         "zero_quantized_gradients": false
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     }, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "bf16": {
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m         "enabled": true
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     }, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "gradient_clipping": 1.0, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "prescale_gradients": false, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "wall_clock_breakdown": false, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "data_types": {
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m         "grad_accum_dtype": "fp32"
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     }, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m     "gradient_accumulation_steps": 1
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m }
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,850] [INFO] [utils.py:782:see_memory_usage] MA 25.76 GB         Max_MA 28.66 GB         CA 30.43 GB         Max_CA 30 GB 
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,851] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 92.98 GB, percent = 5.2%
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,850] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,851] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,851] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,851] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f692fd34370>
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,851] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,853] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,853] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(CriticRayActorBase pid=13205)[0m     "partition_activations": false, 
[36m(CriticRayActorBase pid=13205)[0m     "contiguous_memory_optimization": false, 
[36m(CriticRayActorBase pid=13205)[0m     "cpu_checkpointing": false, 
[36m(CriticRayActorBase pid=13205)[0m     "number_checkpoints": null, 
[36m(CriticRayActorBase pid=13205)[0m     "synchronize_checkpoint_boundary": false, 
[36m(CriticRayActorBase pid=13205)[0m     "profile": false
[36m(CriticRayActorBase pid=13205)[0m }[32m [repeated 4x across cluster][0m
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,853] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(CriticRayActorBase pid=13205)[0m     "enabled": false, [32m [repeated 3x across cluster][0m
[36m(CriticRayActorBase pid=13205)[0m     "start_step": null, 
[36m(CriticRayActorBase pid=13205)[0m     "end_step": null, 
[36m(CriticRayActorBase pid=13205)[0m     "metric_path": null, 
[36m(CriticRayActorBase pid=13205)[0m     "arg_mappings": null, 
[36m(CriticRayActorBase pid=13205)[0m     "metric": "throughput", 
[36m(CriticRayActorBase pid=13205)[0m     "model_info": null, 
[36m(CriticRayActorBase pid=13205)[0m     "results_dir": "autotuning_results", 
[36m(CriticRayActorBase pid=13205)[0m     "exps_dir": "autotuning_exps", 
[36m(CriticRayActorBase pid=13205)[0m     "overwrite": true, 
[36m(CriticRayActorBase pid=13205)[0m     "fast": true, 
[36m(CriticRayActorBase pid=13205)[0m     "start_profile_step": 3, 
[36m(CriticRayActorBase pid=13205)[0m     "end_profile_step": 5, 
[36m(CriticRayActorBase pid=13205)[0m     "tuner_type": "gridsearch", 
[36m(CriticRayActorBase pid=13205)[0m     "tuner_early_stopping": 5, 
[36m(CriticRayActorBase pid=13205)[0m     "tuner_num_trials": 50, 
[36m(CriticRayActorBase pid=13205)[0m     "model_info_path": null, 
[36m(CriticRayActorBase pid=13205)[0m     "mp_size": 1, 
[36m(CriticRayActorBase pid=13205)[0m     "max_train_batch_size": null, 
[36m(CriticRayActorBase pid=13205)[0m     "min_train_batch_size": 1, 
[36m(CriticRayActorBase pid=13205)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(CriticRayActorBase pid=13205)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(CriticRayActorBase pid=13205)[0m     "num_tuning_micro_batch_sizes": 3
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False[32m [repeated 2x across cluster][0m
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f692e50eb30>
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,854] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(CriticRayActorBase pid=13205)[0m     "recompute_fwd_factor": 0.0, 
[36m(CriticRayActorBase pid=13205)[0m     "profile_step": 1, 
[36m(CriticRayActorBase pid=13205)[0m     "module_depth": -1, 
[36m(CriticRayActorBase pid=13205)[0m     "top_modules": 1, 
[36m(CriticRayActorBase pid=13205)[0m     "detailed": true, 
[36m(CriticRayActorBase pid=13205)[0m     "output_file": null
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   grad_accum_dtype ............. fp32
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(CriticRayActorBase pid=13205)[0m     "persistent_storage_path": null, 
[36m(CriticRayActorBase pid=13205)[0m     "persistent_time_interval": 100, 
[36m(CriticRayActorBase pid=13205)[0m     "num_of_version_in_retention": 2, 
[36m(CriticRayActorBase pid=13205)[0m     "enable_nebula_load": true, 
[36m(CriticRayActorBase pid=13205)[0m     "load_path": null
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,855] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,856] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,856] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,856] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,856] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,856] [INFO] [config.py:1003:print]   train_batch_size ............. 12
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,856] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,856] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,856] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,856] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,856] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,856] [INFO] [config.py:1003:print]   world_size ................... 12
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,856] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,856] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,856] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,856] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,856] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(CriticRayActorBase pid=13205)[0m [2025-04-08 05:47:53,856] [INFO] [config.py:989:print_user_config]   json = {
[36m(CriticRayActorBase pid=13205)[0m     "steps_per_print": 100, 
[36m(CriticRayActorBase pid=13205)[0m     "zero_optimization": {
[36m(CriticRayActorBase pid=13205)[0m         "stage": 3, 
[36m(CriticRayActorBase pid=13205)[0m         "offload_param": {
[36m(CriticRayActorBase pid=13205)[0m             "device": "none"
[36m(CriticRayActorBase pid=13205)[0m         }, 
[36m(CriticRayActorBase pid=13205)[0m         "offload_optimizer": {
[36m(CriticRayActorBase pid=13205)[0m             "device": "none", 
[36m(CriticRayActorBase pid=13205)[0m             "pin_memory": true
[36m(CriticRayActorBase pid=13205)[0m         }, 
[36m(CriticRayActorBase pid=13205)[0m         "sub_group_size": "auto", 
[36m(CriticRayActorBase pid=13205)[0m         "stage3_max_live_parameters": "auto", 
[36m(CriticRayActorBase pid=13205)[0m         "stage3_max_reuse_distance": "auto", 
[36m(CriticRayActorBase pid=13205)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(CriticRayActorBase pid=13205)[0m         "stage3_prefetch_bucket_size": "auto", 
[36m(CriticRayActorBase pid=13205)[0m         "reduce_bucket_size": "auto", 
[36m(CriticRayActorBase pid=13205)[0m         "zero_hpz_partition_size": 1, 
[36m(CriticRayActorBase pid=13205)[0m         "zero_quantized_weights": false, 
[36m(CriticRayActorBase pid=13205)[0m         "zero_quantized_gradients": false
[36m(CriticRayActorBase pid=13205)[0m     }, 
[36m(CriticRayActorBase pid=13205)[0m     "bf16": {
[36m(CriticRayActorBase pid=13205)[0m         "enabled": true
[36m(CriticRayActorBase pid=13205)[0m     }, 
[36m(CriticRayActorBase pid=13205)[0m     "gradient_clipping": 1.0, 
[36m(CriticRayActorBase pid=13205)[0m     "prescale_gradients": false, 
[36m(CriticRayActorBase pid=13205)[0m     "wall_clock_breakdown": false, 
[36m(CriticRayActorBase pid=13205)[0m     "data_types": {
[36m(CriticRayActorBase pid=13205)[0m         "grad_accum_dtype": "fp32"
[36m(CriticRayActorBase pid=13205)[0m     }, 
[36m(CriticRayActorBase pid=13205)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(CriticRayActorBase pid=13205)[0m     "gradient_accumulation_steps": 1
[36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m WARNING:using --vllm_sync_backend=gloo for vLLM version > 0.4.2 (or export NCCL_P2P_DISABLE=1)
[36m(LLMActor pid=12145)[0m init_process_group: master_address=10.4.38.122, master_port=32827,  rank=1, world_size=9, group_name=openrlhf
Episode [1/20]:   0%|          | 0/889 [00:00<?, ?it/s][36m(PolicyRayActorBase pid=5104, ip=10.4.38.122)[0m Broadcast actor weights to vllm engines done
[36m(CriticRayActorBase pid=13205)[0m }
[36m(RayWorkerWrapper pid=5592, ip=10.4.34.192)[0m init_process_group: master_address=10.4.38.122, master_port=32827,  rank=8, world_size=9, group_name=openrlhf[32m [repeated 7x across cluster][0m
2025-04-08 05:50:41.815 | INFO     | __main__:eval:2558 - Start evaluating on val set
[36m(LLMActor pid=12145)[0m Processed prompts:   0%|          | 0/364 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(PolicyRayActorBase pid=5266, ip=10.4.38.122)[0m Loading extension module fused_adam...[32m [repeated 7x across cluster][0m
[36m(LLMActor pid=12145)[0m Processed prompts:   0%|          | 1/364 [00:07<45:07,  7.46s/it, est. speed input: 27.62 toks/s, output: 3.49 toks/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Processed prompts:   0%|          | 0/364 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(LLMActor pid=12145)[0m Processed prompts:   1%|          | 2/364 [00:08<20:54,  3.46s/it, est. speed input: 43.55 toks/s, output: 7.01 toks/s]
[36m(LLMActor pid=12145)[0m Processed prompts:   4%|â–         | 15/364 [00:12<01:53,  3.08it/s, est. speed input: 233.87 toks/s, output: 56.30 toks/s][32m [repeated 18x across cluster][0m
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Processed prompts:   9%|â–‰         | 32/364 [00:17<00:56,  5.90it/s, est. speed input: 510.43 toks/s, output: 116.14 toks/s][32m [repeated 37x across cluster][0m
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Processed prompts:  15%|â–ˆâ–        | 53/364 [00:22<01:01,  5.06it/s, est. speed input: 602.14 toks/s, output: 181.18 toks/s][32m [repeated 36x across cluster][0m
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Processed prompts:  20%|â–ˆâ–‰        | 71/364 [00:27<01:22,  3.57it/s, est. speed input: 661.87 toks/s, output: 227.03 toks/s][32m [repeated 33x across cluster][0m
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 97/364 [00:32<00:51,  5.22it/s, est. speed input: 757.65 toks/s, output: 316.18 toks/s][32m [repeated 39x across cluster][0m
[36m(LLMActor pid=12145)[0m Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 181/364 [00:38<00:15, 11.58it/s, est. speed input: 899.77 toks/s, output: 597.10 toks/s][32m [repeated 39x across cluster][0m
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 151/364 [00:43<00:37,  5.61it/s, est. speed input: 911.15 toks/s, output: 488.33 toks/s][32m [repeated 44x across cluster][0m
[36m(LLMActor pid=12145)[0m Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 259/364 [00:48<00:12,  8.35it/s, est. speed input: 1024.50 toks/s, output: 834.99 toks/s][32m [repeated 44x across cluster][0m
[36m(LLMActor pid=12145)[0m Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 295/364 [00:53<00:12,  5.43it/s, est. speed input: 1081.16 toks/s, output: 949.72 toks/s][32m [repeated 32x across cluster][0m
[36m(LLMActor pid=12145)[0m Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 319/364 [00:58<00:11,  3.78it/s, est. speed input: 1075.58 toks/s, output: 1003.40 toks/s][32m [repeated 34x across cluster][0m
[36m(LLMActor pid=12145)[0m Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 334/364 [01:01<00:09,  3.05it/s, est. speed input: 1075.55 toks/s, output: 1044.51 toks/s]
[36m(LLMActor pid=12145)[0m Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 335/364 [01:02<00:09,  3.07it/s, est. speed input: 1072.74 toks/s, output: 1044.78 toks/s]
[36m(LLMActor pid=12145)[0m Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 336/364 [01:03<00:13,  2.14it/s, est. speed input: 1058.78 toks/s, output: 1034.38 toks/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 285/364 [01:03<00:29,  2.67it/s, est. speed input: 1228.04 toks/s, output: 937.52 toks/s][32m [repeated 25x across cluster][0m
[36m(LLMActor pid=12145)[0m Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 337/364 [01:04<00:16,  1.65it/s, est. speed input: 1047.36 toks/s, output: 1023.13 toks/s]
[36m(LLMActor pid=12145)[0m Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 338/364 [01:05<00:18,  1.40it/s, est. speed input: 1035.25 toks/s, output: 1015.19 toks/s]
[36m(LLMActor pid=12145)[0m Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 339/364 [01:05<00:16,  1.55it/s, est. speed input: 1031.66 toks/s, output: 1014.03 toks/s]
[36m(LLMActor pid=12145)[0m Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 340/364 [01:05<00:13,  1.80it/s, est. speed input: 1029.39 toks/s, output: 1015.17 toks/s]
[36m(LLMActor pid=12145)[0m Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 341/364 [01:06<00:11,  2.06it/s, est. speed input: 1027.21 toks/s, output: 1018.48 toks/s]
[36m(LLMActor pid=12145)[0m Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 342/364 [01:07<00:13,  1.62it/s, est. speed input: 1015.91 toks/s, output: 1012.41 toks/s]
[36m(LLMActor pid=12145)[0m Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 343/364 [01:07<00:11,  1.81it/s, est. speed input: 1012.72 toks/s, output: 1012.98 toks/s]
[36m(LLMActor pid=12145)[0m Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 344/364 [01:08<00:11,  1.75it/s, est. speed input: 1006.38 toks/s, output: 1009.90 toks/s]
[36m(LLMActor pid=12145)[0m Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 345/364 [01:08<00:10,  1.85it/s, est. speed input: 1002.86 toks/s, output: 1008.89 toks/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 310/364 [01:08<00:09,  5.55it/s, est. speed input: 1244.15 toks/s, output: 1030.35 toks/s][32m [repeated 16x across cluster][0m
[36m(LLMActor pid=12145)[0m Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 346/364 [01:10<00:17,  1.03it/s, est. speed input: 977.14 toks/s, output: 986.46 toks/s]  
[36m(LLMActor pid=12145)[0m Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 347/364 [01:11<00:15,  1.07it/s, est. speed input: 968.62 toks/s, output: 982.62 toks/s]
[36m(LLMActor pid=12145)[0m Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 348/364 [01:11<00:12,  1.30it/s, est. speed input: 965.97 toks/s, output: 985.91 toks/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 333/364 [01:13<00:13,  2.31it/s, est. speed input: 1257.77 toks/s, output: 1107.41 toks/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 332/364 [01:13<00:09,  3.30it/s, est. speed input: 1257.27 toks/s, output: 1110.59 toks/s][32m [repeated 17x across cluster][0m
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 336/364 [01:14<00:06,  4.49it/s, est. speed input: 1265.69 toks/s, output: 1129.65 toks/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 338/364 [01:14<00:05,  5.11it/s, est. speed input: 1267.43 toks/s, output: 1139.28 toks/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 339/364 [01:14<00:04,  5.39it/s, est. speed input: 1268.13 toks/s, output: 1145.29 toks/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 341/364 [01:15<00:05,  4.26it/s, est. speed input: 1265.56 toks/s, output: 1148.76 toks/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 347/364 [01:18<00:07,  2.31it/s, est. speed input: 1230.37 toks/s, output: 1137.22 toks/s][32m [repeated 6x across cluster][0m
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 351/364 [01:24<00:14,  1.15s/it, est. speed input: 1170.96 toks/s, output: 1096.96 toks/s][32m [repeated 10x across cluster][0m
[36m(LLMActor pid=12145)[0m Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 360/364 [01:31<00:08,  2.19s/it, est. speed input: 791.27 toks/s, output: 864.20 toks/s][32m [repeated 10x across cluster][0m
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 361/364 [01:36<00:05,  1.73s/it, est. speed input: 1049.54 toks/s, output: 1031.24 toks/s][32m [repeated 3x across cluster][0m
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 363/364 [01:44<00:03,  3.13s/it, est. speed input: 974.04 toks/s, output: 968.53 toks/s]  [32m [repeated 3x across cluster][0m
[36m(LLMActor pid=12145)[0m Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 362/364 [01:53<00:13,  6.96s/it, est. speed input: 637.51 toks/s, output: 710.21 toks/s]
[36m(LLMActor pid=5111, ip=10.4.34.192)[0m Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 364/364 [01:56<00:00,  5.69s/it, est. speed input: 878.98 toks/s, output: 882.00 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 364/364 [01:56<00:00,  3.13it/s, est. speed input: 878.98 toks/s, output: 882.00 toks/s]
[36m(LLMActor pid=12145)[0m Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 363/364 [02:20<00:12, 12.73s/it, est. speed input: 518.47 toks/s, output: 586.73 toks/s]
[36m(LLMActor pid=12145)[0m Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 364/364 [02:28<00:00, 11.32s/it, est. speed input: 492.12 toks/s, output: 565.57 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 364/364 [02:28<00:00,  2.45it/s, est. speed input: 492.12 toks/s, output: 565.57 toks/s]
2025-04-08 05:53:12.891 | INFO     | __main__:eval:2670 - math500/response_len_in_char: 698.5460,math500/accuracy: 0.1760,aime2024/response_len_in_char: 1085.2333,aime2024/accuracy: 0.0333,gpqa_diamond/response_len_in_char: 1022.0202,gpqa_diamond/accuracy: 0.0202,eval_accuracy: 0.0765
2025-04-08 05:53:12.898 | DEBUG    | orz.ppo.trainer:make_experience:287 - starting make_experience
2025-04-08 05:53:12.898 | DEBUG    | orz.ppo.trainer:make_experience:288 - (before query branch) prompt batch size (bs): len(all_inputs): 64
2025-04-08 05:53:12.898 | DEBUG    | orz.ppo.trainer:make_experience:531 - 
--- Policy Branch ---

2025-04-08 05:53:12.899 | DEBUG    | orz.ppo.trainer:make_experience:532 - (before policy branch) prompt batch size (bs): len(all_inputs): 64
2025-04-08 05:53:12.899 | DEBUG    | orz.ppo.trainer:make_experience:533 - (before policy branch) sample all_inputs[0]: ('A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e. your response format should be: <think> reasoning process here </think>\n\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\nThis is the problem:\nCagney can frost a cupcake every 25 seconds and Lacey can frost a cupcake every 20 seconds. If they both take a 30-second break after every 3 minutes of work, how many cupcakes can they frost together in 10 minutes?\nAssistant: <think>', {'prompt': 'A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e. your response format should be: <think> reasoning process here </think>\n\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\nThis is the problem:\nCagney can frost a cupcake every 25 seconds and Lacey can frost a cupcake every 20 seconds. If they both take a 30-second break after every 3 minutes of work, how many cupcakes can they frost together in 10 minutes?\nAssistant: <think>', 'answer': '48', 'target': '48'})
2025-04-08 05:53:12.900 | INFO     | orz.ppo.trainer:make_experience:564 - start generation
2025-04-08 06:01:14.183 | INFO     | orz.ppo.trainer:make_experience:567 - generate local rollout batch done
2025-04-08 06:01:14.183 | INFO     | orz.ppo.trainer:make_experience:552 - Generate sequences via vllm engines, time cost: 481.28s
2025-04-08 06:01:14.183 | DEBUG    | orz.ppo.trainer:make_experience:576 - 

INSPECT POLICY BRANCH PROMPTS AND RESPONSES
2025-04-08 06:01:14.183 | DEBUG    | orz.ppo.trainer:make_experience:577 - (policy branch) sample prompt, all_inputs[0]: ('A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e. your response format should be: <think> reasoning process here </think>\n\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\nThis is the problem:\nCagney can frost a cupcake every 25 seconds and Lacey can frost a cupcake every 20 seconds. If they both take a 30-second break after every 3 minutes of work, how many cupcakes can they frost together in 10 minutes?\nAssistant: <think>', {'prompt': 'A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e. your response format should be: <think> reasoning process here </think>\n\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\nThis is the problem:\nCagney can frost a cupcake every 25 seconds and Lacey can frost a cupcake every 20 seconds. If they both take a 30-second break after every 3 minutes of work, how many cupcakes can they frost together in 10 minutes?\nAssistant: <think>', 'answer': '48', 'target': '48'})

2025-04-08 06:01:14.183 | DEBUG    | orz.ppo.trainer:make_experience:578 - (policy branch) sample response, outputs[0]: {'response': " First, let's determine how many cupcakes each person can frost in a 3-minute work period without breaks.\n\nCagney: 3 minutes = 180 seconds\n180 seconds / 25 seconds = 7.2\nSince Cagney can only frost whole cupcakes, she can frost 7 cupcakes in 3 minutes.\n\nLacey: 3 minutes = 180 seconds\n180 seconds / 20 seconds = 9\nLacey can frost 9 cupcakes in 3 minutes.\n\nNow, let's calculate the total time spent working in 10 minutes, considering breaks.\n\nCagney and Lacey take a 30-second break after every 3 minutes of work.\n\n10 minutes = 600 seconds\nBreaks taken:\n(600 seconds - initial 3 minutes) / 3 minutes = 1 break\n\nTotal work time:\n600 seconds - 30 seconds (break) = 570 seconds\n\nNow, let's find out how many 3-minute work periods are in 570 seconds.\n\n570 seconds / 180 seconds (3 minutes) = 3.1667\nSince they can only work in complete 3-minute periods, they work for 3 periods.\n\nCupcakes frosted:\nCagney - 3 periods * 7 cupcakes = 21 cupcakes\nLacey - 3 periods * 9 cupcakes = 27 cupcakes\n\nTotal cupcakes frosted together:\n21 (Cagney) + 27 (Lacey) = 48 cupcakes</think>\n\n<answer> 48 cupcakes </answer>", 'iscorrect': False, 'stop_reason': 'stop', 'final_answer': '48 cupcakes', 'extra': {'prompt': 'A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e. your response format should be: <think> reasoning process here </think>\n\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\nThis is the problem:\nCagney can frost a cupcake every 25 seconds and Lacey can frost a cupcake every 20 seconds. If they both take a 30-second break after every 3 minutes of work, how many cupcakes can they frost together in 10 minutes?\nAssistant: <think>', 'answer': '48', 'target': '48'}}


2025-04-08 06:01:51.812 | INFO     | __main__:custom_reward_fn:335 - [STEP 0] [policy_reward_fn/log]: - prompt: A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e. your response format should be: <think> reasoning process here </think>

<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.
This is the problem:
Cagney can frost a cupcake every 25 seconds and Lacey can frost a cupcake every 20 seconds. If they both take a 30-second break after every 3 minutes of work, how many cupcakes can they frost together in 10 minutes?
Assistant: <think>

- output:  First, let's determine how many cupcakes each person can frost in a 3-minute work period without breaks.

Cagney: 3 minutes = 180 seconds
180 seconds / 25 seconds = 7.2
Since Cagney can only frost whole cupcakes, she can frost 7 cupcakes in 3 minutes.

Lacey: 3 minutes = 180 seconds
180 seconds / 20 seconds = 9
Lacey can frost 9 cupcakes in 3 minutes.

Now, let's calculate the total time spent working in 10 minutes, considering breaks.

Cagney and Lacey take a 30-second break after every 3 minutes of work.

10 minutes = 600 seconds
Breaks taken:
(600 seconds - initial 3 minutes) / 3 minutes = 1 break

Total work time:
600 seconds - 30 seconds (break) = 570 seconds

Now, let's find out how many 3-minute work periods are in 570 seconds.

570 seconds / 180 seconds (3 minutes) = 3.1667
Since they can only work in complete 3-minute periods, they work for 3 periods.

Cupcakes frosted:
Cagney - 3 periods * 7 cupcakes = 21 cupcakes
Lacey - 3 periods * 9 cupcakes = 27 cupcakes

Total cupcakes frosted together:
21 (Cagney) + 27 (Lacey) = 48 cupcakes</think>

<answer> 48 cupcakes </answer>

- final_answer: 48 cupcakes

- extras: prompt: A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e. your response format should be: <think> reasoning process here </think>

<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.
This is the problem:
Cagney can frost a cupcake every 25 seconds and Lacey can frost a cupcake every 20 seconds. If they both take a 30-second break after every 3 minutes of work, how many cupcakes can they frost together in 10 minutes?
Assistant: <think>
answer: 48
target: 48


- is_correct: False
2025-04-08 06:01:57.760 | DEBUG    | __main__:custom_reward_fn:360 - [STEP 0] | avg_non_stop_count=0 | percent_score_0=88.56% | percent_score_0_5=0.00% | percent_score_1=11.44% | percent_score_1_exclude_0_5=11.44% | num_tokens=[348, 337, 213, 192, 234, 413, 333, 454, 449, 392, 363, 386, 314, 408, 303, 335, 95, 564, 298, 343, 196, 170, 106, 323, 402, 266, 135, 292, 292, 315, 347, 347, 122, 482, 397, 282, 393, 390, 335, 324, 285, 136, 236, 244, 488, 352, 176, 632, 407, 292, 509, 275, 366, 372, 282, 426, 287, 343, 326, 309, 681, 271, 203, 395, 153, 155, 159, 76, 139, 140, 94, 107, 86, 86, 59, 35, 92, 89, 120, 158, 137, 120, 130, 96, 91, 207, 220, 73, 33, 45, 141, 94, 151, 116, 87, 126, 68, 86, 78, 143, 47, 78, 77, 72, 123, 148, 115, 81, 129, 54, 53, 89, 179, 107, 73, 209, 86, 108, 44, 129, 82, 345, 92, 99, 133, 84, 104, 117, 117, 328, 290, 107, 223, 253, 115, 100, 288, 224, 240, 249, 313, 228, 75, 303, 255, 257, 62, 320, 121, 393, 97, 182, 200, 118, 273, 55, 59, 283, 206, 147, 209, 180, 101, 255, 71, 266, 288, 241, 293, 297, 252, 217, 350, 55, 158, 239, 204, 195, 288, 218, 261, 426, 241, 254, 263, 251, 265, 253, 60, 76, 240, 239, 60, 433, 480, 213, 399, 498, 368, 344, 442, 166, 782, 315, 236, 846, 317, 81, 189, 254, 205, 308, 212, 114, 393, 144, 210, 176, 334, 206, 140, 338, 544, 396, 116, 281, 620, 507, 358, 297, 86, 187, 571, 488, 190, 177, 288, 507, 210, 133, 344, 418, 316, 173, 420, 91, 225, 543, 316, 399, 580, 173, 150, 486, 134, 321, 107, 530, 236, 245, 110, 376, 357, 339, 629, 278, 112, 465, 361, 513, 393, 125, 545, 236, 1643, 568, 514, 247, 552, 110, 236, 31, 767, 482, 432, 566, 519, 156, 673, 717, 128, 462, 418, 482, 572, 168, 182, 402, 603, 460, 157, 493, 459, 346, 344, 211, 97, 51, 205, 409, 520, 128, 356, 410, 475, 400, 759, 350, 250, 96, 103, 160, 200, 260, 791, 292, 300, 201, 729, 248, 210, 349, 399, 284, 118, 286, 318, 137, 122, 333, 164, 114, 386, 305, 186, 193, 421, 151, 179, 331, 820, 229, 246, 352, 147, 106, 130, 131, 117, 204, 318, 166, 109, 139, 120, 207, 173, 428, 167, 205, 160, 752, 207, 141, 483, 139, 345, 119, 204, 437, 311, 175, 83, 959, 479, 413, 570, 80, 486, 128, 235, 239, 187, 489, 162, 673, 146, 621, 95, 367, 728, 211, 624, 371, 1259, 680, 359, 428, 157, 387, 391, 518, 562, 314, 575, 110, 413, 850, 446, 270, 692, 596, 594, 554, 136, 129, 330, 295, 22, 235, 195, 645, 691, 377, 486, 473, 73, 808, 2715, 251, 386, 499, 426, 287, 543, 775, 352, 931, 161, 110, 152, 99, 321, 100, 270, 252, 264, 162, 241, 130, 117, 110, 262, 245, 142, 55, 246, 205, 141, 653, 256, 44, 214, 130, 160, 271, 180, 546, 330, 182, 320, 270, 203, 231, 135, 387, 150, 320, 315, 385, 380, 92, 573, 84, 278, 213, 294, 430, 327, 382, 108, 199, 126, 316, 574, 310, 90, 243, 268, 288, 304, 302, 226, 303, 441, 638, 224, 260, 56, 247, 143, 402, 361, 353, 124, 314, 284, 277, 240, 454, 51, 349, 97, 241, 309, 171, 240, 224, 284, 339, 343, 297, 172, 215, 417, 193, 287, 326, 212, 422, 297, 439, 415, 329, 138, 313, 270, 296, 201, 197, 352, 103, 465, 339, 264, 347, 82, 250, 152, 326, 213, 142, 264, 338, 124, 400, 62, 75, 126, 42, 80, 149, 236, 130, 47, 57, 160, 60, 155, 198, 152, 86, 104, 77, 86, 203, 165, 95, 99, 92, 81, 100, 224, 214, 170, 75, 181, 145, 144, 117, 155, 148, 113, 61, 94, 99, 79, 110, 92, 100, 82, 81, 108, 184, 154, 70, 131, 73, 128, 138, 142, 109, 182, 175, 48, 223, 67, 143, 149, 115, 384, 210, 705, 287, 414, 94, 512, 415, 190, 399, 308, 220, 68, 76, 355, 200, 379, 307, 445, 310, 139, 690, 228, 85, 286, 80, 100, 254, 90, 320, 548, 418, 195, 98, 314, 111, 251, 299, 359, 520, 770, 348, 245, 175, 379, 158, 360, 478, 458, 552, 77, 81, 393, 113, 144, 515, 223, 294, 288, 413, 283, 85, 276, 188, 540, 188, 613, 249, 500, 607, 561, 988, 468, 311, 576, 1058, 404, 443, 564, 122, 566, 499, 742, 698, 406, 537, 638, 627, 591, 218, 382, 992, 389, 92, 439, 262, 816, 448, 358, 740, 186, 278, 888, 260, 454, 215, 651, 646, 370, 1107, 474, 307, 635, 181, 481, 467, 616, 433, 416, 1135, 139, 199, 53, 898, 673, 80, 916, 687, 423, 378, 383, 548, 220, 201, 82, 350, 276, 204, 226, 366, 233, 216, 264, 268, 162, 106, 417, 636, 202, 146, 204, 103, 159, 164, 279, 279, 258, 158, 329, 229, 242, 223, 262, 136, 158, 459, 495, 211, 209, 145, 259, 443, 47, 326, 267, 300, 119, 279, 207, 150, 284, 156, 304, 549, 297, 145, 154, 190, 134, 188, 171, 133, 272, 222, 215, 201, 270, 222, 236, 225, 731, 277, 187, 142, 295, 112, 239, 204, 158, 424, 299, 289, 162, 239, 224, 192, 182, 233, 185, 134, 151, 155, 253, 103, 297, 119, 146, 62, 374, 245, 218, 271, 258, 297, 170, 337, 225, 287, 297, 607, 416, 361, 227, 1622, 372, 184, 178, 225, 330, 237, 371, 264, 150, 248, 318, 103, 397, 428, 290, 78, 208, 335, 416, 424, 310, 186, 804, 407, 276, 163, 202, 372, 519, 211, 528, 365, 253, 427, 95, 191, 713, 399, 441, 471, 230, 58, 36, 240, 425, 607, 560, 494, 335, 376, 184, 385, 361, 389, 34, 313, 464, 572, 313, 289, 362, 451, 187, 321, 137, 546, 413, 236, 340, 85, 201, 331, 560, 377, 276, 435, 50, 172, 357, 549, 372, 112, 323, 596, 47, 234, 457, 401, 420, 565, 418, 454, 437, 266, 382, 414, 340, 592, 389, 212, 334, 948, 327, 565, 50, 387, 534, 130, 432, 279, 217, 145, 411, 259, 332, 243, 308, 242, 504, 598, 380, 604, 747, 325, 493, 268, 506, 257, 412, 555, 356, 342, 250, 501, 296, 194, 99, 306, 207, 237, 285, 188, 178, 180, 256, 205, 86, 221, 98, 298, 373, 183, 154, 257, 57, 219, 262, 214, 311, 234, 196, 107, 185, 107, 106, 255, 133, 199, 273, 330, 227, 114, 193, 217, 76, 189, 368, 210, 154, 104, 176, 118, 272, 208, 220, 249, 311, 183, 181, 192, 173, 190, 322, 135, 139, 75, 327, 266, 253, 63, 292, 304, 206, 137, 136, 374, 165, 605, 191, 92, 259, 368, 401, 81, 295, 150, 355, 246, 80, 302, 326, 281, 106, 136, 132, 323, 66, 1419, 215, 190, 185, 523, 136, 110, 141, 610, 58, 216, 237, 252, 114, 118, 299, 487, 353, 237, 357, 244, 283, 66, 85, 238, 322, 126, 75, 85, 88, 300, 181, 166, 120, 642, 260, 159, 116, 27, 317, 246, 207, 95, 109, 106, 144, 148, 142, 115, 156, 108, 100, 122, 125, 154, 98, 193, 161, 152, 94, 160, 166, 201, 80, 79, 118, 161, 105, 143, 100, 107, 184, 82, 146, 149, 70, 135, 88, 124, 98, 181, 125, 142, 133, 123, 121, 150, 116, 88, 114, 123, 150, 161, 81, 196, 71, 209, 116, 150, 78, 131, 243, 153, 94, 208, 82, 84, 541, 176, 203, 191, 290, 174, 168, 406, 56, 157, 123, 117, 211, 72, 70, 203, 220, 221, 310, 327, 196, 194, 194, 127, 423, 150, 71, 222, 184, 159, 130, 70, 198, 117, 132, 127, 155, 184, 360, 606, 96, 155, 85, 114, 215, 237, 76, 398, 183, 274, 364, 138, 447, 143, 231, 88, 109, 321, 216, 120, 285, 247, 461, 437, 387, 680, 429, 446, 719, 678, 835, 71, 319, 545, 416, 406, 65, 293, 306, 594, 729, 772, 224, 849, 308, 599, 110, 296, 748, 543, 339, 187, 959, 169, 496, 419, 682, 286, 355, 413, 527, 583, 429, 406, 170, 516, 485, 513, 460, 245, 249, 375, 353, 584, 125, 831, 663, 736, 65, 284, 469, 406, 379, 205, 278, 716, 204, 406, 246, 103, 118, 99, 103, 122, 423, 120, 204, 222, 61, 115, 224, 133, 97, 417, 97, 132, 116, 117, 134, 251, 101, 447, 168, 201, 107, 56, 195, 173, 151, 189, 81, 109, 236, 243, 159, 294, 212, 160, 186, 167, 358, 107, 221, 190, 327, 118, 153, 144, 445, 103, 191, 113, 203, 202, 113, 235, 72, 273, 163, 92, 88, 366, 55, 366, 278, 454, 64, 360, 212, 126, 434, 297, 379, 173, 284, 170, 517, 265, 371, 367, 302, 108, 42, 107, 306, 231, 111, 270, 288, 285, 163, 208, 1690, 511, 332, 65, 199, 511, 491, 454, 336, 410, 78, 506, 213, 197, 152, 59, 387, 128, 284, 160, 317, 265, 511, 308, 275, 576, 279, 372, 127, 326, 190, 261, 532, 467, 596, 373, 301, 307, 309, 414, 294, 397, 301, 348, 277, 319, 405, 511, 348, 485, 267, 71, 375, 222, 199, 420, 450, 226, 35, 441, 393, 121, 413, 737, 501, 604, 139, 448, 547, 109, 285, 143, 268, 254, 171, 400, 354, 299, 227, 507, 891, 564, 421, 60, 574, 248, 374, 378, 191, 332, 213, 342, 674, 411, 235, 173, 401, 98, 196, 447, 540, 748, 271, 994, 212, 121, 320, 101, 325, 294, 336, 295, 250, 276, 381, 253, 131, 242, 388, 136, 281, 486, 319, 89, 943, 354, 178, 111, 315, 657, 551, 270, 445, 355, 883, 104, 265, 367, 192, 877, 498, 299, 279, 102, 827, 317, 608, 469, 270, 214, 225, 282, 102, 417, 203, 224, 492, 194, 350, 435, 83, 266, 398, 276, 386, 276, 168, 280, 375, 213, 244, 249, 360, 223, 284, 150, 259, 426, 187, 321, 402, 352, 255, 223, 204, 121, 132, 132, 369, 347, 431, 69, 260, 305, 328, 334, 217, 310, 324, 110, 589, 265, 217, 350, 313, 365, 128, 228, 293, 408, 252, 249, 195, 301, 177, 279, 386, 213, 361, 213, 197, 237, 208, 188, 81, 134, 127, 142, 84, 140, 184, 175, 233, 70, 322, 81, 111, 247, 124, 35, 78, 166, 180, 161, 84, 130, 114, 166, 106, 69, 52, 172, 70, 115, 127, 53, 150, 230, 148, 78, 250, 282, 45, 69, 230, 86, 101, 215, 167, 199, 61, 125, 114, 110, 84, 181, 211, 121, 152, 138, 87, 41, 75, 66, 200, 82, 215, 36, 178, 283, 332, 269, 95, 90, 70, 138, 245, 344, 191, 71, 242, 309, 293, 225, 75, 199, 188, 635, 206, 113, 412, 67, 232, 246, 505, 156, 102, 227, 212, 263, 71, 156, 297, 95, 278, 83, 204, 257, 323, 267, 276, 1203, 75, 148, 244, 241, 147, 95, 49, 158, 250, 380, 234, 293, 786, 106, 290, 309, 176, 151, 435, 225, 309, 284, 166, 145, 409, 97, 139, 10, 246, 131, 79, 434, 448, 259, 596, 133, 91, 381, 185, 111, 354, 91, 120, 245, 271, 396, 186, 307, 288, 159, 373, 279, 124, 56, 267, 64, 123, 328, 89, 93, 432, 561, 174, 212, 147, 104, 427, 344, 109, 551, 396, 383, 183, 123, 364, 125, 151, 358, 116, 117, 345, 305, 115, 71, 332, 270, 206, 154, 472, 276, 285, 206, 194, 263, 45, 103, 185, 293, 314, 125, 207, 251, 204, 264, 78, 148, 159, 199, 291, 314, 303, 248, 383, 183, 234, 259, 282, 159, 247, 196, 286, 120, 146, 111, 341, 223, 238, 178, 231, 179, 170, 83, 228, 277, 188, 313, 234, 180, 206, 328, 70, 318, 212, 310, 266, 257, 193, 242, 329, 151, 50, 108, 168, 148, 75, 271, 207, 306, 288, 154, 63, 322, 228, 260, 90, 260, 146, 247, 157, 119, 340, 120, 276, 174, 265, 126, 433, 90, 376, 288, 114, 278, 205, 93, 229, 146, 325, 88, 233, 322, 428, 322, 116, 171, 285, 352, 93, 52, 160, 322, 117, 312, 331, 297, 308, 126, 110, 124, 319, 343, 302, 147, 737, 357, 635, 205, 206, 65, 292, 623, 531, 623, 658, 497, 329, 562, 527, 446, 392, 335, 262, 603, 469, 265, 63, 157, 222, 266, 16, 662, 679, 276, 683, 293, 301, 108, 526, 515, 748, 36, 513, 380, 575, 524, 148, 446, 199, 161, 131, 355, 159, 502, 510, 24, 733, 457, 532, 245, 151, 397, 381, 502, 345, 522, 381, 113, 63, 564, 449, 380, 556, 396, 194, 505, 676, 105, 1078, 564, 565, 749, 436, 649, 517, 477, 796, 288, 109, 756, 1232, 480, 231, 305, 416, 198, 249, 145, 326, 290, 718, 110, 317, 802, 738, 357, 271, 160, 173, 491, 47, 615, 260, 477, 301, 340, 543, 323, 306, 806, 223, 398, 198, 185, 446, 180, 698, 423, 151, 463, 543, 454, 407, 292, 80, 200, 169, 214, 311, 74, 203, 102, 332, 244, 405, 216, 324, 361, 186, 232, 325, 330, 481, 297, 111, 236, 113, 119, 88, 76, 303, 268, 154, 108, 225, 125, 19, 602, 65, 536, 179, 248, 330, 130, 65, 507, 82, 155, 60, 146, 105, 211, 336, 179, 135, 315, 278, 515, 218, 684, 195, 145, 302, 220, 76, 202, 441, 648, 422, 632, 349, 543, 211, 382, 70, 531, 248, 190, 733, 507, 503, 369, 448, 448, 444, 441, 597, 490, 153, 361, 378, 327, 184, 330, 195, 79, 267, 409, 317, 711, 110, 156, 468, 503, 202, 246, 737, 312, 376, 455, 509, 225, 176, 337, 305, 595, 452, 1935, 136, 299, 138, 178, 123, 478, 341, 355, 837, 178, 79, 281, 172, 428, 47, 509, 247, 515, 183, 107, 294, 221, 72, 241, 723, 247, 203, 276, 307, 199, 292, 266, 232, 556, 233, 196, 169, 96, 268, 273, 361, 628, 557, 482, 90, 87, 282, 477, 498, 383, 165, 246, 300, 198, 85, 223, 207, 106, 642, 240, 83, 291, 162, 352, 555, 25, 325, 261, 393, 266, 132, 487, 398, 229, 305, 172, 162, 225, 342, 160, 265, 125, 181, 543, 328, 313, 417, 369, 183, 340, 366, 86, 385, 845, 474, 257, 549, 395, 458, 110, 101, 54, 313, 338, 192, 145, 395, 534, 18, 624, 178, 96, 110, 486, 82, 96, 312, 305, 171, 144, 667, 408, 312, 335, 166, 359, 550, 456, 916, 338, 35, 204, 216, 173, 183, 450, 156, 699, 906, 250, 102, 219, 86, 157, 44, 106, 70, 94, 223, 211, 199, 193, 253, 269, 156, 117, 44, 207, 226, 238, 225, 237, 37, 256, 76, 88, 169, 227, 160, 302, 171, 241, 175, 231, 163, 207, 36, 232, 281, 327, 130, 285, 43, 182, 54, 113, 209, 54, 101, 101, 65, 366, 407, 266, 216, 252, 219, 93, 117, 162, 90, 225, 242, 212, 675, 651, 45, 308, 901, 174, 571, 409, 576, 579, 764, 286, 1085, 598, 447, 1297, 687, 226, 20, 26, 503, 546, 907, 833, 619, 644, 1081, 96, 613, 370, 755, 98, 308, 213, 349, 486, 938, 363, 783, 636, 703, 624, 183, 548, 251, 572, 218, 39, 427, 290, 616, 771, 351, 651, 1000, 413, 134, 542, 222, 283, 36, 505, 619, 79, 459, 184, 450, 213, 75, 130, 343, 185, 194, 221, 402, 307, 159, 114, 359, 342, 104, 274, 188, 124, 131, 289, 425, 439, 619, 86, 147, 188, 186, 368, 283, 182, 164, 343, 482, 260, 258, 384, 618, 76, 361, 267, 122, 769, 186, 402, 179, 299, 477, 952, 129, 279, 359, 278, 597, 428, 438, 710, 128, 197, 330, 54, 564, 176, 900, 228, 820, 668, 605, 90, 283, 844, 457, 443, 124, 677, 385, 549, 396, 204, 71, 588, 1594, 567, 430, 495, 304, 1036, 696, 654, 1011, 839, 369, 479, 761, 371, 584, 102, 496, 412, 289, 526, 427, 513, 132, 41, 389, 206, 487, 811, 718, 277, 171, 487, 571, 507, 557, 832, 393, 81, 137, 406, 734, 771, 195, 655, 263, 367, 172, 149, 225, 131, 138, 146, 216, 177, 343, 126, 170, 125, 318, 237, 203, 254, 212, 153, 344, 154, 162, 206, 137, 37, 158, 84, 299, 131, 90, 151, 169, 150, 278, 209, 139, 179, 281, 174, 206, 255, 216, 180, 150, 154, 113, 139, 202, 102, 191, 244, 167, 299, 225, 129, 173, 137, 64, 153, 164, 286, 111, 165, 320, 331, 339, 146, 231, 133, 257, 138, 235, 34, 322, 81, 232, 123, 169, 154, 94, 317, 161, 231, 230, 339, 268, 115, 219, 57, 304, 194, 47, 213, 212, 307, 338, 168, 143, 525, 232, 456, 359, 135, 198, 384, 77, 303, 324, 125, 236, 237, 284, 300, 135, 145, 312, 240, 251, 160, 95, 262, 353, 91, 328, 190, 295, 117, 171, 366, 121, 142, 109, 301, 182, 428, 195, 145, 84, 165, 168, 62, 116, 351, 223, 74, 115, 125, 127, 247, 103, 81, 107, 49, 135, 237, 291, 276, 36, 210, 92, 87, 300, 161, 47, 155, 71, 163, 98, 102, 132, 145, 54, 228, 84, 203, 189, 63, 76, 155, 34, 145, 246, 214, 340, 45, 258, 51, 102, 195, 216, 56, 155, 163, 241, 328, 247, 249, 220, 390, 247, 304, 324, 240, 262, 192, 180, 184, 278, 240, 294, 226, 323, 89, 187, 286, 269, 206, 192, 104, 243, 106, 237, 215, 309, 157, 126, 424, 241, 263, 173, 268, 398, 212, 167, 290, 191, 218, 263, 92, 284, 206, 214, 188, 227, 305, 267, 184, 289, 196, 316, 220, 116, 29, 141, 115, 128, 135, 377, 173, 58, 474, 344, 163, 384, 603, 125, 173, 332, 122, 164, 222, 430, 152, 93, 154, 209, 203, 232, 194, 444, 67, 362, 548, 253, 189, 325, 143, 207, 211, 178, 710, 532, 423, 364, 170, 276, 220, 342, 254, 292, 132, 295, 293, 386, 138, 397, 412, 184, 139, 378, 1245, 154, 483, 98, 240, 702, 386, 319, 228, 19, 538, 254, 233, 212, 365, 217, 90, 219, 188, 212, 309, 148, 267, 201, 79, 180, 101, 305, 334, 294, 63, 283, 298, 324, 183, 109, 228, 186, 186, 241, 279, 252, 287, 264, 269, 265, 210, 236, 202, 181, 184, 200, 209, 186, 24, 207, 218, 335, 112, 229, 388, 55, 268, 245, 288, 448, 151, 213, 236, 286, 305, 166, 130, 95, 358, 711, 468, 545, 53, 199, 769, 281, 206, 532, 217, 484, 264, 339, 15, 136, 240, 148, 290, 109, 318, 252, 113, 138, 302, 321, 289, 559, 198, 145, 418, 425, 195, 249, 417, 314, 90, 112, 553, 174, 125, 81, 328, 40, 415, 433, 428, 476, 159, 309, 164, 335, 222, 120, 81, 214, 358, 132, 120, 61, 568, 103, 93, 294, 182, 97, 671, 334, 378, 583, 354, 797, 266, 315, 44, 420, 431, 502, 326, 976, 369, 95, 382, 73, 772, 519, 148, 682, 440, 107, 303, 458, 785, 83, 146, 80, 146, 425, 314, 863, 619, 446, 329, 85, 569, 108, 546, 627, 88, 488, 76, 344, 678, 537, 384, 218, 354, 242, 330, 542, 536, 221, 86, 1498, 227, 458, 618, 399, 202, 314, 200, 325, 128, 195, 154, 220, 564, 426, 397, 617, 80, 308, 163, 674, 377, 448, 428, 549, 535, 381, 455, 310, 612, 276, 613, 142, 98, 112, 362, 283, 475, 517, 312, 464, 918, 749, 338, 841, 283, 487, 210, 151, 466, 535, 123, 259, 444, 98, 727, 396, 43, 729, 234, 80, 294, 805, 615, 491, 445, 246, 309, 336, 506, 200, 140, 100, 82, 81, 118, 67, 127, 280, 116, 293, 138, 165, 182, 226, 159, 126, 313, 188, 293, 250, 210, 130, 78, 152, 149, 248, 233, 141, 146, 101, 254, 332, 211, 292, 180, 231, 204, 111, 258, 97, 147, 167, 231, 178, 128, 142, 215, 143, 97, 40, 69, 152, 163, 86, 242, 253, 55, 131, 63, 204, 207, 157, 76, 203, 799, 1043, 37, 697, 104, 226, 117, 851, 82, 822, 341, 132, 974, 818, 113, 498, 356, 551, 285, 553, 224, 373, 310, 584, 394, 728, 100, 44, 132, 80, 671, 208, 670, 393, 644, 580, 691, 707, 391, 589, 57, 413, 112, 98, 448, 297, 477, 280, 396, 291, 53, 49, 431, 430, 1055, 107, 147, 267, 713, 763, 56, 848, 837, 173, 43, 80, 350, 105, 637, 308, 454, 93, 41, 307, 427, 170, 201, 239, 138, 344, 129, 506, 455, 50, 355, 277, 236, 578, 300, 367, 253, 329, 365, 236, 307, 468, 276, 529, 279, 108, 74, 221, 485, 366, 343, 328, 152, 377, 107, 557, 255, 159, 145, 89, 363, 287, 108, 63, 420, 167, 394, 123, 350, 125, 290, 107, 122, 351, 365, 414, 219, 143, 282, 62, 314, 304, 277, 213, 304, 327, 296, 276, 287, 240, 333, 731, 236, 526, 388, 238, 72, 207, 195, 225, 383, 419, 469, 181, 278, 445, 622, 175, 301, 430, 427, 213, 305, 107, 213, 275, 119, 104, 420, 371, 308, 281, 427, 181, 199, 364, 441, 97, 320, 392, 138, 245, 431, 193, 347, 327, 319, 197, 305, 250, 132, 87, 359, 208, 130, 165, 187, 227, 147, 194, 86, 99, 221, 224, 235, 98, 66, 227, 420, 213, 323, 257, 309, 139, 644, 314, 212, 136, 207, 269, 354, 367, 126, 131, 291, 241, 301, 532, 365, 181, 188, 320, 154, 385, 331, 166, 290, 409, 351, 292, 209, 245, 166, 167, 503, 100, 157, 155, 280, 161, 373, 251, 180, 280, 304, 146, 289, 296, 425, 346, 230, 321, 442, 295, 566, 229, 213, 297, 322, 345, 542, 127, 333, 262, 215, 276, 386, 239, 267, 414, 155, 157, 134, 238, 375, 115, 646, 259, 186, 185, 474, 122, 238, 256, 283, 205, 215, 133, 100, 393, 325, 336, 51, 274, 375, 258, 246, 212, 189, 437, 182, 158, 414, 261, 297, 202, 199, 150, 179, 252, 109, 289, 335, 183, 205, 132, 151, 122, 242, 228, 343, 140, 305, 231, 231, 240, 250, 207, 140, 280, 180, 153, 293, 259, 211, 107, 25, 60, 270, 228, 154, 100, 186, 213, 178, 154, 119, 207, 163, 4, 348, 247, 254, 140, 128, 268, 240, 210, 200, 139, 174, 97, 235, 165, 250, 156, 206, 230, 188, 318, 274, 118, 376, 131, 89, 306, 236, 451, 161, 341, 206, 78, 169, 391, 353, 146, 319, 348, 283, 369, 152, 241, 36, 240, 285, 325, 262, 400, 249, 175, 327, 85, 206, 379, 494, 418, 205, 483, 133, 214, 290, 222, 366, 314, 271, 371, 515, 165, 80, 139, 176, 162, 149, 504, 116, 149, 140, 130, 264, 35, 392, 302, 437, 522, 66, 60, 246, 154, 282, 180, 324, 342, 168, 64, 244, 274, 126, 41, 194, 255, 515, 169, 234, 178, 107, 70, 198, 142, 171, 126, 178, 101, 177, 87, 166, 273, 293, 1080, 165, 113, 85, 395, 152, 298, 175, 321, 135, 180, 132, 34, 238, 134, 927, 110, 143, 213, 39, 57, 146, 357, 162, 150, 109, 316, 242, 266, 130, 419, 123, 366, 332, 297, 262, 186, 156, 282, 168, 90, 85, 189, 325, 205, 348, 136, 150, 362, 186, 185, 287, 223, 370, 165, 362, 368, 181, 336, 307, 296, 95, 269, 238, 198, 323, 150, 120, 220, 268, 505, 275, 269, 394, 201, 233, 187, 281, 330, 220, 329, 139, 137, 135, 497, 252, 178, 295, 324, 343, 234, 329, 417, 281, 397, 600, 148, 204, 449, 714, 176, 381, 68, 410, 199, 238, 271, 687, 362, 50, 293, 400, 225, 280, 272, 401, 354, 381, 39, 243, 19, 334, 356, 428, 297, 461, 188, 236, 166, 244, 378, 373, 286, 347, 261, 436, 165, 96, 145, 228, 261, 40, 86, 341, 271, 279, 220, 139, 217, 189, 281, 677, 448, 310, 49, 452, 314, 146, 631, 176, 536, 356, 303, 622, 121, 225, 110, 86, 246, 675, 248, 115, 428, 614, 345, 449, 734, 115, 163, 80, 272, 342, 352, 304, 309, 199, 315, 488, 436, 127, 173, 162, 680, 178, 149, 240, 293, 247, 113, 74, 341, 352, 149, 104, 470, 185, 849, 156, 194, 215, 259, 237, 400, 431, 167, 170, 604, 98, 194, 231, 236, 192, 247, 731, 105, 502, 495, 340, 190, 378, 183, 212, 54, 694, 529, 290, 260, 438, 235, 285, 425, 194, 239, 556, 579, 273, 428, 148, 66, 664, 345, 238, 530, 1229, 102, 152, 334, 221, 1075, 319, 83, 356, 336, 181, 76, 198, 61, 693, 323, 199, 692, 683, 345, 957, 1349, 531, 335, 280, 863, 443, 427, 450, 500, 288, 193, 301, 185, 294, 69, 1099, 1729, 283, 389, 187, 1277, 82, 425, 566, 131, 412, 327, 581, 389, 513, 362, 215, 1002, 234, 676, 698, 79, 346, 289, 180, 1099, 91, 279, 210, 237, 596, 114, 280, 217, 333, 210, 681, 1057, 160, 126, 800, 114, 191, 141, 93, 117, 723, 422, 38, 175, 100, 578, 1639, 500, 531, 1135, 589, 424, 287, 227, 270] | stop_percentage=92.21
