[2025-04-10 21:02:33,324] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2025-04-10 21:02:41.846 | INFO     | __main__:PPOExpConfig:164 - prompt_data: ['data/orz_math_57k_collected.json'], eval_prompt_data: ['data/eval_data/math500.json', 'data/eval_data/aime2024.json', 'data/eval_data/gpqa_diamond.json'], prompt_data_probs: [1.0]
2025-04-10 21:02:41.856 | INFO     | __main__:<module>:2831 - --------- config key ---------                  ------ value ------
seed                                            42
ref_num_nodes                                   8
ref_num_gpus_per_node                           1
reward_num_nodes                                1
reward_num_gpus_per_node                        2
actor_num_nodes                                 8
actor_num_gpus_per_node                         1
critic_num_nodes                                8
critic_num_gpus_per_node                        1
colocate_critic_reward                          True
colocate_actor_ref                              True
colocate_all                                    True
vllm_num_engines                                8
vllm_tensor_parallel_size                       1
vllm_sync_backend                               nccl
local_rank                                      -1
pretrain                                        Qwen/Qwen2.5-7B
critic_pretrain                                 Qwen/Qwen2.5-7B
reward_pretrain                                 <class 'NoneType'>
ckpt_path                                       /vc_data_blob/users/kevihuang/models/orz/orz_ckpt/debug_orz_7b_ppo_self_play__math__v1439__self_play_False
save_path                                       /vc_data_blob/users/kevihuang/models/orz/orz_ckpt/debug_orz_7b_ppo_self_play__math__v1439__self_play_False
tensorboard_log_dir                             orz_logs/debug_orz_7b_ppo_self_play__math__v1439__self_play_False
prompt_data                                     <class 'omegaconf.listconfig.ListConfig'>
load_checkpoint                                 True
zero_stage                                      3
bf16                                            True
zpg                                             1
adam_offload                                    False
flash_attn                                      True
grad_accum_dtype                                <class 'NoneType'>
disable_trace_cache                             False
gradient_checkpointing                          True
gradient_checkpointing_use_reentrant            False
disable_fast_tokenizer                          False
target_modules                                  all-linear
enable_prefix_caching                           True
enable_chunked_prefill                          False
max_num_batched_tokens                          2048
enforce_eager                                   False
gpu_memory_utilization                          0.5
eval_steps                                      -1
save_steps                                      -1
save_interval                                   50
actor_learning_rate                             1e-06
critic_learning_rate                            5e-06
num_episodes                                    20
max_epochs                                      1
prompt_max_len                                  8000
generate_max_len                                8000
train_batch_size                                256
micro_train_batch_size                          1
rollout_batch_size                              8
micro_rollout_batch_size                        128
micro_forward_batch_size                        1
policy_update_steps                             1
critic_update_steps                             1
max_len                                         8192
max_norm                                        1.0
num_warmup_steps                                50
l2                                              0.0
eps_clip                                        0.2
value_clip                                      0.2
lambd                                           1.0
gamma                                           1.0
normalize_reward                                True
top_p                                           1.0
temperature                                     1.0
freezing_actor_steps                            -1
n_samples_per_prompt                            8
kl_target                                       <class 'NoneType'>
init_kl_coef                                    0
use_kl_estimator_k3                             True
use_abs_kl                                      False
use_kl_loss                                     True
kl_loss_coef                                    0.0
adam_betas                                      (0.9, 0.95)
reward_clip_range                               (-10, 10)
use_compute_reward_fn                           True
advantage_normalize                             True
value_head_prefix                               value_head
ref_reward_offload                              False
enable_eval                                     True
eval_interval                                   10
update_ref_every_epoch                          True
use_orm_score                                   False
total_num_nodes                                 8
n_policy_evaluator_samples_per_policy_response  1
eval_prompt_data                                <class 'omegaconf.listconfig.ListConfig'>
prompt_data_probs                               <class 'omegaconf.listconfig.ListConfig'>
packing_max_len                                 16384
top_k                                           -1
stop                                            <class 'omegaconf.listconfig.ListConfig'>
use_grpo                                        False
2025-04-10 21:02:44,700	WARNING utils.py:580 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2025-04-10 21:02:44,701	WARNING utils.py:592 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 90.16 to 90.
2025-04-10 21:02:44,906	INFO worker.py:1821 -- Started a local Ray instance.
[36m(LLMActor pid=1829141)[0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[36m(LLMActor pid=1829141)[0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.74it/s]
[36m(LLMActor pid=1829141)[0m 
[36m(LLMActor pid=1829147)[0m 
[36m(LLMActor pid=1829143)[0m 
[36m(LLMActor pid=1829139)[0m 
[36m(LLMActor pid=1829144)[0m 
[36m(LLMActor pid=1829142)[0m 
[36m(LLMActor pid=1829145)[0m 
[36m(LLMActor pid=1829146)[0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s][32m [repeated 7x across cluster][0m
[36m(LLMActor pid=1829146)[0m 
2025-04-10 21:03:28.735 | INFO     | orz.ppo.utils:create_vllm_engines:501 - Offloaded all vLLM engines to CPU
2025-04-10 21:03:29.852 | INFO     | __main__:train_dataset:2775 - Start processing 56878 dialogues
2025-04-10 21:04:09.504 | INFO     | __main__:train_dataset:2786 - Finished processing 56878 dialogues
2025-04-10 21:04:09.561 | INFO     | __main__:eval_dataset:2804 - Start processing 728 dialogues
2025-04-10 21:04:10.078 | INFO     | __main__:eval_dataset:2825 - Finished processing 728 dialogues
[36m(RefRayActorBase pid=1831192)[0m [W410 21:04:57.256587326 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[36m(LLMActor pid=1829146)[0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.36it/s][32m [repeated 39x across cluster][0m
[36m(RefRayActorBase pid=1831192)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(RefRayActorBase pid=1831192)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[36m(RefRayActorBase pid=1832132)[0m [W410 21:04:57.217522321 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())[32m [repeated 7x across cluster][0m
[36m(RefRayActorBase pid=1832132)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 7x across cluster][0m
[36m(RefRayActorBase pid=1832133)[0m Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:03,  1.16s/it]
[36m(RefRayActorBase pid=1831192)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.05s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.10s/it]
[36m(RefRayActorBase pid=1832132)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1831007)[0m [W410 21:05:18.463431669 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[36m(RefRayActorBase pid=1832132)[0m Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:03<00:01,  1.15s/it][32m [repeated 23x across cluster][0m
[36m(PolicyRayActorBase pid=1831194)[0m [W410 21:05:18.417253899 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[36m(PolicyRayActorBase pid=1831007)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(RefRayActorBase pid=1832132)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.10s/it][32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1831007)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1831196)[0m [W410 21:05:18.451756492 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())[32m [repeated 6x across cluster][0m
[36m(PolicyRayActorBase pid=1831196)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1831194)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[36m(PolicyRayActorBase pid=1831195)[0m Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:03,  1.14s/it]
[36m(PolicyRayActorBase pid=1831194)[0m Using /home/aiscuser/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
[36m(PolicyRayActorBase pid=1831195)[0m Detected CUDA files, patching ldflags
[36m(PolicyRayActorBase pid=1831195)[0m Emitting ninja build file /home/aiscuser/.cache/torch_extensions/py310_cu124/fused_adam/build.ninja...
[36m(PolicyRayActorBase pid=1831195)[0m /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36m(PolicyRayActorBase pid=1831195)[0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36m(PolicyRayActorBase pid=1831195)[0m   warnings.warn(
[36m(PolicyRayActorBase pid=1831195)[0m Building extension module fused_adam...
[36m(PolicyRayActorBase pid=1831195)[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[36m(PolicyRayActorBase pid=1831007)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.18s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.20s/it][32m [repeated 8x across cluster][0m
[36m(PolicyRayActorBase pid=1831196)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][32m [repeated 6x across cluster][0m
[36m(PolicyRayActorBase pid=1831195)[0m Loading extension module fused_adam...
[36m(PolicyRayActorBase pid=1831007)[0m Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:03<00:01,  1.25s/it][32m [repeated 23x across cluster][0m
[36m(PolicyRayActorBase pid=1831007)[0m set_mempolicy: Operation not permitted
[36m(PolicyRayActorBase pid=1831007)[0m Using /home/aiscuser/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...[32m [repeated 7x across cluster][0m
[36m(CriticRayActorBase pid=1832137)[0m [W410 21:05:50.837359189 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[36m(PolicyRayActorBase pid=1831196)[0m Loading extension module fused_adam...[32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1831196)[0m set_mempolicy: Operation not permitted[32m [repeated 7x across cluster][0m
[36m(CriticRayActorBase pid=1832137)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[36m(CriticRayActorBase pid=1832137)[0m Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:01,  2.82it/s]
[36m(CriticRayActorBase pid=1832137)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  4.14it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  3.86it/s]
[36m(CriticRayActorBase pid=1832137)[0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[36m(CriticRayActorBase pid=1832137)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(CriticRayActorBase pid=1833032)[0m [W410 21:05:50.833443438 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())[32m [repeated 7x across cluster][0m
[36m(CriticRayActorBase pid=1832137)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][32m [repeated 8x across cluster][0m
[36m(CriticRayActorBase pid=1833077)[0m Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:01<00:00,  2.41it/s][32m [repeated 23x across cluster][0m
[36m(CriticRayActorBase pid=1833077)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.40it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.44it/s][32m [repeated 7x across cluster][0m
[36m(CriticRayActorBase pid=1833077)[0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.[32m [repeated 7x across cluster][0m
[36m(CriticRayActorBase pid=1833077)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 7x across cluster][0m
[36m(CriticRayActorBase pid=1833023)[0m Some weights of CriticModel were not initialized from the model checkpoint at Qwen/Qwen2.5-7B and are newly initialized: ['value_head.weight']
[36m(CriticRayActorBase pid=1833023)[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[36m(CriticRayActorBase pid=1833023)[0m 2025-04-10 21:06:10.497 | INFO     | orz.ppo.models:get_llm_for_sequence_regression:572 - initialize value_head for ZeRO-3 reward model training.
[36m(CriticRayActorBase pid=1832137)[0m Using /home/aiscuser/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
[36m(CriticRayActorBase pid=1833049)[0m Detected CUDA files, patching ldflags
[36m(CriticRayActorBase pid=1833049)[0m Emitting ninja build file /home/aiscuser/.cache/torch_extensions/py310_cu124/fused_adam/build.ninja...
[36m(CriticRayActorBase pid=1833049)[0m /scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/venv/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[36m(CriticRayActorBase pid=1833049)[0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[36m(CriticRayActorBase pid=1833049)[0m   warnings.warn(
[36m(CriticRayActorBase pid=1833049)[0m Building extension module fused_adam...
[36m(CriticRayActorBase pid=1833049)[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[36m(CriticRayActorBase pid=1832137)[0m Loading extension module fused_adam...
[36m(CriticRayActorBase pid=1832137)[0m set_mempolicy: Operation not permitted
[36m(CriticRayActorBase pid=1833032)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][32m [repeated 7x across cluster][0m
[36m(CriticRayActorBase pid=1833077)[0m Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:03<00:01,  1.08s/it][32m [repeated 24x across cluster][0m
[36m(CriticRayActorBase pid=1833077)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.06it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.00s/it][32m [repeated 8x across cluster][0m
2025-04-10 21:06:18.374 | INFO     | orz.ppo.trainer:build_models:1949 - init policy/ref/critic/reward models done
2025-04-10 21:06:21.008 | INFO     | orz.ppo.trainer:train:79 - Create vllm engine groups done.
2025-04-10 21:06:22.999 | INFO     | orz.ppo.trainer:train:81 - Sync actor weights to vllm engines, time cost: 1.99s
2025-04-10 21:06:24.378 | INFO     | orz.ppo.trainer:train:85 - Offload policy model to cpu, time cost: 1.38s
2025-04-10 21:06:24.378 | DEBUG    | orz.ppo.trainer:train:89 - train_dataset size: 56878
2025-04-10 21:06:24.378 | DEBUG    | orz.ppo.trainer:train:90 - train_dataset sample: ('A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e. your response format should be: <think> reasoning process here </think>\n\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\nThis is the problem:\n$P(x)$ is a polynomial of degree $3n$ such that\n\\begin{eqnarray*} P(0) = P(3) = \\cdots &=& P(3n) = 2, \\\\ P(1) = P(4) = \\cdots &=& P(3n-2) = 1, \\\\ P(2) = P(5) = \\cdots &=& P(3n-1) = 0, \\quad\\text{ and }\\\\ && P(3n+1) = 730.\\end{eqnarray*}\nDetermine $n$.\nAssistant: <think>', {'prompt': 'A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e. your response format should be: <think> reasoning process here </think>\n\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\nThis is the problem:\n$P(x)$ is a polynomial of degree $3n$ such that\n\\begin{eqnarray*} P(0) = P(3) = \\cdots &=& P(3n) = 2, \\\\ P(1) = P(4) = \\cdots &=& P(3n-2) = 1, \\\\ P(2) = P(5) = \\cdots &=& P(3n-1) = 0, \\quad\\text{ and }\\\\ && P(3n+1) = 730.\\end{eqnarray*}\nDetermine $n$.\nAssistant: <think>', 'answer': 'n = 4', 'target': 'n = 4'})
[36m(LLMActor pid=1829141)[0m INFO 04-10 21:03:14 config.py:478] This model supports multiple tasks: {'generate', 'embed', 'classify', 'reward', 'score'}. Defaulting to 'generate'.
[36m(LLMActor pid=1829141)[0m WARNING 04-10 21:03:14 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(LLMActor pid=1829141)[0m WARNING 04-10 21:03:14 config.py:604] Async output processing is not supported on the current platform type cuda.
[36m(LLMActor pid=1829141)[0m INFO 04-10 21:03:14 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='Qwen/Qwen2.5-7B', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=Qwen/Qwen2.5-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[],"max_capture_size":0}, use_cached_outputs=False, 
[36m(LLMActor pid=1829143)[0m INFO 04-10 21:03:14 config.py:478] This model supports multiple tasks: {'generate', 'score', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.
[36m(LLMActor pid=1829147)[0m INFO 04-10 21:03:14 config.py:478] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting to 'generate'.
[36m(LLMActor pid=1829146)[0m INFO 04-10 21:03:14 config.py:478] This model supports multiple tasks: {'classify', 'score', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
[36m(LLMActor pid=1829139)[0m INFO 04-10 21:03:14 config.py:478] This model supports multiple tasks: {'embed', 'generate', 'reward', 'classify', 'score'}. Defaulting to 'generate'.
[36m(LLMActor pid=1829142)[0m INFO 04-10 21:03:14 config.py:478] This model supports multiple tasks: {'classify', 'generate', 'score', 'embed', 'reward'}. Defaulting to 'generate'.
[36m(LLMActor pid=1829144)[0m INFO 04-10 21:03:14 config.py:478] This model supports multiple tasks: {'reward', 'score', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.
[36m(LLMActor pid=1829145)[0m INFO 04-10 21:03:14 config.py:478] This model supports multiple tasks: {'score', 'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.
[36m(LLMActor pid=1829141)[0m INFO 04-10 21:03:15 selector.py:120] Using Flash Attention backend.
[36m(LLMActor pid=1829141)[0m INFO 04-10 21:03:16 model_runner.py:1092] Starting to load model Qwen/Qwen2.5-7B...
[36m(LLMActor pid=1829141)[0m INFO 04-10 21:03:17 weight_utils.py:243] Using model weights format ['*.safetensors']
[36m(LLMActor pid=1829141)[0m INFO 04-10 21:03:20 model_runner.py:1097] Loading model weights took 14.2716 GB
[36m(LLMActor pid=1829145)[0m WARNING 04-10 21:03:14 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(LLMActor pid=1829145)[0m WARNING 04-10 21:03:14 config.py:604] Async output processing is not supported on the current platform type cuda.[32m [repeated 7x across cluster][0m
[36m(LLMActor pid=1829145)[0m INFO 04-10 21:03:14 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='Qwen/Qwen2.5-7B', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=49, served_model_name=Qwen/Qwen2.5-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"candidate_compile_sizes":[],"compile_sizes":[],"capture_sizes":[],"max_capture_size":0}, use_cached_outputs=False, [32m [repeated 7x across cluster][0m
[36m(LLMActor pid=1829146)[0m INFO 04-10 21:03:15 selector.py:120] Using Flash Attention backend.[32m [repeated 7x across cluster][0m
[36m(LLMActor pid=1829141)[0m INFO 04-10 21:03:22 worker.py:241] Memory profiling takes 1.27 seconds
[36m(LLMActor pid=1829141)[0m INFO 04-10 21:03:22 worker.py:241] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.50) = 39.58GiB
[36m(LLMActor pid=1829141)[0m INFO 04-10 21:03:22 worker.py:241] model weights take 14.27GiB; non_torch_memory takes 0.16GiB; PyTorch activation peak memory takes 1.09GiB; the rest of the memory reserved for KV Cache is 24.05GiB.
[36m(LLMActor pid=1829145)[0m INFO 04-10 21:03:17 model_runner.py:1092] Starting to load model Qwen/Qwen2.5-7B...[32m [repeated 7x across cluster][0m
[36m(LLMActor pid=1829141)[0m INFO 04-10 21:03:22 gpu_executor.py:76] # GPU blocks: 1759, # CPU blocks: 292
[36m(LLMActor pid=1829141)[0m INFO 04-10 21:03:22 gpu_executor.py:80] Maximum concurrency for 8192 tokens per request: 54.97x
[36m(LLMActor pid=1829146)[0m INFO 04-10 21:03:18 weight_utils.py:243] Using model weights format ['*.safetensors'][32m [repeated 7x across cluster][0m
[36m(LLMActor pid=1829141)[0m INFO 04-10 21:03:25 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 4.80 seconds
[36m(LLMActor pid=1829146)[0m INFO 04-10 21:03:23 model_runner.py:1097] Loading model weights took 14.2716 GB[32m [repeated 7x across cluster][0m
[36m(LLMActor pid=1829146)[0m INFO 04-10 21:03:24 worker.py:241] Memory profiling takes 1.27 seconds[32m [repeated 7x across cluster][0m
[36m(LLMActor pid=1829146)[0m INFO 04-10 21:03:24 worker.py:241] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.50) = 39.58GiB[32m [repeated 7x across cluster][0m
[36m(LLMActor pid=1829146)[0m INFO 04-10 21:03:24 worker.py:241] model weights take 14.27GiB; non_torch_memory takes 0.16GiB; PyTorch activation peak memory takes 1.09GiB; the rest of the memory reserved for KV Cache is 24.05GiB.[32m [repeated 7x across cluster][0m
[36m(LLMActor pid=1829146)[0m INFO 04-10 21:03:24 gpu_executor.py:76] # GPU blocks: 1759, # CPU blocks: 292[32m [repeated 7x across cluster][0m
[36m(LLMActor pid=1829146)[0m INFO 04-10 21:03:24 gpu_executor.py:80] Maximum concurrency for 8192 tokens per request: 54.97x[32m [repeated 7x across cluster][0m
[36m(pid=1831007)[0m [2025-04-10 21:04:16,439] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(LLMActor pid=1829146)[0m INFO 04-10 21:03:28 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 5.35 seconds[32m [repeated 7x across cluster][0m
[36m(pid=1831195)[0m [2025-04-10 21:04:33,614] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(pid=1831193)[0m [2025-04-10 21:04:33,795] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(pid=1832131)[0m [2025-04-10 21:04:47,318] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)[32m [repeated 7x across cluster][0m
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:04:56,668] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:04:56,669] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[36m(pid=1832133)[0m [2025-04-10 21:04:47,984] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)[32m [repeated 7x across cluster][0m
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:04:58,083] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[36m(RefRayActorBase pid=1831192)[0m NCCL version 2.21.5+cuda12.4
[36m(RefRayActorBase pid=1832132)[0m [2025-04-10 21:04:56,682] [INFO] [comm.py:652:init_distributed] cdb=None[32m [repeated 7x across cluster][0m
[36m(pid=1833023)[0m [2025-04-10 21:05:02,644] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(pid=1833049)[0m [2025-04-10 21:05:02,707] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:12,580] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 339, num_elems = 7.62B
[36m(RefRayActorBase pid=1832132)[0m [2025-04-10 21:04:58,082] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8[32m [repeated 7x across cluster][0m
[36m(pid=1833077)[0m [2025-04-10 21:05:02,737] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)[32m [repeated 5x across cluster][0m
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,193] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,193] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,202] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,204] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,444] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,445] [INFO] [utils.py:782:see_memory_usage] MA 1.77 GB         Max_MA 3.8 GB         CA 3.88 GB         Max_CA 4 GB 
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,445] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 160.23 GB, percent = 9.0%
[36m(RefRayActorBase pid=1831192)[0m Parameter Offload: Total persistent parameters: 333312 in 141 params
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,748] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,749] [INFO] [utils.py:782:see_memory_usage] MA 1.77 GB         Max_MA 1.77 GB         CA 3.88 GB         Max_CA 4 GB 
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,749] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 160.24 GB, percent = 9.0%
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,752] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,752] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(RefRayActorBase pid=1831192)[0m     "partition_activations": false, 
[36m(RefRayActorBase pid=1831192)[0m     "contiguous_memory_optimization": false, 
[36m(RefRayActorBase pid=1831192)[0m     "cpu_checkpointing": false, 
[36m(RefRayActorBase pid=1831192)[0m     "number_checkpoints": null, 
[36m(RefRayActorBase pid=1831192)[0m     "synchronize_checkpoint_boundary": false, 
[36m(RefRayActorBase pid=1831192)[0m     "profile": false
[36m(RefRayActorBase pid=1831192)[0m }
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(RefRayActorBase pid=1831192)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1831192)[0m     "start_step": null, 
[36m(RefRayActorBase pid=1831192)[0m     "end_step": null, 
[36m(RefRayActorBase pid=1831192)[0m     "metric_path": null, 
[36m(RefRayActorBase pid=1831192)[0m     "arg_mappings": null, 
[36m(RefRayActorBase pid=1831192)[0m     "metric": "throughput", 
[36m(RefRayActorBase pid=1831192)[0m     "model_info": null, 
[36m(RefRayActorBase pid=1831192)[0m     "results_dir": "autotuning_results", 
[36m(RefRayActorBase pid=1831192)[0m     "exps_dir": "autotuning_exps", 
[36m(RefRayActorBase pid=1831192)[0m     "overwrite": true, 
[36m(RefRayActorBase pid=1831192)[0m     "fast": true, 
[36m(RefRayActorBase pid=1831192)[0m     "start_profile_step": 3, 
[36m(RefRayActorBase pid=1831192)[0m     "end_profile_step": 5, 
[36m(RefRayActorBase pid=1831192)[0m     "tuner_type": "gridsearch", 
[36m(RefRayActorBase pid=1831192)[0m     "tuner_early_stopping": 5, 
[36m(RefRayActorBase pid=1831192)[0m     "tuner_num_trials": 50, 
[36m(RefRayActorBase pid=1831192)[0m     "model_info_path": null, 
[36m(RefRayActorBase pid=1831192)[0m     "mp_size": 1, 
[36m(RefRayActorBase pid=1831192)[0m     "max_train_batch_size": null, 
[36m(RefRayActorBase pid=1831192)[0m     "min_train_batch_size": 1, 
[36m(RefRayActorBase pid=1831192)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(RefRayActorBase pid=1831192)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1831192)[0m     "num_tuning_micro_batch_sizes": 3
[36m(RefRayActorBase pid=1831192)[0m }
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f92d3035c90>
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,753] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(RefRayActorBase pid=1831192)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1831192)[0m     "recompute_fwd_factor": 0.0, 
[36m(RefRayActorBase pid=1831192)[0m     "profile_step": 1, 
[36m(RefRayActorBase pid=1831192)[0m     "module_depth": -1, 
[36m(RefRayActorBase pid=1831192)[0m     "top_modules": 1, 
[36m(RefRayActorBase pid=1831192)[0m     "detailed": true, 
[36m(RefRayActorBase pid=1831192)[0m     "output_file": null
[36m(RefRayActorBase pid=1831192)[0m }
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(RefRayActorBase pid=1831192)[0m     "enabled": false, 
[36m(RefRayActorBase pid=1831192)[0m     "persistent_storage_path": null, 
[36m(RefRayActorBase pid=1831192)[0m     "persistent_time_interval": 100, 
[36m(RefRayActorBase pid=1831192)[0m     "num_of_version_in_retention": 2, 
[36m(RefRayActorBase pid=1831192)[0m     "enable_nebula_load": true, 
[36m(RefRayActorBase pid=1831192)[0m     "load_path": null
[36m(RefRayActorBase pid=1831192)[0m }
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,754] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   train_batch_size ............. 8
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   world_size ................... 8
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(RefRayActorBase pid=1831192)[0m [2025-04-10 21:05:17,755] [INFO] [config.py:989:print_user_config]   json = {
[36m(RefRayActorBase pid=1831192)[0m     "steps_per_print": 100, 
[36m(RefRayActorBase pid=1831192)[0m     "zero_optimization": {
[36m(RefRayActorBase pid=1831192)[0m         "stage": 3, 
[36m(RefRayActorBase pid=1831192)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(RefRayActorBase pid=1831192)[0m         "offload_param": {
[36m(RefRayActorBase pid=1831192)[0m             "device": "cpu", 
[36m(RefRayActorBase pid=1831192)[0m             "pin_memory": true
[36m(RefRayActorBase pid=1831192)[0m         }
[36m(RefRayActorBase pid=1831192)[0m     }, 
[36m(RefRayActorBase pid=1831192)[0m     "bf16": {
[36m(RefRayActorBase pid=1831192)[0m         "enabled": true
[36m(RefRayActorBase pid=1831192)[0m     }, 
[36m(RefRayActorBase pid=1831192)[0m     "gradient_clipping": 1.0, 
[36m(RefRayActorBase pid=1831192)[0m     "prescale_gradients": false, 
[36m(RefRayActorBase pid=1831192)[0m     "wall_clock_breakdown": false, 
[36m(RefRayActorBase pid=1831192)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(RefRayActorBase pid=1831192)[0m     "gradient_accumulation_steps": 1
[36m(RefRayActorBase pid=1831192)[0m }
[36m(RefRayActorBase pid=1832134)[0m [2025-04-10 21:05:17,186] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8[32m [repeated 8x across cluster][0m
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:17,865] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:17,865] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[36m(PolicyRayActorBase pid=1831007)[0m NCCL version 2.21.5+cuda12.4
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:32,986] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 339, num_elems = 7.62B
[36m(PolicyRayActorBase pid=1831196)[0m [2025-04-10 21:05:19,213] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8[32m [repeated 8x across cluster][0m
[36m(PolicyRayActorBase pid=1831196)[0m [2025-04-10 21:05:17,875] [INFO] [comm.py:652:init_distributed] cdb=None[32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1831195)[0m ninja: no work to do.
[36m(PolicyRayActorBase pid=1831195)[0m Time to load fused_adam op: 2.879330635070801 seconds
[36m(PolicyRayActorBase pid=1831195)[0m [2025-04-10 21:05:40,612] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:40,648] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:40,648] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:40,649] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:40,658] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:40,659] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:40,659] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:40,673] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:40,673] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:40,673] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:40,673] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:41,044] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:41,045] [INFO] [utils.py:782:see_memory_usage] MA 1.77 GB         Max_MA 3.8 GB         CA 3.88 GB         Max_CA 4 GB 
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:41,046] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 163.25 GB, percent = 9.2%
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:41,047] [INFO] [stage3.py:167:__init__] Reduce bucket size 500000000
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:41,047] [INFO] [stage3.py:168:__init__] Prefetch bucket size 50000000
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:41,295] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:41,296] [INFO] [utils.py:782:see_memory_usage] MA 1.77 GB         Max_MA 1.77 GB         CA 3.88 GB         Max_CA 4 GB 
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:41,296] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 163.29 GB, percent = 9.2%
[36m(PolicyRayActorBase pid=1831007)[0m Parameter Offload: Total persistent parameters: 333312 in 141 params
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:41,525] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:41,526] [INFO] [utils.py:782:see_memory_usage] MA 1.77 GB         Max_MA 1.77 GB         CA 3.88 GB         Max_CA 4 GB 
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:41,526] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 163.29 GB, percent = 9.2%
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:41,740] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:41,741] [INFO] [utils.py:782:see_memory_usage] MA 1.77 GB         Max_MA 1.77 GB         CA 3.88 GB         Max_CA 4 GB 
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:41,741] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 163.29 GB, percent = 9.2%
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:43,192] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 2
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:43,193] [INFO] [utils.py:782:see_memory_usage] MA 1.77 GB         Max_MA 1.77 GB         CA 1.78 GB         Max_CA 4 GB 
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:43,193] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 166.9 GB, percent = 9.4%
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:43,407] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:43,408] [INFO] [utils.py:782:see_memory_usage] MA 1.77 GB         Max_MA 1.77 GB         CA 1.78 GB         Max_CA 2 GB 
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:43,409] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 163.39 GB, percent = 9.2%
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:43,636] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:43,636] [INFO] [utils.py:782:see_memory_usage] MA 5.32 GB         Max_MA 7.09 GB         CA 7.1 GB         Max_CA 7 GB 
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:43,637] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 163.39 GB, percent = 9.2%
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:43,849] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:43,850] [INFO] [utils.py:782:see_memory_usage] MA 5.32 GB         Max_MA 5.32 GB         CA 7.1 GB         Max_CA 7 GB 
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:43,850] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 163.39 GB, percent = 9.2%
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,068] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,069] [INFO] [utils.py:782:see_memory_usage] MA 5.32 GB         Max_MA 8.87 GB         CA 10.64 GB         Max_CA 11 GB 
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,069] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 163.38 GB, percent = 9.2%
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,070] [INFO] [stage3.py:525:_setup_for_real_optimizer] optimizer state initialized
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,629] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,630] [INFO] [utils.py:782:see_memory_usage] MA 9.8 GB         Max_MA 11.83 GB         CA 12.67 GB         Max_CA 13 GB 
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,630] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 163.38 GB, percent = 9.2%
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,630] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,630] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,630] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f10f2d029b0>
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,631] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,631] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,632] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(PolicyRayActorBase pid=1831007)[0m     "partition_activations": false, 
[36m(PolicyRayActorBase pid=1831007)[0m     "contiguous_memory_optimization": false, 
[36m(PolicyRayActorBase pid=1831007)[0m     "cpu_checkpointing": false, 
[36m(PolicyRayActorBase pid=1831007)[0m     "number_checkpoints": null, 
[36m(PolicyRayActorBase pid=1831007)[0m     "synchronize_checkpoint_boundary": false, 
[36m(PolicyRayActorBase pid=1831007)[0m     "profile": false
[36m(PolicyRayActorBase pid=1831007)[0m }
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,632] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,633] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,633] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,633] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(PolicyRayActorBase pid=1831007)[0m     "enabled": false, 
[36m(PolicyRayActorBase pid=1831007)[0m     "start_step": null, 
[36m(PolicyRayActorBase pid=1831007)[0m     "end_step": null, 
[36m(PolicyRayActorBase pid=1831007)[0m     "metric_path": null, 
[36m(PolicyRayActorBase pid=1831007)[0m     "arg_mappings": null, 
[36m(PolicyRayActorBase pid=1831007)[0m     "metric": "throughput", 
[36m(PolicyRayActorBase pid=1831007)[0m     "model_info": null, 
[36m(PolicyRayActorBase pid=1831007)[0m     "results_dir": "autotuning_results", 
[36m(PolicyRayActorBase pid=1831007)[0m     "exps_dir": "autotuning_exps", 
[36m(PolicyRayActorBase pid=1831007)[0m     "overwrite": true, 
[36m(PolicyRayActorBase pid=1831007)[0m     "fast": true, 
[36m(PolicyRayActorBase pid=1831007)[0m     "start_profile_step": 3, 
[36m(PolicyRayActorBase pid=1831007)[0m     "end_profile_step": 5, 
[36m(PolicyRayActorBase pid=1831007)[0m     "tuner_type": "gridsearch", 
[36m(PolicyRayActorBase pid=1831007)[0m     "tuner_early_stopping": 5, 
[36m(PolicyRayActorBase pid=1831007)[0m     "tuner_num_trials": 50, 
[36m(PolicyRayActorBase pid=1831007)[0m     "model_info_path": null, 
[36m(PolicyRayActorBase pid=1831007)[0m     "mp_size": 1, 
[36m(PolicyRayActorBase pid=1831007)[0m     "max_train_batch_size": null, 
[36m(PolicyRayActorBase pid=1831007)[0m     "min_train_batch_size": 1, 
[36m(PolicyRayActorBase pid=1831007)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(PolicyRayActorBase pid=1831007)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(PolicyRayActorBase pid=1831007)[0m     "num_tuning_micro_batch_sizes": 3
[36m(PolicyRayActorBase pid=1831007)[0m }
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,633] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,633] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,633] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,633] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,633] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,633] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f10f2b21ab0>
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,633] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(PolicyRayActorBase pid=1831007)[0m     "enabled": false, 
[36m(PolicyRayActorBase pid=1831007)[0m     "recompute_fwd_factor": 0.0, 
[36m(PolicyRayActorBase pid=1831007)[0m     "profile_step": 1, 
[36m(PolicyRayActorBase pid=1831007)[0m     "module_depth": -1, 
[36m(PolicyRayActorBase pid=1831007)[0m     "top_modules": 1, 
[36m(PolicyRayActorBase pid=1831007)[0m     "detailed": true, 
[36m(PolicyRayActorBase pid=1831007)[0m     "output_file": null
[36m(PolicyRayActorBase pid=1831007)[0m }
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   grad_accum_dtype ............. fp32
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,634] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(PolicyRayActorBase pid=1831007)[0m     "enabled": false, 
[36m(PolicyRayActorBase pid=1831007)[0m     "persistent_storage_path": null, 
[36m(PolicyRayActorBase pid=1831007)[0m     "persistent_time_interval": 100, 
[36m(PolicyRayActorBase pid=1831007)[0m     "num_of_version_in_retention": 2, 
[36m(PolicyRayActorBase pid=1831007)[0m     "enable_nebula_load": true, 
[36m(PolicyRayActorBase pid=1831007)[0m     "load_path": null
[36m(PolicyRayActorBase pid=1831007)[0m }
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   train_batch_size ............. 8
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   world_size ................... 8
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,635] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,636] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,636] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,636] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,636] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(PolicyRayActorBase pid=1831007)[0m [2025-04-10 21:05:44,636] [INFO] [config.py:989:print_user_config]   json = {
[36m(PolicyRayActorBase pid=1831007)[0m     "steps_per_print": 100, 
[36m(PolicyRayActorBase pid=1831007)[0m     "zero_optimization": {
[36m(PolicyRayActorBase pid=1831007)[0m         "stage": 3, 
[36m(PolicyRayActorBase pid=1831007)[0m         "offload_param": {
[36m(PolicyRayActorBase pid=1831007)[0m             "device": "none"
[36m(PolicyRayActorBase pid=1831007)[0m         }, 
[36m(PolicyRayActorBase pid=1831007)[0m         "offload_optimizer": {
[36m(PolicyRayActorBase pid=1831007)[0m             "device": "none", 
[36m(PolicyRayActorBase pid=1831007)[0m             "pin_memory": true
[36m(PolicyRayActorBase pid=1831007)[0m         }, 
[36m(PolicyRayActorBase pid=1831007)[0m         "sub_group_size": "auto", 
[36m(PolicyRayActorBase pid=1831007)[0m         "stage3_max_live_parameters": "auto", 
[36m(PolicyRayActorBase pid=1831007)[0m         "stage3_max_reuse_distance": "auto", 
[36m(PolicyRayActorBase pid=1831007)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(PolicyRayActorBase pid=1831007)[0m         "stage3_prefetch_bucket_size": "auto", 
[36m(PolicyRayActorBase pid=1831007)[0m         "reduce_bucket_size": "auto", 
[36m(PolicyRayActorBase pid=1831007)[0m         "zero_hpz_partition_size": 1, 
[36m(PolicyRayActorBase pid=1831007)[0m         "zero_quantized_weights": false, 
[36m(PolicyRayActorBase pid=1831007)[0m         "zero_quantized_gradients": false
[36m(PolicyRayActorBase pid=1831007)[0m     }, 
[36m(PolicyRayActorBase pid=1831007)[0m     "bf16": {
[36m(PolicyRayActorBase pid=1831007)[0m         "enabled": true
[36m(PolicyRayActorBase pid=1831007)[0m     }, 
[36m(PolicyRayActorBase pid=1831007)[0m     "gradient_clipping": 1.0, 
[36m(PolicyRayActorBase pid=1831007)[0m     "prescale_gradients": false, 
[36m(PolicyRayActorBase pid=1831007)[0m     "wall_clock_breakdown": false, 
[36m(PolicyRayActorBase pid=1831007)[0m     "data_types": {
[36m(PolicyRayActorBase pid=1831007)[0m         "grad_accum_dtype": "fp32"
[36m(PolicyRayActorBase pid=1831007)[0m     }, 
[36m(PolicyRayActorBase pid=1831007)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(PolicyRayActorBase pid=1831007)[0m     "gradient_accumulation_steps": 1
[36m(PolicyRayActorBase pid=1831007)[0m }
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:05:49,248] [INFO] [comm.py:652:init_distributed] cdb=None
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:05:49,248] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[36m(PolicyRayActorBase pid=1831196)[0m Time to load fused_adam op: 2.918632984161377 seconds[32m [repeated 7x across cluster][0m
[36m(PolicyRayActorBase pid=1831196)[0m [2025-04-10 21:05:40,677] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8[32m [repeated 6x across cluster][0m
[36m(CriticRayActorBase pid=1832137)[0m NCCL version 2.21.5+cuda12.4
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:06,524] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 339, num_elems = 7.07B
[36m(CriticRayActorBase pid=1833077)[0m [2025-04-10 21:05:49,255] [INFO] [comm.py:652:init_distributed] cdb=None[32m [repeated 7x across cluster][0m
[36m(CriticRayActorBase pid=1833077)[0m [2025-04-10 21:05:52,947] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8[32m [repeated 8x across cluster][0m
[36m(CriticRayActorBase pid=1832137)[0m Time to load fused_adam op: 0.10405874252319336 seconds
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:10,721] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:10,721] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:10,729] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:10,730] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:10,730] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:10,744] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:10,744] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:10,744] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:10,744] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[36m(CriticRayActorBase pid=1833049)[0m ninja: no work to do.
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:10,938] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:10,939] [INFO] [utils.py:782:see_memory_usage] MA 1.65 GB         Max_MA 3.68 GB         CA 3.88 GB         Max_CA 4 GB 
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:10,939] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 199.49 GB, percent = 11.3%
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:10,941] [INFO] [stage3.py:167:__init__] Reduce bucket size 500000000
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:10,941] [INFO] [stage3.py:168:__init__] Prefetch bucket size 50000000
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:11,134] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:11,135] [INFO] [utils.py:782:see_memory_usage] MA 1.65 GB         Max_MA 1.65 GB         CA 3.88 GB         Max_CA 4 GB 
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:11,135] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 199.49 GB, percent = 11.3%
[36m(CriticRayActorBase pid=1832137)[0m Parameter Offload: Total persistent parameters: 336896 in 142 params
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:11,346] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:11,347] [INFO] [utils.py:782:see_memory_usage] MA 1.65 GB         Max_MA 1.65 GB         CA 3.88 GB         Max_CA 4 GB 
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:11,347] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 199.51 GB, percent = 11.3%
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:11,543] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:11,544] [INFO] [utils.py:782:see_memory_usage] MA 1.65 GB         Max_MA 1.65 GB         CA 3.88 GB         Max_CA 4 GB 
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:11,544] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 199.51 GB, percent = 11.3%
[36m(CriticRayActorBase pid=1833077)[0m [2025-04-10 21:06:10,722] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8[32m [repeated 8x across cluster][0m
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:12,601] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 2
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:12,602] [INFO] [utils.py:782:see_memory_usage] MA 1.65 GB         Max_MA 1.65 GB         CA 1.65 GB         Max_CA 4 GB 
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:12,602] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 209.43 GB, percent = 11.8%
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:12,861] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:12,862] [INFO] [utils.py:782:see_memory_usage] MA 1.65 GB         Max_MA 1.65 GB         CA 1.65 GB         Max_CA 2 GB 
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:12,862] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 202.9 GB, percent = 11.5%
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:13,077] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:13,078] [INFO] [utils.py:782:see_memory_usage] MA 4.94 GB         Max_MA 6.59 GB         CA 6.59 GB         Max_CA 7 GB 
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:13,078] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 201.21 GB, percent = 11.4%
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:13,502] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:13,503] [INFO] [utils.py:782:see_memory_usage] MA 4.94 GB         Max_MA 4.94 GB         CA 6.59 GB         Max_CA 7 GB 
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:13,503] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 199.57 GB, percent = 11.3%
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:13,755] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:13,756] [INFO] [utils.py:782:see_memory_usage] MA 4.94 GB         Max_MA 8.23 GB         CA 9.88 GB         Max_CA 10 GB 
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:13,756] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 199.57 GB, percent = 11.3%
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:13,757] [INFO] [stage3.py:525:_setup_for_real_optimizer] optimizer state initialized
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,299] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,300] [INFO] [utils.py:782:see_memory_usage] MA 9.16 GB         Max_MA 11.2 GB         CA 11.91 GB         Max_CA 12 GB 
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,300] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 199.55 GB, percent = 11.3%
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,301] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,301] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,301] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f5f7c3d03d0>
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,301] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,302] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,302] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
[36m(CriticRayActorBase pid=1832137)[0m     "partition_activations": false, 
[36m(CriticRayActorBase pid=1832137)[0m     "contiguous_memory_optimization": false, 
[36m(CriticRayActorBase pid=1832137)[0m     "cpu_checkpointing": false, 
[36m(CriticRayActorBase pid=1832137)[0m     "number_checkpoints": null, 
[36m(CriticRayActorBase pid=1832137)[0m     "synchronize_checkpoint_boundary": false, 
[36m(CriticRayActorBase pid=1832137)[0m     "profile": false
[36m(CriticRayActorBase pid=1832137)[0m }
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   amp_enabled .................. False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   amp_params ................... False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   autotuning_config ............ {
[36m(CriticRayActorBase pid=1832137)[0m     "enabled": false, 
[36m(CriticRayActorBase pid=1832137)[0m     "start_step": null, 
[36m(CriticRayActorBase pid=1832137)[0m     "end_step": null, 
[36m(CriticRayActorBase pid=1832137)[0m     "metric_path": null, 
[36m(CriticRayActorBase pid=1832137)[0m     "arg_mappings": null, 
[36m(CriticRayActorBase pid=1832137)[0m     "metric": "throughput", 
[36m(CriticRayActorBase pid=1832137)[0m     "model_info": null, 
[36m(CriticRayActorBase pid=1832137)[0m     "results_dir": "autotuning_results", 
[36m(CriticRayActorBase pid=1832137)[0m     "exps_dir": "autotuning_exps", 
[36m(CriticRayActorBase pid=1832137)[0m     "overwrite": true, 
[36m(CriticRayActorBase pid=1832137)[0m     "fast": true, 
[36m(CriticRayActorBase pid=1832137)[0m     "start_profile_step": 3, 
[36m(CriticRayActorBase pid=1832137)[0m     "end_profile_step": 5, 
[36m(CriticRayActorBase pid=1832137)[0m     "tuner_type": "gridsearch", 
[36m(CriticRayActorBase pid=1832137)[0m     "tuner_early_stopping": 5, 
[36m(CriticRayActorBase pid=1832137)[0m     "tuner_num_trials": 50, 
[36m(CriticRayActorBase pid=1832137)[0m     "model_info_path": null, 
[36m(CriticRayActorBase pid=1832137)[0m     "mp_size": 1, 
[36m(CriticRayActorBase pid=1832137)[0m     "max_train_batch_size": null, 
[36m(CriticRayActorBase pid=1832137)[0m     "min_train_batch_size": 1, 
[36m(CriticRayActorBase pid=1832137)[0m     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[36m(CriticRayActorBase pid=1832137)[0m     "min_train_micro_batch_size_per_gpu": 1, 
[36m(CriticRayActorBase pid=1832137)[0m     "num_tuning_micro_batch_sizes": 3
[36m(CriticRayActorBase pid=1832137)[0m }
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5f6c89abc0>
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   communication_data_type ...... None
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   disable_allgather ............ False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   dump_state ................... False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,303] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
[36m(CriticRayActorBase pid=1832137)[0m     "enabled": false, 
[36m(CriticRayActorBase pid=1832137)[0m     "recompute_fwd_factor": 0.0, 
[36m(CriticRayActorBase pid=1832137)[0m     "profile_step": 1, 
[36m(CriticRayActorBase pid=1832137)[0m     "module_depth": -1, 
[36m(CriticRayActorBase pid=1832137)[0m     "top_modules": 1, 
[36m(CriticRayActorBase pid=1832137)[0m     "detailed": true, 
[36m(CriticRayActorBase pid=1832137)[0m     "output_file": null
[36m(CriticRayActorBase pid=1832137)[0m }
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   global_rank .................. 0
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   grad_accum_dtype ............. fp32
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   nebula_config ................ {
[36m(CriticRayActorBase pid=1832137)[0m     "enabled": false, 
[36m(CriticRayActorBase pid=1832137)[0m     "persistent_storage_path": null, 
[36m(CriticRayActorBase pid=1832137)[0m     "persistent_time_interval": 100, 
[36m(CriticRayActorBase pid=1832137)[0m     "num_of_version_in_retention": 2, 
[36m(CriticRayActorBase pid=1832137)[0m     "enable_nebula_load": true, 
[36m(CriticRayActorBase pid=1832137)[0m     "load_path": null
[36m(CriticRayActorBase pid=1832137)[0m }
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   optimizer_name ............... None
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   optimizer_params ............. None
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   pld_enabled .................. False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   pld_params ................... False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   scheduler_name ............... None
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   scheduler_params ............. None
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,304] [INFO] [config.py:1003:print]   sparse_attention ............. None
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,305] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,305] [INFO] [config.py:1003:print]   steps_per_print .............. 100
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,305] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,305] [INFO] [config.py:1003:print]   train_batch_size ............. 8
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,305] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,305] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,305] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,305] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,305] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,305] [INFO] [config.py:1003:print]   world_size ................... 8
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,305] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,305] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,305] [INFO] [config.py:1003:print]   zero_enabled ................. True
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,305] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,305] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[36m(CriticRayActorBase pid=1832137)[0m [2025-04-10 21:06:14,305] [INFO] [config.py:989:print_user_config]   json = {
[36m(CriticRayActorBase pid=1832137)[0m     "steps_per_print": 100, 
[36m(CriticRayActorBase pid=1832137)[0m     "zero_optimization": {
[36m(CriticRayActorBase pid=1832137)[0m         "stage": 3, 
[36m(CriticRayActorBase pid=1832137)[0m         "offload_param": {
[36m(CriticRayActorBase pid=1832137)[0m             "device": "none"
[36m(CriticRayActorBase pid=1832137)[0m         }, 
[36m(CriticRayActorBase pid=1832137)[0m         "offload_optimizer": {
[36m(CriticRayActorBase pid=1832137)[0m             "device": "none", 
[36m(CriticRayActorBase pid=1832137)[0m             "pin_memory": true
[36m(CriticRayActorBase pid=1832137)[0m         }, 
[36m(CriticRayActorBase pid=1832137)[0m         "sub_group_size": "auto", 
[36m(CriticRayActorBase pid=1832137)[0m         "stage3_max_live_parameters": "auto", 
[36m(CriticRayActorBase pid=1832137)[0m         "stage3_max_reuse_distance": "auto", 
[36m(CriticRayActorBase pid=1832137)[0m         "stage3_param_persistence_threshold": "auto", 
[36m(CriticRayActorBase pid=1832137)[0m         "stage3_prefetch_bucket_size": "auto", 
[36m(CriticRayActorBase pid=1832137)[0m         "reduce_bucket_size": "auto", 
[36m(CriticRayActorBase pid=1832137)[0m         "zero_hpz_partition_size": 1, 
[36m(CriticRayActorBase pid=1832137)[0m         "zero_quantized_weights": false, 
[36m(CriticRayActorBase pid=1832137)[0m         "zero_quantized_gradients": false
[36m(CriticRayActorBase pid=1832137)[0m     }, 
[36m(CriticRayActorBase pid=1832137)[0m     "bf16": {
[36m(CriticRayActorBase pid=1832137)[0m         "enabled": true
[36m(CriticRayActorBase pid=1832137)[0m     }, 
[36m(CriticRayActorBase pid=1832137)[0m     "gradient_clipping": 1.0, 
[36m(CriticRayActorBase pid=1832137)[0m     "prescale_gradients": false, 
[36m(CriticRayActorBase pid=1832137)[0m     "wall_clock_breakdown": false, 
[36m(CriticRayActorBase pid=1832137)[0m     "data_types": {
[36m(CriticRayActorBase pid=1832137)[0m         "grad_accum_dtype": "fp32"
[36m(CriticRayActorBase pid=1832137)[0m     }, 
[36m(CriticRayActorBase pid=1832137)[0m     "train_micro_batch_size_per_gpu": 1, 
[36m(CriticRayActorBase pid=1832137)[0m     "gradient_accumulation_steps": 1
[36m(CriticRayActorBase pid=1832137)[0m }
[36m(PolicyRayActorBase pid=1831007)[0m WARNING:using --vllm_sync_backend=gloo for vLLM version > 0.4.2 (or export NCCL_P2P_DISABLE=1)
[36m(CriticRayActorBase pid=1833077)[0m Time to load fused_adam op: 0.10504674911499023 seconds[32m [repeated 7x across cluster][0m
[36m(LLMActor pid=1829146)[0m init_process_group: master_address=10.4.68.151, master_port=48333,  rank=6, world_size=9, group_name=openrlhf
[36m(PolicyRayActorBase pid=1831007)[0m Broadcast actor weights to vllm engines done
Episode [1/20]:   0%|          | 0/7110 [00:00<?, ?it/s]2025-04-10 21:06:24.591 | INFO     | __main__:eval:2635 - Start evaluating on val set
[36m(LLMActor pid=1829143)[0m Processed prompts:   0%|          | 0/91 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
[36m(CriticRayActorBase pid=1833077)[0m Some weights of CriticModel were not initialized from the model checkpoint at Qwen/Qwen2.5-7B and are newly initialized: ['value_head.weight'][32m [repeated 7x across cluster][0m
[36m(CriticRayActorBase pid=1833077)[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.[32m [repeated 7x across cluster][0m
[36m(CriticRayActorBase pid=1833077)[0m 2025-04-10 21:06:10.583 | INFO     | orz.ppo.models:get_llm_for_sequence_regression:572 - initialize value_head for ZeRO-3 reward model training.[32m [repeated 7x across cluster][0m
[36m(CriticRayActorBase pid=1833032)[0m Using /home/aiscuser/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...[32m [repeated 7x across cluster][0m
[36m(CriticRayActorBase pid=1833032)[0m Loading extension module fused_adam...[32m [repeated 7x across cluster][0m
[36m(CriticRayActorBase pid=1833032)[0m set_mempolicy: Operation not permitted[32m [repeated 7x across cluster][0m
[36m(LLMActor pid=1829145)[0m Processed prompts:   1%|          | 1/91 [00:01<02:49,  1.88s/it, est. speed input: 158.71 toks/s, output: 2.12 toks/s]
[36m(LLMActor pid=1829145)[0m Processed prompts:   0%|          | 0/91 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][32m [repeated 7x across cluster][0m
[36m(LLMActor pid=1829143)[0m Processed prompts:   9%|â–‰         | 8/91 [00:06<00:50,  1.64it/s, est. speed input: 191.44 toks/s, output: 109.23 toks/s][32m [repeated 44x across cluster][0m
[36m(LLMActor pid=1829141)[0m Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 43/91 [00:12<00:07,  6.10it/s, est. speed input: 663.50 toks/s, output: 647.07 toks/s][32m [repeated 100x across cluster][0m
[36m(LLMActor pid=1829141)[0m Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 61/91 [00:17<00:11,  2.53it/s, est. speed input: 651.95 toks/s, output: 807.19 toks/s][32m [repeated 124x across cluster][0m
[36m(LLMActor pid=1829141)[0m Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 75/91 [00:22<00:09,  1.60it/s, est. speed input: 621.57 toks/s, output: 916.40 toks/s][32m [repeated 116x across cluster][0m
[36m(LLMActor pid=1829144)[0m Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 70/91 [00:23<00:06,  3.22it/s, est. speed input: 577.62 toks/s, output: 897.17 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 73/91 [00:23<00:02,  6.31it/s, est. speed input: 599.42 toks/s, output: 966.96 toks/s]
[36m(LLMActor pid=1829145)[0m Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 77/91 [00:27<00:05,  2.71it/s, est. speed input: 870.85 toks/s, output: 1078.64 toks/s][32m [repeated 70x across cluster][0m
[36m(LLMActor pid=1829144)[0m Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 84/91 [00:29<00:05,  1.37it/s, est. speed input: 548.50 toks/s, output: 1003.06 toks/s]
[36m(LLMActor pid=1829139)[0m Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 81/91 [00:32<00:10,  1.02s/it, est. speed input: 525.39 toks/s, output: 904.19 toks/s][32m [repeated 39x across cluster][0m
[36m(LLMActor pid=1829142)[0m Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 87/91 [00:34<00:04,  1.16s/it, est. speed input: 505.80 toks/s, output: 942.47 toks/s][32m [repeated 28x across cluster][0m
[36m(LLMActor pid=1829146)[0m Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 83/91 [00:33<00:03,  2.09it/s, est. speed input: 581.94 toks/s, output: 1035.30 toks/s][32m [repeated 6x across cluster][0m
[36m(LLMActor pid=1829147)[0m Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:39<00:00,  1.44s/it, est. speed input: 868.01 toks/s, output: 980.96 toks/s] Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:39<00:00,  2.31it/s, est. speed input: 868.01 toks/s, output: 980.96 toks/s]
[36m(LLMActor pid=1829146)[0m Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 87/91 [00:40<00:04,  1.25s/it, est. speed input: 509.56 toks/s, output: 963.51 toks/s][32m [repeated 15x across cluster][0m
[36m(LLMActor pid=1829141)[0m Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:43<00:00,  2.34s/it, est. speed input: 390.29 toks/s, output: 783.50 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:43<00:00,  2.08it/s, est. speed input: 390.29 toks/s, output: 783.50 toks/s]
[36m(LLMActor pid=1829143)[0m Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:45<00:00,  1.49s/it, est. speed input: 410.97 toks/s, output: 855.59 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:45<00:00,  2.02it/s, est. speed input: 410.97 toks/s, output: 855.59 toks/s]
[36m(LLMActor pid=1829139)[0m Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 89/91 [00:45<00:06,  3.05s/it, est. speed input: 410.19 toks/s, output: 811.10 toks/s][32m [repeated 5x across cluster][0m
[36m(LLMActor pid=1829144)[0m Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:46<00:00,  3.39s/it, est. speed input: 386.23 toks/s, output: 790.36 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:46<00:00,  1.97it/s, est. speed input: 386.23 toks/s, output: 790.36 toks/s]
[36m(LLMActor pid=1829142)[0m Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 90/91 [00:51<00:04,  4.15s/it, est. speed input: 353.97 toks/s, output: 706.93 toks/s][32m [repeated 4x across cluster][0m
[36m(LLMActor pid=1829145)[0m Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:53<00:00,  5.88s/it, est. speed input: 537.34 toks/s, output: 761.68 toks/s] Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:53<00:00,  1.71it/s, est. speed input: 537.34 toks/s, output: 761.68 toks/s]
[36m(LLMActor pid=1829146)[0m Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 90/91 [00:54<00:03,  3.70s/it, est. speed input: 391.73 toks/s, output: 782.23 toks/s]
[36m(LLMActor pid=1829139)[0m Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:59<00:00,  5.34s/it, est. speed input: 319.73 toks/s, output: 666.38 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [00:59<00:00,  1.52it/s, est. speed input: 319.73 toks/s, output: 666.38 toks/s]
[36m(LLMActor pid=1829146)[0m Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:04<00:00,  5.59s/it, est. speed input: 334.75 toks/s, output: 689.65 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [01:04<00:00,  1.40it/s, est. speed input: 334.75 toks/s, output: 689.65 toks/s]
[36m(LLMActor pid=1829142)[0m Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [02:11<00:00, 26.96s/it, est. speed input: 139.70 toks/s, output: 307.94 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91/91 [02:11<00:00,  1.45s/it, est. speed input: 139.70 toks/s, output: 307.94 toks/s]
2025-04-10 21:08:38.935 | INFO     | __main__:eval:2748 - math500/response_len_in_char: 1189.6080,math500/accuracy: 0.2240,aime2024/response_len_in_char: 1678.8000,aime2024/accuracy: 0.0000,gpqa_diamond/response_len_in_char: 1591.3535,gpqa_diamond/accuracy: 0.0556,eval_accuracy: 0.0932
2025-04-10 21:08:38.942 | DEBUG    | orz.ppo.trainer:make_experience:287 - starting make_experience
2025-04-10 21:08:38.942 | DEBUG    | orz.ppo.trainer:make_experience:288 - (before query branch) prompt batch size (bs): len(all_inputs): 8
2025-04-10 21:08:38.942 | DEBUG    | orz.ppo.trainer:make_experience:531 - 
--- Policy Branch ---

2025-04-10 21:08:38.942 | DEBUG    | orz.ppo.trainer:make_experience:532 - (before policy branch) prompt batch size (bs): len(all_inputs): 8
2025-04-10 21:08:38.942 | DEBUG    | orz.ppo.trainer:make_experience:533 - (before policy branch) sample all_inputs[0]: ('A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e. your response format should be: <think> reasoning process here </think>\n\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\nThis is the problem:\nClara, a literature major fascinated by the artistic movements of the Roaring Twenties, decides to visit an art exhibition that features famous works from this era. The exhibition has 5 rooms, each dedicated to a different aspect of the 1920s, such as jazz music, art deco, surrealism, literature, and fashion. Each room contains exactly 12 pieces of art.\n\nClara spends an average of 15 minutes viewing each piece in the jazz music room. In the art deco room, she spends 10 minutes per piece. In the surrealism room, she spends 20 minutes per piece. In the literature room, she spends 8 minutes per piece, and in the fashion room, she spends 5 minutes per piece.\n\nHow many total minutes does Clara spend viewing all the pieces of art in the exhibition?\nAssistant: <think>', {'prompt': 'A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e. your response format should be: <think> reasoning process here </think>\n\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\nThis is the problem:\nClara, a literature major fascinated by the artistic movements of the Roaring Twenties, decides to visit an art exhibition that features famous works from this era. The exhibition has 5 rooms, each dedicated to a different aspect of the 1920s, such as jazz music, art deco, surrealism, literature, and fashion. Each room contains exactly 12 pieces of art.\n\nClara spends an average of 15 minutes viewing each piece in the jazz music room. In the art deco room, she spends 10 minutes per piece. In the surrealism room, she spends 20 minutes per piece. In the literature room, she spends 8 minutes per piece, and in the fashion room, she spends 5 minutes per piece.\n\nHow many total minutes does Clara spend viewing all the pieces of art in the exhibition?\nAssistant: <think>', 'answer': '696', 'target': '696'})
2025-04-10 21:08:38.942 | INFO     | orz.ppo.trainer:make_experience:564 - start generation
2025-04-10 21:09:19.394 | INFO     | orz.ppo.trainer:make_experience:567 - generate local rollout batch done
2025-04-10 21:09:19.395 | INFO     | orz.ppo.trainer:make_experience:552 - Generate sequences via vllm engines, time cost: 40.45s
2025-04-10 21:09:19.395 | DEBUG    | orz.ppo.trainer:make_experience:576 - 

INSPECT POLICY BRANCH PROMPTS AND RESPONSES
2025-04-10 21:09:19.395 | DEBUG    | orz.ppo.trainer:make_experience:577 - (policy branch) sample prompt, all_inputs[0]: ('A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e. your response format should be: <think> reasoning process here </think>\n\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\nThis is the problem:\nClara, a literature major fascinated by the artistic movements of the Roaring Twenties, decides to visit an art exhibition that features famous works from this era. The exhibition has 5 rooms, each dedicated to a different aspect of the 1920s, such as jazz music, art deco, surrealism, literature, and fashion. Each room contains exactly 12 pieces of art.\n\nClara spends an average of 15 minutes viewing each piece in the jazz music room. In the art deco room, she spends 10 minutes per piece. In the surrealism room, she spends 20 minutes per piece. In the literature room, she spends 8 minutes per piece, and in the fashion room, she spends 5 minutes per piece.\n\nHow many total minutes does Clara spend viewing all the pieces of art in the exhibition?\nAssistant: <think>', {'prompt': 'A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e. your response format should be: <think> reasoning process here </think>\n\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\nThis is the problem:\nClara, a literature major fascinated by the artistic movements of the Roaring Twenties, decides to visit an art exhibition that features famous works from this era. The exhibition has 5 rooms, each dedicated to a different aspect of the 1920s, such as jazz music, art deco, surrealism, literature, and fashion. Each room contains exactly 12 pieces of art.\n\nClara spends an average of 15 minutes viewing each piece in the jazz music room. In the art deco room, she spends 10 minutes per piece. In the surrealism room, she spends 20 minutes per piece. In the literature room, she spends 8 minutes per piece, and in the fashion room, she spends 5 minutes per piece.\n\nHow many total minutes does Clara spend viewing all the pieces of art in the exhibition?\nAssistant: <think>', 'answer': '696', 'target': '696'})

2025-04-10 21:09:19.395 | DEBUG    | orz.ppo.trainer:make_experience:578 - (policy branch) sample response, outputs[0]: {'response': ' First, I will calculate the total number of pieces of art she views in each room by multiplying the number of pieces per room by the number of rooms. Then, I will calculate the total time spent in each room by multiplying the average time spent per piece by the total number of pieces in that room. Finally, I will add up the total time spent in all the rooms to get the final answer.\n\nFour times the number of rooms is: 4 * 12 pieces/room = 48 pieces.\n\nIn the jazz music room, she spends 15 minutes per piece for a total of: 48 pieces * 15 minutes/piece = 720 minutes.\n\nIn the art deco room, she spends 10 minutes per piece for a total of: 48 pieces * 10 minutes/piece = 480 minutes.\n\nIn the surrealism room, she spends 20 minutes per piece for a total of: 48 pieces * 20 minutes/piece = 960 minutes.\n\nIn the literature room, she spends 8 minutes per piece for a total of: 48 pieces * 8 minutes/piece = 384 minutes.\n\nIn the fashion room, she spends 5 minutes per piece for a total of: 48 pieces * 5 minutes/piece = 240 minutes.\n\nThe total time Clara spends in all the rooms is 720 minutes + 480 minutes + 960 minutes + 384 minutes + 240 minutes = 2764 minutes.\n\n</think>\n<answer> 2764 minutes </answer>', 'iscorrect': False, 'stop_reason': 'stop', 'final_answer': '2764 minutes', 'extra': {'prompt': 'A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e. your response format should be: <think> reasoning process here </think>\n\n<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.\nThis is the problem:\nClara, a literature major fascinated by the artistic movements of the Roaring Twenties, decides to visit an art exhibition that features famous works from this era. The exhibition has 5 rooms, each dedicated to a different aspect of the 1920s, such as jazz music, art deco, surrealism, literature, and fashion. Each room contains exactly 12 pieces of art.\n\nClara spends an average of 15 minutes viewing each piece in the jazz music room. In the art deco room, she spends 10 minutes per piece. In the surrealism room, she spends 20 minutes per piece. In the literature room, she spends 8 minutes per piece, and in the fashion room, she spends 5 minutes per piece.\n\nHow many total minutes does Clara spend viewing all the pieces of art in the exhibition?\nAssistant: <think>', 'answer': '696', 'target': '696'}}


2025-04-10 21:09:47.335 | INFO     | __main__:custom_reward_fn:339 - [STEP 0] [policy_reward_fn/log]: - prompt: A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e. your response format should be: <think> reasoning process here </think>

<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.
This is the problem:
Clara, a literature major fascinated by the artistic movements of the Roaring Twenties, decides to visit an art exhibition that features famous works from this era. The exhibition has 5 rooms, each dedicated to a different aspect of the 1920s, such as jazz music, art deco, surrealism, literature, and fashion. Each room contains exactly 12 pieces of art.

Clara spends an average of 15 minutes viewing each piece in the jazz music room. In the art deco room, she spends 10 minutes per piece. In the surrealism room, she spends 20 minutes per piece. In the literature room, she spends 8 minutes per piece, and in the fashion room, she spends 5 minutes per piece.

How many total minutes does Clara spend viewing all the pieces of art in the exhibition?
Assistant: <think>

- output:  First, I will calculate the total number of pieces of art she views in each room by multiplying the number of pieces per room by the number of rooms. Then, I will calculate the total time spent in each room by multiplying the average time spent per piece by the total number of pieces in that room. Finally, I will add up the total time spent in all the rooms to get the final answer.

Four times the number of rooms is: 4 * 12 pieces/room = 48 pieces.

In the jazz music room, she spends 15 minutes per piece for a total of: 48 pieces * 15 minutes/piece = 720 minutes.

In the art deco room, she spends 10 minutes per piece for a total of: 48 pieces * 10 minutes/piece = 480 minutes.

In the surrealism room, she spends 20 minutes per piece for a total of: 48 pieces * 20 minutes/piece = 960 minutes.

In the literature room, she spends 8 minutes per piece for a total of: 48 pieces * 8 minutes/piece = 384 minutes.

In the fashion room, she spends 5 minutes per piece for a total of: 48 pieces * 5 minutes/piece = 240 minutes.

The total time Clara spends in all the rooms is 720 minutes + 480 minutes + 960 minutes + 384 minutes + 240 minutes = 2764 minutes.

</think>
<answer> 2764 minutes </answer>

- final_answer: 2764 minutes

- extras: prompt: A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e. your response format should be: <think> reasoning process here </think>

<answer> answer here </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>.
This is the problem:
Clara, a literature major fascinated by the artistic movements of the Roaring Twenties, decides to visit an art exhibition that features famous works from this era. The exhibition has 5 rooms, each dedicated to a different aspect of the 1920s, such as jazz music, art deco, surrealism, literature, and fashion. Each room contains exactly 12 pieces of art.

Clara spends an average of 15 minutes viewing each piece in the jazz music room. In the art deco room, she spends 10 minutes per piece. In the surrealism room, she spends 20 minutes per piece. In the literature room, she spends 8 minutes per piece, and in the fashion room, she spends 5 minutes per piece.

How many total minutes does Clara spend viewing all the pieces of art in the exhibition?
Assistant: <think>
answer: 696
target: 696


- is_correct: False
2025-04-10 21:09:47.459 | DEBUG    | __main__:custom_reward_fn:364 - [STEP 0] | avg_non_stop_count=0 | percent_score_0=77.97% | percent_score_0_5=0.00% | percent_score_1=22.03% | percent_score_1_exclude_0_5=22.03% | num_tokens=[345, 276, 363, 318, 282, 279, 261, 243, 139, 145, 206, 231, 370, 301, 278, 353, 440, 625, 401, 546, 144, 692, 533, 383, 539, 340, 154, 269, 180, 171, 174, 299, 552, 329, 829, 617, 372, 256, 418, 1, 867, 499, 465, 294, 714, 1317, 235, 750, 760, 520, 272, 614, 1124, 448, 111, 341, 225, 21, 173, 235, 104, 147, 148, 201] | stop_percentage=92.19
2025-04-10 21:09:47.462 | INFO     | orz.ppo.trainer:make_experience:593 - Calculate custom rewards, time cost: 28.07s
Traceback (most recent call last):
  File "/opt/conda/envs/ptca/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/ptca/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/playground/orz_7b_ppo_self_play.py", line 2838, in <module>
    asyncio.run(exp.run())
  File "/opt/conda/envs/ptca/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/ptca/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/orz/exps/examples/ppo/ppo_base_exp.py", line 231, in run
    await self.trainer.train()
  File "/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/orz/ppo/trainer.py", line 120, in train
    await self.make_experience(rand_prompts)
  File "/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/orz/ppo/trainer.py", line 596, in make_experience
    all_prompts, outputs, custom_rewards, policy_evaluator_all_prompts, policy_evaluator_all_outputs, policy_evaluator_all_custom_rewards = await reward_fn(all_prompts, outputs, all_extras, gen_func)
  File "/scratch/AzureNfsServer_INPUT1/data/users/kevihuang/projects/Open-Reasoner-Zero/playground/orz_7b_ppo_self_play.py", line 380, in custom_reward_fn
    sampling_params: SamplingParams,
UnboundLocalError: local variable 'SamplingParams' referenced before assignment
[36m(get_repeat_score pid=1828859)[0m [2025-04-10 21:09:25,528] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.
[36m(get_repeat_score pid=1828859)[0m [2025-04-10 21:09:25,534] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)
[36m(LLMActor pid=1829147)[0m init_process_group: master_address=10.4.68.151, master_port=48333,  rank=7, world_size=9, group_name=openrlhf[32m [repeated 7x across cluster][0m
[36m(pid=1838314)[0m [2025-04-10 21:09:30,603] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.[32m [repeated 22x across cluster][0m
[36m(pid=1838314)[0m [2025-04-10 21:09:30,605] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)[32m [repeated 22x across cluster][0m
[36m(pid=1840422)[0m [2025-04-10 21:09:34,199] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.[32m [repeated 41x across cluster][0m
[36m(pid=1840422)[0m [2025-04-10 21:09:34,202] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)[32m [repeated 41x across cluster][0m
Episode [1/20]:   0%|          | 0/7110 [03:26<?, ?it/s]
